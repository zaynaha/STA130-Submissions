{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66d4cf7",
   "metadata": {},
   "source": [
    "# STA130 Homework 07\n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c467d",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Pre-lecture\" and \"Post-lecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Pre-lecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "    \n",
    "### Prompt Engineering? \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f84f1d",
   "metadata": {},
   "source": [
    "\n",
    "### Marking Rubric (which may award partial credit) \n",
    "\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 4\"\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 7\"\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 9\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ef03a",
   "metadata": {},
   "source": [
    "## \"Pre-lecture\" HW [*completion prior to next LEC is suggested but not mandatory*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf63d8b",
   "metadata": {},
   "source": [
    "### 1. Explain succinctly in your own words (but working with a ChatBot if needed)...<br>\n",
    "\n",
    "1. the difference between **Simple Linear Regression** and **Multiple Linear Regression**; and the benefit the latter provides over the former\n",
    "\n",
    "\n",
    "2. the difference between using a **continuous variable** and an **indicator variable** in **Simple Linear Regression**; and these two **linear forms**\n",
    "\n",
    "\n",
    "3. the change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single **indicator variable** is introduced alongside a **continuous variable** to create a **Multiple Linear Regression**; and these two **linear forms** (i.e., the **Simple Linear Regression** versus the **Multiple Linear Regression**)\n",
    "\n",
    "\n",
    "4. the effect of adding an **interaction** between a **continuous** and an **indicator variable** in **Multiple Linear Regression** models; and this **linear form**\n",
    "\n",
    "\n",
    "5. the behavior of a **Multiple Linear Regression** model (i.e., the expected nature of the data it models) based only on **indicator variables** derived from a **non-binary categorical variable**; this **linear form**; and the necessarily resulting **binary variable encodings** it utilizes\n",
    "       \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The requested **linear forms** are **equations**, and answers must include **equations** along with the explanations that interpret the **linear forms**. Write you **linear forms** using a style something like_\n",
    "> \n",
    "> - _\"outcome = $\\beta_0$ + $\\beta_A$ predictorA + $\\beta_B$ 1(predictorB)\"_ \n",
    "> - _where the \"1(.)\" notation is for indicator variables_ \n",
    "> - _or feel free to use an similar alternative if a ChatBot provides you with another notation you think is clearer and like better if you prefer_\n",
    ">\n",
    "> _DO INCLUDE the **intercept** in your **linear forms**. You don't have to include notation related to the **error** term since this is essentially always assumed (and, actually, we usually don't even bother to include the **intercept** in such shorthand specifications either, for the same reason), but don't forget to include the **intercept** here this time (for practice). The modeling **assumptions** do not need to be addressed beyond this, but explanations will likely address the number of variables and the essential use-case (perhaps illustrated through examples) the different models imply._    \n",
    "> \n",
    "> _Answers to the final question above should address the notion of a \"baseline\" group and it's role for **model interpretation**, why \"number of categories minus one\" **indicator variables** are used to represent the original **categorical variable**, and the relationship between the **binary** and **categorical variables** that are relevant for this model specification. An example use-case would likely be helpful for illustration here._ \n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a8823",
   "metadata": {},
   "source": [
    "### 1.1 **The difference between Simple Linear Regression and Multiple Linear Regression; and the benefit the latter provides over the former**\n",
    "\n",
    "- **Simple Linear Regression (SLR)** models the relationship between a **single predictor variable** (either continuous or categorical) and an outcome variable. It has the form:\n",
    "  \n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot \\text{PredictorA}\n",
    "  $]\n",
    "\n",
    "  where:\n",
    "  - $( \\beta_0 $) is the intercept.\n",
    "  - $( \\beta_1 $) is the coefficient for the predictor variable.\n",
    "\n",
    "- **Multiple Linear Regression (MLR)** models the relationship between **two or more predictor variables** and the outcome variable. It includes the benefits of handling multiple predictors simultaneously and capturing more complex relationships. The equation is:\n",
    "\n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot \\text{PredictorA} + \\beta_2 \\cdot \\text{PredictorB} + \\cdots\n",
    "  $]\n",
    "\n",
    "  **Benefit of MLR**:\n",
    "  - MLR allows for the inclusion of multiple predictors, which leads to a better, more accurate model because it can account for additional factors that influence the outcome.\n",
    "  - It can reduce the potential bias that may arise in SLR if there are other influential variables not included in the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 **The difference between using a continuous variable and an indicator variable in Simple Linear Regression; and these two linear forms**\n",
    "\n",
    "- **Using a Continuous Variable**: When the predictor is continuous (e.g., age or income), the relationship is modeled as a straight line (linear relationship). The equation is:\n",
    "\n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot \\text{ContinuousVariable}\n",
    "  $]\n",
    "\n",
    "  where:\n",
    "  - $( \\beta_0 $) is the intercept.\n",
    "  - $( \\beta_1 $) is the slope, representing the change in outcome for each unit change in the continuous variable.\n",
    "\n",
    "- **Using an Indicator Variable**: When the predictor is categorical, it is typically transformed into an indicator variable (binary) that takes the value 1 for one category and 0 for the others. For example, if the predictor is **gender**, with **Male** and **Female**, the model becomes:\n",
    "\n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot 1(\\text{Male})\n",
    "  $]\n",
    "\n",
    "  where:\n",
    "  - $( \\beta_0 $) is the outcome for the baseline group (Female, if Male = 1 is the indicator).\n",
    "  - $( \\beta_1 $) is the difference in outcome between **Male** and the baseline group (Female).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 **The change that happens in the behavior of the model when a single indicator variable is introduced alongside a continuous variable to create a Multiple Linear Regression; and these two linear forms**\n",
    "\n",
    "- **Simple Linear Regression (Single Continuous Variable)**:\n",
    "  \n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot \\text{ContinuousVariable}\n",
    "  $]\n",
    "\n",
    "- **Multiple Linear Regression (Continuous + Indicator Variable)**: When you introduce an indicator variable (e.g., Gender), the model accounts for the impact of **both** the continuous variable and the indicator on the outcome. The equation becomes:\n",
    "\n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot \\text{ContinuousVariable} + \\beta_2 \\cdot 1(\\text{Gender})\n",
    "  $]\n",
    "\n",
    "  **Explanation**:\n",
    "  - $( \\beta_1 $) still represents the slope of the relationship between the continuous variable and the outcome.\n",
    "  - $( \\beta_2 $) represents the **shift** in the outcome for the indicator variable (e.g., the difference between **Male** and **Female** in the gender case).\n",
    "  - The addition of the indicator variable allows the model to capture differences in the outcome due to categorical factors (e.g., gender) alongside the continuous factor (e.g., age).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 **The effect of adding an interaction between a continuous and an indicator variable in Multiple Linear Regression models; and this linear form**\n",
    "\n",
    "- **Equation without Interaction Term**:\n",
    "\n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot \\text{ContinuousVariable} + \\beta_2 \\cdot 1(\\text{Gender})\n",
    "  $]\n",
    "\n",
    "- **Equation with Interaction Term**:\n",
    "\n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot \\text{ContinuousVariable} + \\beta_2 \\cdot 1(\\text{Gender}) + \\beta_3 \\cdot \\text{ContinuousVariable} \\cdot 1(\\text{Gender})\n",
    "  $]\n",
    "\n",
    "  **Explanation**:\n",
    "  - The interaction term ($( \\beta_3 $)) allows the model to account for how the relationship between the **continuous variable** and the **outcome** changes depending on the **category** of the indicator variable (e.g., gender).\n",
    "  - If $( \\beta_3 \\neq 0 $), it means the slope of the continuous variable (e.g., age) is **different** for each category of the indicator variable (e.g., Male and Female). This can capture situations where, for example, the impact of age on salary might differ between men and women.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 **The behavior of a Multiple Linear Regression model based only on indicator variables derived from a non-binary categorical variable; this linear form; and the resulting binary variable encodings it utilizes**\n",
    "\n",
    "- **Non-Binary Categorical Variable**: Consider a categorical variable with more than two categories, like **Car Color** (with categories Red, Blue, Green). To include this in a regression model, you create indicator variables.\n",
    "\n",
    "- **Linear Form** (with Red as baseline):\n",
    "\n",
    "  $[\n",
    "  \\text{Outcome} = \\beta_0 + \\beta_1 \\cdot 1(\\text{Blue}) + \\beta_2 \\cdot 1(\\text{Green})\n",
    "  $]\n",
    "\n",
    "  **Explanation**:\n",
    "  - $( \\beta_0 $) is the **baseline** outcome for **Red** cars (since it's implicitly represented by the intercept).\n",
    "  - $( \\beta_1 $) is the difference in outcome for **Blue** cars compared to **Red** cars.\n",
    "  - $( \\beta_2 $) is the difference in outcome for **Green** cars compared to **Red** cars.\n",
    "\n",
    "  **Binary Encoding**:\n",
    "  - For **Blue**, the binary indicator is 1 for Blue and 0 otherwise.\n",
    "  - For **Green**, the binary indicator is 1 for Green and 0 otherwise.\n",
    "  - The number of indicator variables used is always **one less than the number of categories** to avoid perfect multicollinearity (this is why we use $( k-1 $) indicator variables for a categorical variable with $( k $) categories).\n",
    "\n",
    "**Example Use Case**:\n",
    "- If you were modeling customer satisfaction based on car color, the model would allow you to measure how satisfaction differs between Blue or Green cars relative to Red (the baseline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc155a4",
   "metadata": {},
   "source": [
    "### 2. Explain in your own words (but working with a ChatBot if needed) what the specific (outcome and predictor) variables are for the scenario below; whether or not any meaningful interactions might need to be taken into account when predicting the outcome; and provide the linear forms with and without the potential interactions that might need to be considered<br>\n",
    "\n",
    "> Imagine a company that sells sports equipment. The company runs advertising campaigns on TV and online platforms. The effectiveness of the TV ad might depend on the amount spent on online advertising and vice versa, leading to an interaction effect between the two advertising mediums.    \n",
    "\n",
    "1. Explain how to use these two formulas to make **predictions** of the **outcome**, and give a high level explaination in general terms of the difference between **predictions** from the models with and without the **interaction** \n",
    "\n",
    "2. Explain how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables)    \n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _While working on this question, it's important to clearly understand the (**outcome** and **predictor**) **variables** under consideration, and they way they are being considered. Similarly to the previous (first) question of this homework assignment, this question requires the **equations** of the indicated **linear forms** and an explanation of their **interpretation** and use. What is different here is that the **interactions** being considered are between two **continuous variables** or two **binary variables** (for a total of four **equations** under consideration where two include the **interactions** and two do not)._\n",
    ">\n",
    "> _The way an **interaction** actually appears and works in the context of the linear form of a **multiple linear regression** model may not necessarily be immediately intuitive, as it is actually in fact somewhat subtle and tricky. Of course, an **interaction** is when the relationship of one **predictor variable** with the **outcome variable** depends on the value of another different **predictor variable**, so the impact of one **predictor variable** changes based on the presence or magnitude of another **predictor variable**. But are you sure you know what this means in the context of the **linear form** of a **multiple linear regression** model?_\n",
    ">\n",
    "> - _Imagine you're selling smoothies, the taste of your smoothie depends on the ingredients included in the smoothie, and there are two ingredients (bananas and strawberries) in the smoothie._\n",
    ">     - _Adding more bananas into the smoothie will of course increase the \"banana\" flavor of the smoothie, and vice-versa for strawberries..._\n",
    ">     - _But is this \"banana\" influence on the flavor always constant, or can it change depending on the absolute amount of strawberries in the smoothie?_ \n",
    ">     - _If the \"banana\" flavor influence is constant and does not depend on the  absolute amount of strawberries in the smoothie, then there is no **interaction** and the **linear form** of the model is $\\beta_b b_i + \\beta_s s_i$ and the model is said to be only **additive**._\n",
    ">     - _But if the \"banana\" flavor influence does depend on the absolute amount of strawberries in the smoothie, then there IS an **interaction** and the **linear form** of the model is $\\beta_b b_i + \\beta_s s_i + \\beta_{bs} (b_i\\times s_i)$ and the model is said to be **synergistic**._\n",
    ">         \n",
    "> _These **linear forms** show that either bananas and strawberries do not have any **synergistic interaction** and contribute to the flavor independently; or, they do have a **synergistic interaction** and there is an interesting interplay between bananas and strawberries in the way they influence the taste of the smoothie._ \n",
    "> \n",
    "> - _So, if there is no **interaction**, then the effect of adding more bananas on the taste of the smoothie will always be the same, no matter how many strawberries you put in. So the effect of bananas on the smoothie is the same whether you add a lot of strawberries or just a few: $\\beta_b b_i + \\beta_s s_i$_\n",
    "> - _Or, on the other hand, if there is an **interaction**, then the effect of adding bananas (on the smootie flavor) will be different depending on how many strawberries there currently are in the smoothie: $\\beta_b b_i + \\beta_s s_i + \\beta_{bs} (b_i\\times s_i)$_\n",
    "> \n",
    "> _In this case, the right answer is probably that the **linear form** with the **interaction** is correct. This is because the flavor probably depends on the relative amount of bananas and strawberries in the smoothie; so, the effect of adding a fixed amount of bananas to the smoothie probalby depends on the absolute amount of strawberries that are in the smoothie._\n",
    "> \n",
    "> _Again, because understanding **interactions** in the context of **linear forms** is somewhat subtle and tricky and indeed not necessarily obviously intuitive, let's think about this a bit more. And we can simplify the concept a little bit by considering how this **interaction** would actually technically work in a **linear form** if we just had **binary indicator variables**._\n",
    ">         \n",
    "> - _To consider the smootie example in terms of binary variables, suppose that if both fruits are added to the smootie, they will be added in the same amount. So the smoothie will be made with either just bananas, just strawberries, or both (or neither and you won't make a smoothie)._ \n",
    ">     - _The question regarding an **interaction** then is, is the influence of the ingredients on the taste of the smoothie **additive** or **synergistic**? That is, does the way bananas affects the flavor of the smoothie change depending on the inclusion or exclusion of strawberries in the smoothie?_\n",
    ">     - _**Additive** $\\beta_b 1_{[b_i=1]}(b_i) + \\beta_s 1_{[s_i=1]}(s_i)$ means there are three different flavors but they are explained by just two **parameters**: banana $\\beta_b$, strawberry $\\beta_s$, and banana-strawberry $\\beta_b+\\beta_s$_\n",
    ">     - _**Synergistic** $\\beta_b 1_{[b_i=1]}(b_i) + \\beta_s 1_{[s_i=1]}(s_i) + \\beta_{bs}(1_{[b_i=1]}(b_i) \\times 1_{[s_i=1]}(s_i))$ means there are of course again three different flavors, but this time they are explained by three **parameters**: banana $\\beta_b$, strawberry $\\beta_s$, and banana-strawberry $\\beta_b+\\beta_s + \\beta_{bs}$, which indicates that the flavor is \"more than just sum of its parts\", meaning there is a **synergistic interaction** and there is an interesting interplay between bananas and strawberries in the way they influence the taste of the smoothie_\n",
    ">     \n",
    "> _As the **additive** and **synergistic** versions of the **linear form** of the two **binary indicator variables** context shows, we don't need an interaction to make different predictions for different combinations of things. Instead, what these show is that the prediction will either be **additive** and \"just the sum of it's parts\" or **synergistic** (**interactive**) and \"more than just sum of its parts\"._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "      \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8a891",
   "metadata": {},
   "source": [
    "### 2. Explanation of Variables, Interaction, and Linear Forms\n",
    "\n",
    "In this scenario, we are predicting the **effectiveness of advertising** based on the company’s spending on TV and online advertising.\n",
    "\n",
    "1. **Outcome Variable**: \n",
    "   - The outcome variable, or what we're predicting, is the **effectiveness of advertising**. This could be measured by metrics like *sales increase*, *customer engagement*, or *brand awareness*, depending on how the company gauges the success of its ad campaign.\n",
    "\n",
    "2. **Predictor Variables**:\n",
    "   - The predictor variables are **amount spent on TV advertising** and **amount spent on online advertising**. These are continuous variables, representing the monetary investment in each platform.\n",
    "\n",
    "3. **Interaction Effect**:\n",
    "   - The effectiveness of one advertising platform (e.g., TV) might depend on how much is spent on the other platform (e.g., online). This dependency creates an **interaction effect**. For instance, the impact of TV ad spending might vary based on the amount spent on online ads, and vice versa.\n",
    "\n",
    "4. **Linear Forms**:\n",
    "   - **Without Interaction**: If there’s no interaction, each platform’s effect on advertising effectiveness is independent of the other. The linear form without interaction would be:\n",
    "     $[\n",
    "     \\text{Effectiveness} = \\beta_0 + \\beta_{\\text{TV}} \\cdot (\\text{TV spending}) + \\beta_{\\text{Online}} \\cdot (\\text{Online spending})\n",
    "     $]\n",
    "     - Here, $(\\beta_0$) is the intercept (baseline effectiveness with no spending on either platform).\n",
    "     - $(\\beta_{\\text{TV}}$) represents the effect of spending on TV ads alone.\n",
    "     - $(\\beta_{\\text{Online}}$) represents the effect of spending on online ads alone.\n",
    "\n",
    "   - **With Interaction**: Adding an interaction term allows for the possibility that the effectiveness of TV spending depends on online spending, and vice versa. The linear form with interaction is:\n",
    "     $[\n",
    "     \\text{Effectiveness} = \\beta_0 + \\beta_{\\text{TV}} \\cdot (\\text{TV spending}) + \\beta_{\\text{Online}} \\cdot (\\text{Online spending}) + \\beta_{\\text{Interaction}} \\cdot (\\text{TV spending} \\times \\text{Online spending})\n",
    "     $]\n",
    "     - The additional term, $(\\beta_{\\text{Interaction}} \\cdot (\\text{TV spending} \\times \\text{Online spending})$), represents the **interaction effect**. This term captures how the combined spending on both platforms impacts effectiveness in a way that goes beyond their individual contributions.\n",
    "\n",
    "Including the interaction term provides a more nuanced prediction of effectiveness, as it accounts for potential *synergistic effects* between TV and online ad spending, which might be missed if we assume only an additive relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3436f",
   "metadata": {},
   "source": [
    "### 2.1 Making Predictions Using the Two Models\n",
    "\n",
    "To predict the effectiveness of advertising, we can plug specific values of TV and online spending into each of the two models—one without interaction and one with interaction.\n",
    "\n",
    "1. **Without Interaction**:\n",
    "   - The model without interaction assumes that the effectiveness of TV and online spending is *independent* of each other. Given spending values for TV and online ads, we can predict effectiveness by substituting those values directly into the formula:\n",
    "     $[\n",
    "     \\text{Effectiveness} = \\beta_0 + \\beta_{\\text{TV}} \\cdot (\\text{TV spending}) + \\beta_{\\text{Online}} \\cdot (\\text{Online spending})\n",
    "     $]\n",
    "   - This prediction will simply add up the separate effects of TV and online spending, assuming each operates independently.\n",
    "\n",
    "2. **With Interaction**:\n",
    "   - The model with interaction takes into account that the impact of TV ad spending might *change depending on the level of online ad spending*, and vice versa. To use this model, we substitute the values of TV and online spending, just like before, but now we also include an interaction term:\n",
    "     $[\n",
    "     \\text{Effectiveness} = \\beta_0 + \\beta_{\\text{TV}} \\cdot (\\text{TV spending}) + \\beta_{\\text{Online}} \\cdot (\\text{Online spending}) + \\beta_{\\text{Interaction}} \\cdot (\\text{TV spending} \\times \\text{Online spending})\n",
    "     $]\n",
    "   - Here, the interaction term ($(\\beta_{\\text{Interaction}} \\cdot (\\text{TV spending} \\times \\text{Online spending})$)) adjusts the prediction by considering the *combined effect* of TV and online spending, allowing for synergistic influences between the two.\n",
    "\n",
    "#### Differences in Predictions\n",
    "- **Without Interaction**: Predictions assume a straightforward, additive relationship between the two advertising budgets and the outcome, making the effect of each platform *constant* regardless of the other.\n",
    "- **With Interaction**: Predictions allow for the possibility that the effect of TV spending depends on online spending, creating a *synergistic or variable impact*. This model is more flexible, potentially providing a more accurate prediction if the interaction is significant.\n",
    "\n",
    "### 2.2 Predictions with Binary (High/Low) Categories\n",
    "\n",
    "If the advertisement budgets are instead categorized as “high” or “low” rather than exact continuous spending amounts, we treat these budgets as binary variables (e.g., 1 for \"high\" and 0 for \"low\").\n",
    "\n",
    "1. **Model without Interaction**:\n",
    "   - For binary variables, we substitute 1 for high spending and 0 for low spending. The formula then becomes:\n",
    "     $[\n",
    "     \\text{Effectiveness} = \\beta_0 + \\beta_{\\text{TV}} \\cdot \\text{TV (1 or 0)} + \\beta_{\\text{Online}} \\cdot \\text{Online (1 or 0)}\n",
    "     $]\n",
    "   - Here, $(\\beta_{\\text{TV}}$) and $(\\beta_{\\text{Online}}$) capture the effect of high spending for each ad type independently.\n",
    "\n",
    "2. **Model with Interaction**:\n",
    "   - With an interaction term, we add an additional term that captures the combined effect of both ad types being “high”:\n",
    "     $[\n",
    "     \\text{Effectiveness} = \\beta_0 + \\beta_{\\text{TV}} \\cdot \\text{TV (1 or 0)} + \\beta_{\\text{Online}} \\cdot \\text{Online (1 or 0)} + \\beta_{\\text{Interaction}} \\cdot (\\text{TV (1 or 0)} \\times \\text{Online (1 or 0)})\n",
    "     $]\n",
    "   - The interaction term ($(\\beta_{\\text{Interaction}}$)) will be added only when both TV and online spending are high (both set to 1). If either variable is low (0), the interaction term drops out, so the effect becomes purely additive.\n",
    "\n",
    "This categorical approach allows us to predict the effectiveness of different combinations of high or low spending and still consider if high spending on one type of ad interacts with high spending on the other to influence effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097af67",
   "metadata": {},
   "source": [
    "### 3. Use *smf* to fit *multiple linear regression* models to the course project dataset from the canadian social connection survey<br>\n",
    "\n",
    "> **EDIT: No, you probably actually care about CATEGORICAL or BINARY outcomes rather than CONTINUOUS outcomes... so you'll probably not actually want to do _multiple linear regression_ and instead do _logistic regression_ or _multi-class classification_. Okay, I'll INSTEAD guide you through doing _logistic regression_.**\n",
    "\n",
    "1. ~~for an **additive** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables** and a **CONTINUOUS OUTCOME varaible**~~ \n",
    "    1. This would have been easy to do following the instructions [here](https://www.statsmodels.org/dev/example_formulas.html). A good alternative analagous presentation for logistic regression I just found seems to be this one from a guy named [Andrew](https://www.andrewvillazon.com/logistic-regression-python-statsmodels/). He walks you through the `logit` alternative to `OLS` given [here](https://www.statsmodels.org/dev/api.html#discrete-and-count-models).\n",
    "    2. Logistic is for a **binary outcome** so go see this [piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1) describing how you can turn any **non-binary categorical variable** into a **binary variable**. \n",
    "    3. Then instead do this problem like this: **catogorical outcome** turned into a **binary outcome** for **logistic regression** and then use any **additive** combination of a couple of **continuous**, **binary**, and/or **categorical variables** as **predictor variables**. \n",
    "\n",
    "\n",
    "```python\n",
    "# Here's an example of how you can do this\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()\n",
    "```\n",
    "\n",
    "\n",
    "2. ~~for a **synertistic interaction** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables**~~\n",
    "    1. But go ahead and AGAIN do this for **logistic regression** like above.\n",
    "    2. Things are going to be A LOT simpler if you restrict yourself to **continuous** and/or **binary predictor variables**.  But of course you could *use the same trick again* to treat any **categorical variable** as just a **binary variable** (in the manner of [that piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1).\n",
    "    \n",
    "\n",
    "3. and **interpretively explain** your **linear forms** and how to use them to make **predictions**\n",
    "    1. Look, intereting **logistic regression** *IS NOT* as simple as interpreting **multivariate linear regression**. This is because it requires you to understand so-called **log odds** and that's a bit tricky. \n",
    "    2. So, INSTEAD, **just intepret you logistic regression models** *AS IF* they were **multivariate linear regression model predictions**, okay?\n",
    "\n",
    "\n",
    "4. and interpret the statistical evidence associated with the **predictor variables** for each of your model specifications \n",
    "    1. **Yeah, you're going to be able to do this based on the `.fit().summary()` table _just like with multiple linear regression_**... now you might be starting to see how AWESOME all of this stuff we're doing is going to be able to get...\n",
    "\n",
    "\n",
    "5. and finally use `plotly` to visualize the data with corresponding \"best fit lines\" for a model with **continuous** plus **binary indicator** specification under both (a) **additive** and (b) **synergistic** specifications of the **linear form** (on separate figures), commenting on the apparent necessity (or lack thereof) of the **interaction** term for the data in question\n",
    "    1. Aw, shit, you DEF not going to be able to do this if you're doing **logistic regression** because of that **log odds** thing I mentioned... hmm...\n",
    "    2. OKAY! Just *pretend* it's **multivariate linear regression** (even if you're doing **logistic regression**) and *pretend* your **fitted coefficients** belong to a **continuous** and a **binary predictor variable**; then, draw the lines as requested, and simulate **random noise** for the values of your **predictor data** and plot your lines along with that data.\n",
    "    \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _This [link](https://www.statsmodels.org/dev/examples/notebooks/generated/formulas.html) offers guidance on using `statsmodels.formula.api` (`smf`) to build statistical models in Python using formulas._\n",
    ">\n",
    "> _The \"best fit lines\" summarize the relationship between the **outcome** and **predictor variables** observed in the data as well as the **linear form** of the **multiple linear regression** allows. The statistical evidence for the these estimated realtionship characterizations of course depends on an evaluation of the **hypothesis testing** for the **coefficients** of the model. **Model building** is the process of exploring the evidence for observed relationships captured through the modeling of the data in order to arrive at reliable (**generalizable**) claims based on the data, and perhaps make predictions about the future based on these created beliefs and understandings (whose value of course depends on how trustworthy these created beliefs and understandings are)._\n",
    ">\n",
    "> _When we do not find sufficient sufficient evidence for supposed relationships that we'd like to leverage for understanding or prediction, attempting to move forward on the basis of such \"findings\" is certainly a dangerous errand..._\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5728cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in CONNECTION_neighbours_name_num:\n",
      "[nan '5 or more' '1–2' '3–4' 'Presented but no response']\n",
      "\n",
      "Unique values in WELLNESS_self_rated_mental_health:\n",
      "['Poor' 'Very good' 'Good' 'Fair' 'Excellent' nan\n",
      " 'Presented but no response']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the dataset\n",
    "cscs_data = pd.read_csv(\"/home/jovyan/STA130/COURSE PROJECT/CSCS_data_anon.csv\", low_memory=False)\n",
    "\n",
    "# Display unique values in the columns of interest\n",
    "print(\"Unique values in CONNECTION_neighbours_name_num:\")\n",
    "print(cscs_data[\"CONNECTION_neighbours_name_num\"].unique())\n",
    "\n",
    "print(\"\\nUnique values in WELLNESS_self_rated_mental_health:\")\n",
    "print(cscs_data[\"WELLNESS_self_rated_mental_health\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93004430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.618916\n",
      "         Iterations 5\n",
      "                            Logit Regression Results                            \n",
      "================================================================================\n",
      "Dep. Variable:     mental_health_binary   No. Observations:                 9258\n",
      "Model:                            Logit   Df Residuals:                     9256\n",
      "Method:                             MLE   Df Model:                            1\n",
      "Date:                  Fri, 15 Nov 2024   Pseudo R-squ.:                 0.01465\n",
      "Time:                          00:46:30   Log-Likelihood:                -5729.9\n",
      "converged:                         True   LL-Null:                       -5815.1\n",
      "Covariance Type:              nonrobust   LLR p-value:                 5.967e-39\n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Intercept                          0.5977      0.025     24.184      0.000       0.549       0.646\n",
      "CONNECTION_neighbours_name_num     0.1764      0.014     12.250      0.000       0.148       0.205\n",
      "==================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 3.1AB\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the dataset\n",
    "cscs_data = pd.read_csv(\"/home/jovyan/STA130/COURSE PROJECT/CSCS_data_anon.csv\", low_memory=False)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = [\"CONNECTION_neighbours_name_num\", \"WELLNESS_self_rated_mental_health\"]\n",
    "cscs_data = cscs_data[columns_of_interest]\n",
    "\n",
    "# Drop rows with 'Presented but no response' and convert 'NaN' values in CONNECTION_neighbours_name_num to 0\n",
    "cscs_data = cscs_data[\n",
    "    ~cscs_data[\"CONNECTION_neighbours_name_num\"].isin([\"Presented but no response\"]) &\n",
    "    ~cscs_data[\"WELLNESS_self_rated_mental_health\"].isin([\"Presented but no response\"])\n",
    "]\n",
    "\n",
    "# Map the `CONNECTION_neighbours_name_num` categories to numerical values\n",
    "cscs_data[\"CONNECTION_neighbours_name_num\"] = cscs_data[\"CONNECTION_neighbours_name_num\"].map({\n",
    "    '5 or more': 6,\n",
    "    '1–2': 1.5,\n",
    "    '3–4': 3.5,\n",
    "    None: 0\n",
    "}).fillna(0)\n",
    "\n",
    "# Map `WELLNESS_self_rated_mental_health` to binary outcome: 1 for positive, 0 for negative\n",
    "cscs_data[\"mental_health_binary\"] = cscs_data[\"WELLNESS_self_rated_mental_health\"].map({\n",
    "    'Excellent': 1,\n",
    "    'Very good': 1,\n",
    "    'Good': 1,\n",
    "    'Fair': 0,\n",
    "    'Poor': 0\n",
    "})\n",
    "\n",
    "# Drop any remaining rows with NaN values in the new binary column\n",
    "cscs_data = cscs_data.dropna()\n",
    "\n",
    "# Fit a logistic regression model\n",
    "log_reg_formula = 'mental_health_binary ~ CONNECTION_neighbours_name_num'\n",
    "log_reg_fit = smf.logit(log_reg_formula, data=cscs_data).fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(log_reg_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a83f5a",
   "metadata": {},
   "source": [
    "### Linear Model for Mental Health Binary\n",
    "\n",
    "The linear model can be represented as:\n",
    "\n",
    "$[\n",
    "\\text{Mental Health Binary} = \\text{Intercept} + \\text{Slope} \\times \\text{CONNECTION\\_neighbours\\_name\\_num}\n",
    "$]\n",
    "\n",
    "Given the output from your logistic regression model:\n",
    "\n",
    "- **Intercept** = 0.5977\n",
    "- **Slope** (for `CONNECTION_neighbours_name_num`) = 0.1764\n",
    "\n",
    "The linear model becomes:\n",
    "\n",
    "$[\n",
    "\\text{Mental Health Binary} = 0.5977 + 0.1764 \\times \\text{CONNECTION\\_neighbours\\_name\\_num}\n",
    "$]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0a4873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.618916\n",
      "         Iterations 5\n",
      "                            Logit Regression Results                            \n",
      "================================================================================\n",
      "Dep. Variable:     mental_health_binary   No. Observations:                 9258\n",
      "Model:                            Logit   Df Residuals:                     9256\n",
      "Method:                             MLE   Df Model:                            1\n",
      "Date:                  Fri, 15 Nov 2024   Pseudo R-squ.:                 0.01465\n",
      "Time:                          00:49:39   Log-Likelihood:                -5729.9\n",
      "converged:                         True   LL-Null:                       -5815.1\n",
      "Covariance Type:              nonrobust   LLR p-value:                 5.967e-39\n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Intercept                          0.5977      0.025     24.184      0.000       0.549       0.646\n",
      "CONNECTION_neighbours_name_num     0.1764      0.014     12.250      0.000       0.148       0.205\n",
      "==================================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3.1AB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load and clean the dataset (building on your existing code)\n",
    "cscs_data = pd.read_csv(\"/home/jovyan/STA130/COURSE PROJECT/CSCS_data_anon.csv\", low_memory=False)\n",
    "columns_of_interest = [\"CONNECTION_neighbours_name_num\", \"WELLNESS_self_rated_mental_health\"]\n",
    "cscs_data = cscs_data[columns_of_interest]\n",
    "\n",
    "# Drop rows with 'Presented but no response' and convert 'NaN' values in CONNECTION_neighbours_name_num to 0\n",
    "cscs_data = cscs_data[\n",
    "    ~cscs_data[\"CONNECTION_neighbours_name_num\"].isin([\"Presented but no response\"]) &\n",
    "    ~cscs_data[\"WELLNESS_self_rated_mental_health\"].isin([\"Presented but no response\"])\n",
    "]\n",
    "\n",
    "# Map the `CONNECTION_neighbours_name_num` categories to numerical values\n",
    "cscs_data[\"CONNECTION_neighbours_name_num\"] = cscs_data[\"CONNECTION_neighbours_name_num\"].map({\n",
    "    '5 or more': 6,\n",
    "    '1–2': 1.5,\n",
    "    '3–4': 3.5,\n",
    "    None: 0\n",
    "}).fillna(0)\n",
    "\n",
    "# Map `WELLNESS_self_rated_mental_health` to binary outcome: 1 for positive, 0 for negative\n",
    "cscs_data[\"mental_health_binary\"] = cscs_data[\"WELLNESS_self_rated_mental_health\"].map({\n",
    "    'Excellent': 1,\n",
    "    'Very good': 1,\n",
    "    'Good': 1,\n",
    "    'Fair': 0,\n",
    "    'Poor': 0\n",
    "})\n",
    "\n",
    "# Drop any remaining rows with NaN values in the new binary column\n",
    "cscs_data = cscs_data.dropna()\n",
    "\n",
    "# Fit a logistic regression model\n",
    "log_reg_formula = 'mental_health_binary ~ CONNECTION_neighbours_name_num'\n",
    "log_reg_fit = smf.logit(log_reg_formula, data=cscs_data).fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(log_reg_fit.summary())\n",
    "\n",
    "# Create a grid for `CONNECTION_neighbours_name_num` to predict probabilities\n",
    "neighbours_vals = np.linspace(cscs_data[\"CONNECTION_neighbours_name_num\"].min(),\n",
    "                              cscs_data[\"CONNECTION_neighbours_name_num\"].max(), 100)\n",
    "grid_data = pd.DataFrame({\"CONNECTION_neighbours_name_num\": neighbours_vals})\n",
    "\n",
    "# Predict probabilities using the fitted model\n",
    "grid_data[\"predicted_prob\"] = log_reg_fit.predict(grid_data)\n",
    "\n",
    "# Plotting the data points and the logistic regression prediction line\n",
    "fig = px.scatter(cscs_data, x=\"CONNECTION_neighbours_name_num\", y=\"mental_health_binary\",\n",
    "                 title=\"Logistic Regression: Mental Health Outcome by Number of Neighbours\",\n",
    "                 labels={\"CONNECTION_neighbours_name_num\": \"Number of Neighbours\",\n",
    "                         \"mental_health_binary\": \"Mental Health Outcome (Binary)\"})\n",
    "\n",
    "# Add the logistic regression prediction line\n",
    "fig.add_trace(go.Scatter(x=neighbours_vals, y=grid_data[\"predicted_prob\"],\n",
    "                         mode='lines', name=\"Predicted Probability\", line=dict(color=\"blue\")))\n",
    "\n",
    "# Show the figure\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b936786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAH0CAYAAADfWf7fAAAgAElEQVR4XuydB3gUVduGnw279I50KYoVRemIgjSlKL0L0nsnEHqAQKih9957770IKlKkiAgKSJHepEjPbpL/ek/+ybcJCdlkdjezyTPX9V+/H5k5c+Z+z8ze8847Z0whISEh4EICJEACJEACJEACJEAC8ZSAicIbTyPLwyIBEiABEiABEiABElAEKLwcCCRAAiRAAiRAAiRAAvGaAIU3XoeXB0cCJEACJEACJEACJEDh5RggARIgARIgARIgARKI1wQovPE6vDw4EiABEiABEiABEiABCi/HAAmQAAmQAAmQAAmQQLwmQOGN1+HlwZEACZAACZAACZAACVB4OQZIgARIgARIgARIgATiNQEKb7wOLw+OBEiABEiABEiABEiAwssxQAIkQAIkQAIkQAIkEK8JUHjjdXh5cCRAAiRAAiRAAiRAAhRejgESIAESIAESIAESIIF4TYDCG6/Dy4MjARIgARIgARIgARKg8HIMkAAJkAAJkAAJkAAJxGsCFN54HV4eHAmQAAmQAAmQAAmQAIWXY4AESIAESIAESIAESCBeE6Dwxuvw8uBIgARIgARIgARIgAQovBwDJEACJEACJEACJEAC8ZoAhTdeh5cHRwIkQAIkQAIkQAIkQOHlGCABEiABEiABEiABEojXBCi88Tq8PDgSIAESIAESIAESIAEKL8cACZAACZAACZAACZBAvCZA4Y3X4eXBkQAJkAAJkAAJkAAJUHg5BkiABEiABEiABEiABOI1AQpvvA4vD44ESIAESIAESIAESIDCyzFAAiRAAiRAAiRAAiQQrwlQeON1eHlwJEACJEACJEACJEACFF6OARIgARIgARIgARIggXhNgMIbr8PLgyMBEiABEiABEiABEqDwcgyQAAmQAAmQAAmQAAnEawIU3ngdXh4cCZAACZAACZAACZAAhZdjgARIgARIgARIgARIIF4ToPDG6/Dy4EiABEiABEiABEiABCi8HAMkQAIkQAIkQAIkQALxmgCFN16HlwdHAiRAAiRAAiRAAiRA4eUYIAESIAESIAESIAESiNcEKLzxOrw8OBIgARIgARIgARIgAcMJb/+AuVi//Sec2jsvyuhs2vkLeg+biZ3LRyN7ljcMHUU5nrVbf0T5UoUxblDHSPs6b/k2jJ6+AgU+fheLJ/fTfTyd+k3A9Vv3sHaOv8NtaUy3LRmJnNkzO7xdxQY9cfXGnXDrp0yRTLVRr2oZ1KhUEokSeTncXkxXLPZtO1SvWAJ9OjXErbv3Ua5ONwzp1ULt193LX39fQa2WAzDRvzPKlSwY6e41zm+kT4O9q8ZFyuaPs5dQr80gtf3JPXNgTpTI3Yfy2r6/boxEd/7K2JTj+2H1eKcfk+z75yO/R9n2g0ePUaJaJ/Tt/D0a1vwqRvuXcf5p3jwY6dsmRttFt7I2ZlMkT4oti0YgY4a04TZ5GWhFwfKtMMinGWpXLhVdc7r/HtvrgO4dO9jA4HELsXnXLwgJAX7dNj3SrbRr0rQR3vjys09fWad+u8HIlT1zjGIZm9+cHft+RTe/KVg/bwjefevNKI/QVWPLQaRcjQQSDAGPFN5/rt3GoWOnUfnrzyE/FEZe5Ed4655DCAoOxv41E5AmdYpXulujuS+uXL+DD9/N5ZHCK4Lbs/13Ycf14NF/2H/wJDbsOICGNb9G384NXRYie+F99vwlNu08gKIFPsRbObM6tM/dPx3DjEWbsGqmn0Prv26lmAhv0iSJMdavA0oVf/UHedjExViz5Ue8eBnoEuF99PgpPq/SQQlD8mSOnz+OyJCRhDdibI0svF5eJpQvVQRjBran8EZxkl24fB1Vm/ZD4zoVUK3CF/jgnZxRCu/1W3eRNVMGbFwwDHKu2S+xEd7Y/OZQeHVfUtkACTiVgEcKr1MJOKExq9UGi8UcaUsiAOcvXcOVa7fRqUVNfFe9XLj1NEkqXvgjvHgR6JHCK1n2OWN7vnL8XQdMxt4Dx/HLxikQKY64WG1BsJj1ZS/thTc2oRw7YyUOHjvjduEtWSyfkk2RXvvFFhSE0jW74oN3c+Lg0dMuEd6fDp9C215j4r3wRoytkYW3TuXSWLV5H2aN9sHnhT8OGxLxKcP7uuukI+fur7/9haZdR2DuuF4oVuDDKDeRjGn+j9/Bzn2/Kjnu2qq2buF1pH8R1/EU4dUbl9iw4TYkEBcEPFJ4Iz5e8hs9HyfP/I1+XRph5JRlkExA6lQp1KNu+4udZAAnzlmDnft/xb/3/0PGDGlUlrhDsxrhxGvxml3qx+f6zbuwmM14/52c8G5dRz3SlEUyl32Hz8LCiX3RP2AObLYgVV4R2SLCK1L73ttv4u9L17FixsBwq0l/j/1+FrnfzIIbt/8NJ7w//HJCZR/PXbgKk8mEd9/KjlYNq4R7XL7np+MYN2sVrt24gyyZMqBFg2/w46HfVd/tSxqWrd+DZev24Mr120iePClKFv0EPu3qhT1CjZi90x6rR/f4V35cohLe+Su2Y9S05SrLkidXNpSv74MyXxSAl5cXVmzYi/7ejVXpgaNxmbpgg9pOMpTvv50DvTo2QPs+41S2J6qShgv/3MDoaStw9ORZSBbtkw/zoHvbuio71KTLcPXv2uLTth6a1a/kUH+ePH0O/3ELITEKCQnBZwXzok6V0mjXe5xDJQ0yVgOmLsP+tROQJtX/sv57D5yA94DJ6NKqFsZMXxlOeKV0RCTu8PE/8ezFSzVmmn9XCVXLfxF2DN983wslin6Ct3NlhZTK3Pn3IbJmSo/OLWqhYpmimDJvHYSjtsgjX3n0KzEYP2u1Op67/z5UfRKp6N62HjJnTKdWd1WG99DxM5gybz3OnLsMkwkqRnLefvL/55vsW8btuJmrcOz3c/jvyTNkypAW35T7DB2aVg+72bQvaYgsttUrlVAlDcL+zr0HWLftJzx99hwfvJMLvl0bRZkxlP1rEiV9m79im9r+zWyZVD+/KlkIU+evx/RFG7Fn5dhwZQnS1y9rdEazepXQpWWtVy4RWknD1OHeWLRmpzpvN8wbisSJLWrdiMIrHCo17IURfVujSvnPw9qbtWSzit/xnbOQJLEF2jVR4hcwdbniJxnPPp0aIH261BgyfpG6rkhpTZdWtfFtuc/CxXjeuN6Yu3wrjp78S52vZT4voM5X+xvX6K4pviPnqJi2aPAt5KlF6eL5MbR3y0ivk8Jp3IyVkPEvNybp0qRC6c/zo1vruuqp2ITZazBz8aZw257eNz/StiRWUlIkY1jismaOv7r+aEtkGd7ojiXib46c89MWbMDKTfvU9Sjvu6FjyHvgFHxR5GPFShPeRZP6YvGa3arcRsqTPiv0EQZ2axL2tC+6saX1W34/JMZHfz+LFy9eIluWN9T1s8V336prmyxlandV579/z+Zhx3vyzAU0aO+P6SO7Q260o/r9kt+p8bNW4Y+/LuPp8xfIkjEdqpT/Am0bVQ1rPy4khfskAWcRiBfCKxfvLbsP4uMP3lYXmjezZlQ/ZgNGzcXkYV3UxVqWFt0DcPrsZQzwboJPP8qDk6cvYNDY+fj6y8Kq7lMW2U4u1D07fIcyn+fHi5dWTFuwHoeOncGWxSORPm0qbN1zGD38p6FgvvdUnarIRd73ckcaE/kRPn32Enq0q4+WPqPC5E9WDgoKRtk63mjZ4Fv8dvpv3L77IEx4tSycZH4a1voKJpjUD+LqzfsxI6A7ShTNpwS6Zsv+6oehS4takIzp5LnrcOqvi8iQLnWY8Io0i+h3al5TSYL8WEstXEhwMNbMHqx+XCPKjJRYjJq6TElcZHVw2sG+TnhlHys3/oADGyerH59vG/VGYosZb+XMpmooc72ZWf3gOhIXaWfQ2AXq5qTK18XVzYEI0NkLV1G3SulIhfffB/+hWtN+6mZDtpN9T5yzVsVDJFzEoJvfVPz74JHKUCdLmkQ9/nSkPz6Dp2H/wd8wuEdzJWbHfz+HaQs3QB59OlLDu3vFGFRu3Ac92tdH/Wplw8aOZMWl/KVEkY9VjLQaXpGA6s18kTZNSjV+pdZTSmUmzV2L4X1bhUmvPPJ9+vQ5viz+qZIxOR4R8y17DkH2KVnleSu2KfHdtXy0ujEUiZEbOJENEak8ubMp6RXeqVIkDxuTMRHew1umRXo+CLc/z/8TVmcrWbvm3Uaqc7Bdk2pqGxnDPx85hdWzBqnSlODgEFRs0ANpUqdEvy7fq7EtP859hs1Eo9oVwkTSXngfP3n2Smyfv3iphFfaLFeioDoX/nv8FP1GzEaqlMnVuRDVIuNcsu8iTm0aVVHiosnH+nlDkSpFMnxVt7t6iiPns7ZIacrA0fOwY9moSN830IR30pDOyJM7O6o164dWDb5V41WP8Mo1cfPugypb3N+7EbxMXmjbe6wSaqknHdi9qbqRETGWBMDe1ePUOarF+O2cWdGodnnk//hd/PbHeQybtAQVShUJq3t15JoyaMx87D90EjmyZULr76uo67Kc8xEXkceGHYao9w5EBCXBcO7iVdU3uZleOrU/nr8IxMGjf6CT70QIqwL53lVSHNkisZJrYvc2dVGjRX91jZk/vneUwuvIsUQUXhFkYSxjQW44L165qcbD7bv3w94p0IQ33wdvoXbl0qrPMm77DpuFBjW/Qq8OoWVg0Y0tiYWcj3Ity50ji/ptknNAysbkprl5/W/CEjuOCG9kv1/v5cmBMrW6qmtZx2Y1kCJ5Mpw8/be6BrVtXBUtvvvGWc7BdkggzgjEG+GVC5CWSRSachEtVKE1mtStqH4Qj586j0adhqq7cPuygjnLtipxksyM/AA8+u+puri881b2sKCIVNVs0T9MnrULmWR97X/cIouiJryrZw1G+frdUansZyrDKMuPh06iY78J2LdmAoZOWBROeCVDJQIsLwhJdlcTZMmSipDMHOWjsqeLVu9UWULt4i+ZR7noyY+MZHglQyQ/8nJnb//4/NSfFyGZjlH92+GbcsUcyt5F9eOSLXMGzBztE/ZnYbjvl9/gP24BypUsFFaXKDImsi39FdmUxdG41G3jp24Q7KVEMoMtugXg+1pfRyq8IqDTF2zEj+smhmVT7tx7iOGTFuP7WuVR6JP3VEb23v1HYSUNjvQnebIk+KJaRzSo8RV6d2wQdtwin9MXbnRIeA9unoqh4xfhn2u3sHx6aNZfMkWlanbBmAHtce/+w3DCq/0oS/bPfmxKacLVG3fVC0+yCOPHT55i5/IxYU8ttAzP7NE9IKUz2kuS9jW8MtasNpuSEm2RJx3DJy2ByKtIsaPCKy9pvm7J9EbaMOGVm4uL/9zAjqWjwrKaIjdf1euG8l8WVmImwnvt5h0l6yIv2tK5/0TcuPWvEmNZIr60FjG2WkmD1E1LRlVbNLa/7ZodZWmSSIncGMm5qr03INeJ0rW6KumRDLrcPInMyzmrLTI+zeZE6iY1skUTXu0mSW5EZi3dorK8IoexzfCKjMk10f7F3gWrdiBgyjIljGVLhL5UefjEn2juPVLd1MhLs1qM2zepFibdsp7c/Kzb+iMObZmmrkeOXFO0Piyb2j9ctj4ihyMn/kIz7xHhbtxkHXnKJtIrwitP17T1JPtctMAHUQ4xiZXEWJ76aMc3rE8r9SRIFvsMb0yvjxrPOq3lehQU7ima3FT29J8edj3Sfick2dG0XsX/jYnuAap8bckUX/VvjowtGRfTFm585cZJ9vfDL7/hl01T1PnuiPBG9vt17eZdVPiuh8rAy5NRbZHfPhnv9teF157c/CMJGJhAvBFeycwe2zEzHGp5lCjCJVkDeTwnj4i3Lh4ZLssgP1C1Ww3EWL/2qFC6qMqSLly1A/LCy53/lwDJuD189CTs7X/tgrFgQh8U/vT914ZXE16RT3ksJ/0UuZaZC7oPmoqXL61KpOW/7TO8IuvyCDriI0DvgZPVY/if1k9Cm55jVEZx+9KAcH2Qx1fywpPs89Rfl1C/7SD4+TSFZIvtF6l/rfxVcZUVd0RmIjvQyGZpkPUkA1a3amlVCqK9GCUyliFdKsgPlrY4GpcC5Vupvto/qgsMtEL+PSrhFeGRUo9NC4dHGaOIUuRIf+TR8Hft/V95rHzg1z/Qusdoh4X39F+XwmX9pVxjwpw12L92ItZu2R9OeNv2GquyQ/vWhJ/dQJMYGQ/y9EEYyw3I9JHdwo5Zsk9VGvfB6AHtUKlssUiF9/7Dx5i+cAMk4/rg0RMEBwerMfT02YswaXJkjGgvrcm5EdkybuZqJa/aLA0yziXbGtC/bbjVO/adoB7Dy02sLHKezl66Rf1/6ZPc0D5++lxluiRzLYujwtutTd1wGavlG/aqLLjciNkLtX2HZJxnzZw+3NiVv8sTmnwfvI0J/p0UO6kxlVInuZmSGykREJmdRcoeIlsiCq8IWPVm/dTj6jljeuoSXpnt5uj2/10TtadX9rNsaO8QyA2ASKIW44jXNslUyxOzdXOHqD45ck0R4V2xcS9O7p772kfi2jknTxzkuLVFnmBJxluuT/IUJDbCK231GjoDB478gS2LR6gstr3wxvT6qAlv/q9bonqFEuq6qi1yo1akUhv19EpkW/udWD5tAPJ9+HbYenJjJBnszf9/XXJkbMn5L+VZwsh+Wbpuj0qWaMmemAivfYwlmdCwgz8uXb2lEkJyY1zw43ejvAE0sNOwayQQJYF4I7xb9x5SL0fZL0p4SxRUWSKtBiyyl8ukaF9mEpAZBWR6MBFeeRxcrkQhdXcrj9pEIrXprhx9GUH7EZZH6CKfWv2d1FLl/ygPvqzZBQG+bdTjXHvhlUenn5ZrgSZ1KqjHV/aLPB7duPMXnNg5Sz0GlKzcyhnhZxiQrJ9kMmWf8uKTlFKIgJr+v85La0+OW36I5cfaEZmJbBTJxTpt6hTw9W4c9ucUyZKq+saIL6SJjMnjufGD/zc9myNxqVu1LPJ/1UJlSSRbYr/IlE1SdhFZDe/3HYeqH+fXzcAQUXgd6Y88Em/lM1plCe1nWdCy5o6UNEiGN2XyZCrrL4/WRcJkjH38wVtq2iyRX/uSBjmWE3+cf+UHKCQ4RD1qFxGR0o3IGGvCq2XzI2Z4JYPasOMQXL1+BwO6NcHH7+dW2VZ5JD5q6vJYCW9U0wraT0umjXOpP0wUYeo1yZ5JOYVkruRGUCQwR/ZMKs5ZM2dQ41mESmrNYyq8EevSNdbRCa+ULcmNsf0iZSnqCceo0CccImgfvZcbklGU64g8QdqzamyUU8tFFF5pQ7txkpsAOT/tpyVztIZX2ES8JmrCK/3JkjG96q8mvFOGdVVlANp1wP5pmay3be9hSDmKZFufPXvh0DUlsj5Edg3RzrkjW6eHm3VHrmEicNqTtNgKr9x4SJwqli6qBNVeeGN6fRThzfRGOnU9kqd70jf7peg3bVVdrb3wRpyWTK71wl17KiPX0OjGlpz/cgOqPc3Q9innaK8hM6Bl0WMivBH7JXX88sRw149H1Y2lPMmScg05xsheOqZXkYCnEUgwwqu9QCXZl3RpX639ypA2tXrsHfqo7hP1eE1bRDTkgqNXeKU9aSd71jfUC0HyMpX8yIqER8zwFq7YWk1TJD+c9kuX/pNUva9sJyJ7+879VzKYMoeriLAIr9QsSzmAiKLUdUZc5KImP356hDeql9Yi7isyGXM0LnIDUKtyKQywE2vJqAinqDK8khX5+9I17F45NsrzMqLwOtKfi1duqDjKFFKShdcWKVFx9KU1Ed7UKZOrG7GNOw5g3vjeqNSwp7p5+ej93K8Ir7ycd/7S9SgfjUsMpEwkNsKrTfcUcf5ieUlI+qdltRwZIzGdlkwyYiWLfYqOzUNrVu0XL5NJ1Sxq9ZJSB2v/aFXG/+Wrt9wmvFImJLMo2C9yUy1PebRyIckWj5m+Aj+um4RmXYejeOGPI31ZTWsjMuGVv0kWUF5mldIGKZ/R5uGV2noZJxFfWtPKabSX1vQKrzxuz//RO2GHqt0UbJwv4mV16JriqPBqN2AR51U/d/EaZMpGeUJXt2qZWGd45SC0LL4Iu9zEyfkicyrH9Pqo9fGTcs1VJlTEVlvk5lqeWETM8DoivNGNLbmuyGw/2s2dts8la3dh2MQlKlssN+LyxOGLIvnCPQnTSr+0l9YcSdjIE02p7R41bYW66bL/PfQ0yWF/SUAjkGCEVyRRMqKSXZSMqrY8e/5C1e1K1kgekxb4uiW+q/G/FwpkPe3jEc4QXnnpTGZVkJq0bJnfUDXFskQUXqltlB83qW3U3sCVjJh8WEEeoUoZhNRXLl+/Fz9vmKReupFF6hTl5YO3c2VTwiuP/b+o1gk1KpVQmUP7RR6RScZVavIckZnITpvXvbQWcf3IZMyRuEg7UkMtM2bYz3IhL1lJxjAq4ZXZCKT2zf4RrsRaXpJq1bCyklX5IZFaTC1z4kh/pA2REMk4y8wO2iIzbkhWz9EMrwivlrGTrNDvf16ECIUsETO88ha+HIvIu/1jd8mCJUliCZvpISbCq2XUtBu6SUO7oOwXoS94SmlPzea+6mUcTTQdGSMxFV7JlEttrNRma7Xqsn8Z+/KWuGSaNfHWaonl7zJjReVGfdRMK9oNTWQlDfaxjWpaMkczvP89eYp9q8eH1RrfvHMfX9XtpmS9XePQF+7kelKqZldVMjF53rooX1bTxkxUwitxlZc8pe501aZ96rG+fHhCxt7nVTuomSYa1PjfFIdyc/fT4d/DZmnQK7zajCVaP6WcYfOugzi0ear6J0euKY4Kr1Y3H/GGS3tcL+emzFEe2wyv9FeeYnzXbrBKBEhiI1OGdEp4Y3p91IS3apPQxIl96c6OfUfUjYp2PYpKLCPL8EY3trQ6821LApAze6awa46UuMmsLVLSJGVy8mLrWzmzhPvIkVYy8jrhvXHrHk6c/jtstg5tB/ICt7y89rqyMOoUCXgKAUMKrzx6mzbifzWIGkzJYMnLChHfmI3qwmpf0iBtSEZIslkifvIISV6gkjfC/7l+W90hyxvt8rKYvMk8eVhX9RhHfmykVnDVph9Qr2pZdG5ZS5UJOPIFHU2WtZIG+d/yUpm8mBRotWLp1AGQN3hliSi8MiuE9LfmNyXVi3cy9Zm8XS+zUSyY0BcF870blp2Qx+rtmlRXF2+Rois37qjsoTYtmVwsZaYJeWxeqnh+tW85Lsl6SEZRpuhyxSwNjgivo3HRalVFVKUEQGpzp8xfr8RIGEVW0iCyIz9MUl7h3bq2moVBphI6fe4y1s8domY6kC/2yQt2MwO6q+maJIPoyDiRGtNDx0/Dt2tjlZH99bezWLpuNy5duRkj4ZXjl2yxTKsnjw7ljevIhFebpUEyQd3a1FFZ+b8uXFU3PTKGtAyjI8IrL5WJHEqWUERCXtYsV7ebyuj5dW+qpv2SWT3khkyyq/JoXWY62fPTMcXLmV9ak3p0eWGp1jel8F2NcipGMkODvJDZs319lUXTREfe9BfJ06ZneitXVmz/4YiKZfasGTF47IJwX1qLGFspT4rsS2uOCK+80CMZvCL531dTA8p7pPJOgGTPNi0Ypl4S1Ra5Hq3c9AOKF/ooyox8dMIrf5fHy3ITJfsa2K1p2JfWJMObPm1qVVIjNzvrt/+M2Us2QwRcb4Z3484D6DNslnoxsn2T6qrE5sSp86p+t/LXxdWsJLI4ck1xVHilPbnuyrkzqEczNeXgH2cvq3jKuaW98KdHeGUfMkVavbaD1IwVcrOrfTXPkWOJ+Juj3UzLjcHXpQqrvsv16MLlG2HXI0eF15GxJTPOVG3aV33Bsk/HBmq2FnnPRF647tKydtjL0xKn3T8ew6LJ/dS0hb+e/EtltKWE4nXCK+VY8l5C49rlUa1iCVXO8Pfl6xg4ap5KEMkNFxcS8HQChhTeqN7y1t7ujq3wSo2SPPqTCcnv3n+ItKlTqvlTRTQkwyuLPCKVOtk//rqkhFfeWJWphuSiIdlZuejLVD+xFV7Zh9RciXRpLy1EJrzybzLtjIjq2YvXII93RdI7NKuu+qwtUsMlkitvq4u4tGxYGXLxkrty7YUfWdd+nskkSRLj4/ffUtNAaS/duWIeXkeF15G4SIZmwuzV6jPNcgMiUyuJ5Mp0Wtq8l5F9WlgeA8ojZpm/VeYTlcy6T7v6qt5Vlt/PXFBzZ0qGpUmdiipb50h/REBDBeuUaqdI/g9Ulk/qA7UXICO7OGictZIGWUfGld+Y+epTwzLGIxNe+TfJasoPnNwMSSZRagnlh1umsNJmvXBEeGU2iDY9RqtxJTdO8nKUlGPIGL96866S/tbfV1Y1j828R6oXbOQmUepmnS28clxyPCILIiSySBlD/eplw71kKRnuJWt3q2nE5DyQOZjlR1mO4/nLQCyc0AfzV+4IJ7wRYyvT+8VWeOXJitS4yswJC1fvxN17D5ErRxb0aFdPlUDZLyLxInD2syFE9UMRVYZX1pcXiaQcSWTF/tPCMuvGsAmLlZDINUrm45UnNXITo828EdsMr3YzJC9ayRR+x0+dU7NMiPRIVjlZ0v99tSy6a0pMhFemkRs3a7W6qZJzS55iSEmXzH4hcZZFr/BKGzIfsIwjeQHW/jPR0R1LxN8ceQISMGUptuw+pG6EZMox3y6N1Pkv0yRKcsFR4XV0bMnTlrHTV+LIb3+ql51lLMpNov2sQ1KvLDNbyFy98sRSbrpkHmR5yVCr046qXzIH95ylW9W4kmOSJyzyIrfM2KHNC+3pwsP+J2wChhPehB0OHj0JkICnE5AbZpEzeSlJK0fy9GNi/8MTkJsRTcy1v0gWVp4q2t+ckBsJkIBxCFB4jRML9oQESMBDCUhGTGa52PnjUfXExb4e2kMPid1+DQHtRUqZh1tKyqS2WjLiMruB3OhInTAXEiABYxGg8BorHuwNCZCABxKQuuI6bfzUFH1STy+PtbnEXwJSLrBg5Q6s3rJfTVsp70xIvbNMZynlVlxIgASMR4DCa7yYsLwQIGwAACAASURBVEckQAIkQAIkQAIkQAJOJEDhdSJMNkUCJEACJEACJEACJGA8AhRe48WEPSIBEiABEiABEiABEnAiAQqvE2GyKRIgARIgARIgARIgAeMRoPAaLybsEQmQAAmQAAmQAAmQgBMJUHidCJNNkQAJkAAJkAAJkAAJGI8Ahdd4MWGPSIAESIAESIAESIAEnEiAwutEmGyKBEiABEiABEiABEjAeAQovMaLCXtEAiRAAiRAAiRAAiTgRAIUXifCZFMkQAIkQAIkQAIkQALGI0DhNV5M2CMSIAESIAESIAESIAEnEqDwOhEmmyIBEiABEiABEiABEjAeAQqv8WLCHpEACZAACZAACZAACTiRAIXXiTDZFAmQAAmQAAmQAAmQgPEIUHiNFxP2iARIgARIgARIgARIwIkEKLxOhMmmSIAESIAESIAESIAEjEeAwmu8mLBHJEACJEACJEACJEACTiRA4XUiTDZFAiRAAiRAAiRAAiRgPAIUXuPFhD0iARIgARIgARIgARJwIgEKrxNhsikSIAESIAESIAESIAHjEaDwGi8m7BEJkAAJkAAJkAAJkIATCVB4nQiTTZEACZAACZAACZAACRiPAIXXeDFhj0iABEiABEiABEiABJxIgMLrRJhsigRIgARIgARIgARIwHgEKLzGiwl7RAIkQAIkQAIkQAIk4EQCFF4nwmRTJEACJEACJEACJEACxiNA4TVeTNgjEiABEiABEiABEiABJxKg8DoRJpsiARIgARIgARIgARIwHgEKr/Fiwh6RAAmQAAmQAAmQAAk4kQCF14kw2RQJkAAJkAAJkAAJkIDxCFB4jRcT9ogESIAESIAESIAESMCJBCi8ToTJpkiABEiABEiABEiABIxHgMJrvJiwRyRAAiRAAiRAAiRAAk4kQOF1Ikw2RQIkQAIkQAIkQAIkYDwCFF7jxYQ9IgESIAESIAESIAEScCIBCq8TYbIpEiABEiABEiABEiAB4xGg8BovJuwRCZAACZAACZAACZCAEwlQeJ0Ik02RAAmQAAmQAAmQAAkYjwCF13gxYY9IgARIgARIgARIgAScSIDC60SYbIoESIAESIAESIAESMB4BCi8xosJe0QCJEACJEACJEACJOBEAhReJ8JkUyRAAiRAAiRAAiRAAsYjQOE1XkzYIxIgARIgARIgARIgAScSoPA6ESabIgESIAESIAESIAESMB4BCq/xYsIekQAJkAAJkAAJkAAJOJEAhdeJMNkUCZAACZAACZAACZCA8QhQeI0XE/aIBEiABEiABEiABEjAiQQovDph3vj3uc4Wwm+eJV1SeHmZcOv+cwSHOLVpNqaDQMa0SfHw8UtYgxgUHRiduqklkQlpUybG3UcvndouG9NHQGISaA3Cs5dB+hqKsHW2DMmc2h4bIwESSFgEKLw6403h1QnQQzan8BovUBRe48VEekThNWZc2CsSSOgEKLw6RwCFVydAD9mcwmu8QFF4jRcTCq8xY8JekQAJABRenaOAwqsToIdsTuE1XqAovMaLCYXXmDFhr0iABCi8uscAhVc3Qo9ogMJrvDBReI0XEwqvMWPCXpEACVB4dY8BCq9uhB7RAIXXeGGi8BovJhReY8aEvSIBEqDw6h4DFF7dCD2iAQqv8cJE4TVeTCi8xowJe0UCJEDh1T0GKLy6EXpEAxRe44WJwmu8mFB4jRkT9ooESIDCq3sMUHh1I/SIBii8xgsThdd4MaHwGjMm7BUJkACFV/cYoPDqRugRDVB4jRcmCq/xYkLhNWZM2CsSIAEKr+4xQOHVjdAjGqDwGi9MFF7jxYTCG3VMgoNDMH/lNqzevB83bt1DmtQpUfrz/OjaqjbSpUmlNiz2bTtsmD8UWTKmN2Rwo+rf9x2H4tSfF2HyMiGRlxfefSs7vq9dHpW/Kh7tcdiCgrB97xFU/jr6daNtjCuQwGsIcB5encODwqsToIdsTuE1XqAovMaLiacL76NHj3D79m1YLBZkzpwZyZMndxrkoRMW4ZejpzGgWxN8/P5buHPvASbNXYvzF69h9ezBSJLY4jLhFdmWT9brXV4nvPWqlkGV8p/j8ZNnOHT8DIZOWIzGdcqjef1vXrvbP8//g3EzV2HmKB+93eP2JPBaAhRenQOEwqsToIdsTuE1XqAovMaLiScL78WLF3HmzJlwUAsXLowsWbLoBn3z9r+o2KAn1s4ZjDy5s4e1FxQUjBrNfdGw5leoV62sEt62japi5aZ9ePb8BZrVr4SmdSvCarXBN2AOfvvjbwQFB6Pgx+9icM/mSJokMfYfPKmE0WqzIUe2jPDv2QIZM6RFh77j8e5bb2L99p9Rt0pp7Nx/FOvnDQnbt+y3W5t6KJjvXfiPX4iTpy/AYk4U1hdZ8ecjpyCi7uXlpTKwC1buUG1EzEBLhlcTXm0HR078hfZ9xmLfmglImSIZNuw4gBmLNsJmC0LWzBkwol8bpE6ZHFWb9MV/T57ikw/zYM7YnpGulzWTMTPeugcGG3ArAQqvTtwUXp0APWRzCq/xAkXhNV5MPFl4t2/fDpvNFg5qhgwZULy4/kftm3cdxKylm7Fh3tBXgjZ57jpcvHIDY/06KOGtULoIBvk0w5Xrd1C9uS+2LhqB3/+8iJWbfsDs0T0QEgKMmb4C5UoWQs7smVC5cR8smtRXye38Fdtx/I9zmOjfGV36T8L9h/9h1ugeqsygRPVOWDN7MN7MmhHXbt5F3dZ+2L9uomrrwaPHGNG3NR799xR12vhh0pDOqr1ydbthSK8WKFE0H5Zv2Ish4xdh98oxDgmvHGjJ6p0wakA7fPhOLpSu3RUb5w9FjmyZ4Dd6PkwmYGD3ptix71es2bJfZXhl/1GtZ8zRzl55EgEKr85oUXh1AvSQzSm8xgsUhdd4MfFk4d28eXOkQCtXrqwb9NJ1e7Dnp2MqgxlxWbFhr5K+ueN6KeGdNcoHn+TNo1Zr0N4fTepWRMYMadB90FT4dW+GzwrlVeUPskjWdOueQ5gR0F39b8kKf1a5PU7snK3W/+j93GjVMLT/vYbOUKUUjWqXx/yV23Hh8g3492yO8vV9MHpAu7B9jpq2HCmSJUXFssXwXbvBOLxlmtr+ZaAVBcu3wp5VYx0W3lotB6Bdk2r4qmQhPH32AimSJ1VtbdlzCOu2/aQE3l545W9Rrac7CGwgwROg8OocAhRenQA9ZHMKr/ECReE1XkyO/eqFP/9IjNZtrXj2MsipHcyWIZlT24vYmCszvDv2HcGkueuweeHwV45BMryXrt7EmIHtlfBqWVhZsW2vsShXsiDqVC6txHDZ+j2QmteKZYqid8eGWL5+D6bMX4e0///Sm2zz5OlztR/Jxn5R9GO1rSx7fz6OxWt2KbFu1GkY2jWpis8Lf4zCFVsjVcrkSJQokVpPyiekffm/Hv7TsXvFmLA+F6rQGlsWj3BYeCXDO3FIZ3ya9x3VzwNHTqm2Hj1+hmyZM6gbAHvhlVrjqNZzafDZeIIgQOHVGWYKr06AHrI5hdd4gaLwGicm9+6ZMMTPjFXLzapTh49a8Wbu8OUBenvrauF1ZQ3vvfuPULaON1ZMH4gP380VhkIET8oWmtSpgFrffqmEd964Xsj7Xm61TsMOQ9C0XkV8/WXhsG3+e/IM3f2mKpnNkDY1du7/FZOGdnkFb9cBk1UpQu3KpdTfAgOtKF2rK5ZNG4BGnYZi7+pxMCdKpGqLxw/uiA/eyRmujQv/3FAZZi3DK9njIpXaOpzh/eXoH6qfP66fpPo4c/EmLJrUT9Xtbtr5i6otjii8kvmNaj2944fbkwCFV+cYoPDqBOghm1N4jRcoCm/cxyQoCJg/x4xRI8x4/J8Jb+YIxpSpIfiydJDHZXhV5vHRI9y6dUvN0iAvqzlzloYx01cq8ZOXzT7Nmwd37j3ElHnr8Pfl61g+bQAsFrMS3hqVSqJ3xwaqzrZa037YvjRAZUEf/fcE7ZtWV0HvHzAXeXJlU7MiVG/miyVTfJHrzcw49dclbNp5AH07f4+Iwivb+QyepkoTRJT9fJqqtkZOWYYXL16q2SNsQcEYO2Olmk7svTw5UKZWVwzv2xoli+XDvOXbMHbmSuxa8foaXqstCIeOnYHvyNno0KyGemFuydrdOPDrKUwd7o1Hj5+i28ApePr8hTpuyTzPWroFS6f4Qko/olov7kc7e+DpBCi8OiNI4dUJ0EM2p/AaL1AU3riNyaFfvNDbx4Lz57yQLBnQyduGdh2tyJQ+MQKtnim8riQaEhKiSgpWbvwBV2/eRZpUKdSLZzIPr2Q9ZSlSqQ06t6iFNVt+VNN7tWz4Lb6rXk69VNZvxGycu3AVJi8v5PvgLfUyWfJkScNmaXjxMlDVyPbr8j0K5nsvUuHd9eNR9e9zxvRUtcCySM2slD+c+OM8ZE7c0sXzo1fHBmrGBpHR4ZOXQvpe+9tS6sU1kevsWd4Ih8p+Hl4vkwlv5cyK5t99g2/LfabWu//wMdr3GadeSsuSKR28W9VBJ9+JSu6/r/U16rcdpGafWDPbP8r1hBMXEtBDgMKrhx4ACq9OgB6yOYXXeIGi8MZNTO7eMcHP14L1a0NrPitVDsLgoVZkyx6i/nfalBTeuIkM90oCJPA6AhReneODwqsToIdsTuE1XqAovO6NiczYNWu6BeNGmfH0KZArdzACxllRomRwuI5QeN0bF+6NBEjAMQIUXsc4RbkWhVcnQA/ZnMJrvEBReN0Xk59/8kIfHwsuXvCCfHzM28eGVm2tsCR+tQ8UXvfFhXsiARJwnACF13FWka5J4dUJ0EM2p/AaL1AUXtfH5MZ1Ewb6WrB1U2j5QtXqQfAbYkXmLKHlC5EtFF7Xx4V7IAESiDkBCm/MmYXbgsKrE6CHbE7hNV6gKLyui4k1EJg2xYKJY814/hx4O08wRo2z4rPPw5cvUHhdFwO2TAIk4FwCFF6dPCm8OgF6yOYUXuMFisLrmpjs2+uFvj0t+OeyF1KmDEH3XjY0b2WDOXSK3WgXZnijRcQVSIAE4oAAhVcndAqvToAesjmF13iBovA6NyZXr5owoI8FO7eHli/UrGPDgEE2ZMwUdfkCM7zOjQFbIwEScB0BCq9OthRenQA9ZHMKr/ECReF1TkxevgQmjTNj6iQL5L/ffS8Yo8dbUbho9OULFF7nxICtkAAJuJ4AhVcnYwqvToAesjmF13iBovDqj8n2rYnUnLpXr5iQKnUIeva1oUkzGxKFJnljtbCkIVbYuBEJkICLCVB4dQKm8OoE6CGbU3iNFygKb+xjcvmSSdXp7v8hEUwmoE59G3z9bMiQIWblC8zwxj4G3JIESMC9BCi8OnlTeHUC9JDNKbzGCxSFN+YxkRkXxo8xY8ZUC2Qmhg/zBmPU+EAUKKhfdLXeMMP7alzkk72flmsBiyX0zT/5/G7uHFnQvW09fFHk45gH0m6LTv0moHypIqhS/nMUrtgamxeNQJaM6aNsc922n9QnfWOy/PX3FfVJ4u1LA8Jtph1XtQpfYFifVuH+5jd6PlZt3oeTe+bAHMtHBgePnsZbubK+9nhkp8W+bYcN84e+sp588rhe1TKKjbbIsTTpMhyHt0yLCYJw69rvz57n51U7YPm0AciZPXO0bZ88cwHDJy3Bokn90KzriFf6KQ38cfYSfAZNe4V7tI3rXMEVcV24agfOX7oO/57No+ydNs42LRyORp2Gok+nhvg0bx6dR/O/zSm8OlFSeHUC9JDNKbzGCxSFN2Yx2bwxtHzh5g0T0qYLQa++NnzfxAYvr5i1E93aFN6ohXfPqrFKykQo9vx0DL4j52LbkpF4I32asI2CgoKRKJHjQbEX3n8f/Id0aVLBy8sUaZhCQkJQqmYX/LhuYnRhDPf31wlvsW/aIX261Ni8cDiSJLao7eT4qjTug9t3H+DItumxFt6OfSeg1feVo5WeuBLezG+kC8fTUeGVGFdt2hfjB3fEu2+9icjEXOP4+MkzFVNXLhHHnMTP2XGNifDKjdW5i9fQpf8kNa5icj68jhOFV+coovDqBOghm1N4jRcoCq9jMbnwtwk9vC04fDCRktv6DW3o29+GdOmdl9W17wmFN3rh1dao2aI/OjaviSwZ08F35BwlP7fvPcD88b2x/+BJjJu5ClabDTmyZYR/zxbImCEtrt+6B5/B03D/wX/46P3cePb8Bb4tV/yVDO/O/UcxYfZq9feiBT6Ef4/m6D54Gvb+fBx5cmfHjIDuePkyEANHz8O9+4+QPFlS9O3cEAU+fld1b9aSzVi+fi/SpE6BCqWLQjKZkWV4C1dsgwqli6DsFwXUerL8eOgkNuw4gJ37f8WJXbOV8EZ1PItW78TZC1cRGGjFlRt3IPI1YXBHbN17GJPnrYNIpU+7eviqZCGMnLJM9T84JARF83+AwT2bq7b1CG9U/ZKbg+j2N3TC4nA8a7Xsj07Na2LZ+r249+9DNK1XEa2/r/LKgNiy5xC27jmEKcO6qr9FJbz2Gd6oOGXL8gYuX70VZRwlDjMWbYTNFoSsmTNgRL82yJopPeav3I7zF6/hz/P/4MvPPkXXVrXD+inC62hcd+w7ginzN8Bms6kbNz+fZng7Z1a8DLTCd+RsnDh1HlkyZUDe93Lh+YtAleGNqr8Rb6za9hqDKuW/wLflPnPsYhfNWhRenRgpvDoBesjmFF7jBYrC+/qYPH0KjBlpwZxZZtiswMefBGP0uEDk+9Q1oqv1xijCO2iQ+8ds0qRAr16OC2+1Zv3g07YeMmdMjwbtByuprVS2mBLQyo37YNGkvkqC56/YjuN/nMNE/87o5jdVCbB36zoqC1a3jZ+SWfuSBi+TF0SmV8wYqDLKnXwnKJGtXbkUytT2xm+7ZqtO1mo5APWrl0WdyqVx6q9LkGzxruWjlXg27DBEZdcypEuN3kNnQh7BRya8Bb5uiQmDOykhnjS0i2q397CZKFeioCqDkJKGh4+eRHk8S9buxpT567Bx/jAlTIPGzEea1CmVgFVt2k8JkjzW/uGXExg3YxVWzRoEyV/XazsIrRpWwTflisVaeF/H2ZH9SUbbnqdkeL8p+xn6dfleSV31Zr44tGUakiUN/w1u74GTUbLYJ6j5zZcOC+/rOEUVx2fPX6J07a7YOH8ocmTLBCkzkZr9gd2bYsnaXZi2YCOWTeuv/ma/iPA6Ete79x6iRov+WDljoCrjWLlpH9Zu2Y/l0wdi+Ya92LL7IOaN743nz1+iQXt/5P/4XRXPqPp74Z8b4UpnpL1Dx05jrF8Hp5zMFF6dGCm8OgF6yOYUXuMFisIbdUzWrTZj8EAz7tw2IX36EPQdYFOZXfmxc/ViBOF99AhIm9bVR/pq+7LPBw+iF16rLQiSGRs2YTG2LQ3AnXsPUK/NIBzdPlOVI0hWTjKAkoWVRbK0n1VujxM7Z6NM7a7q3z98N5f623ft/dGgerlwwnvo2Bns/vEYJg8LFVDJrCXyMuHp8xdhgnbz9r+o0qQPjmydEVYCIfLco119XPznBn46fCps+5+PnMKQ8YuiFN7jO2bhq3rdsXHBMCRNkhiVGvbEtiUBKFi+lRLeLbtDM5qRHY+IkdTqan1dvGYXTp+9jOF9W4UTXsm4Pn/xUmWiZRF5y5Ylg8qgvi7De/7StbBSC9lOZE6yyFLD+zrOEofo9heZ8M4e3QN538ut+li8cnsl6G9mzRhuUHxVtxumDPfG+3lyxEh4I+PUuUXNKONYJP8HePrsBVIkD2UmmWW5MZE+ikDvP/gbZo7yeWXAasIbXVw3bD+A3T8dw7QR3qoNyepKzA9tngq/MfPxSd48aFKngvqbPK24//Ax2jepFmV/U6VMHk54JePbsd8E7F4xxiknM4VXJ0YKr06AHrI5hdd4gaLwvhqTM6dN6O1jwbFfQ8sXGjW1oXc/G1KncW1W174nRhBe6Y8RM7zaS2uJvLyQJ3c29OrQAIU+eQ8iZa18RmPfmvEK5bzl21TWM61d7eaTp89VxrVcnW7YumQksmd5Q63bttdY9cjXPsMrcnn+4nUljfbLg0ePw4RXHpd/126wyi5riwjeAO8m+OfaLZWh1F5Ek+xvj8GvvjylidGpvfMwYNRc5PvwbaRJlVKJ1NDeLfFR6aZKeBet2hnl8ezY9yt+P3MBI33bqG6IiGn/2z7DK7I0etpyJeNy53bj1j00qPEV2jau+lrh/abcZyhfqnDYMf596Tq6DJikhPd1nL28vKLdX2TCa//SWlQ1vSKFO5aNUiUqsjhS0mDPxZ5Tozrlo4zj118WVtwPHDml9vPo8TNky5wBc8b2VJxPnv4bAf3bRim80cVV+F3852a4cVaoQmusm+uPweMWqnGpvSA5d/lWXLpyC/WqlYmyv7nezBxOeO/ce4iKDXrg+M5ZTvkBovDqxEjh1QnQQzan8BovUBTe/8Xk8WMTAoaasWCeGUFBQP4CobMv5P3IfaKr9cYowmukEau99a69tBaxbyK8rXuMxg+rQ4V3085fVP2rViJgv37J6p0wa3QPfPBOTvXPdVr7oXHt8q9keLf/cBjTR4ZmiB89fgp5vJ00yf8ewd+6ex/VmvaLdLYCybqKJGn73/fLbxgxeWmUGV4Ro8Mn/sScpVsgWTp5VC+zT2jCu23P4SiPJyqREwG2F17J6AZararsQ15i6h8wV0l/dML7ulkaXsfZkf3FVngLlG+FnU4SXu82daKMo2R0Zy7epGaCSJ0yuRpX67f/HCa89jca9mPM/kbmdXGVDO+uH38NG2fyJEFmCjmydbqKT/6P8qDx/2d4R01djv+ePEOHZtWj7G/EGl4Kr5GuYgAovAYLiIu6Q+F1EVgdzVJ4gZAQYOVyM4YNMuPePRMyZgxBv4E21K7nnvKFyMJH4X2VSkyFV2pLpf5zyRRfSNZLMqybdh5A387fqzrbt3NlUzW8IiyNOw9TEmif4TXBpGYBWDrFF7lzZIXP4KnqMXv9amUhWcfDW6YjebIkqN1qIJrVr6QycZJBHT5pMQb5NMOV63fUVFlSopA+bWp0HzQVIiNR1fCK8AYHh6By494wmUyqHlekVBNeqeGN6nheJ7xS69m1VR2ULJZPZf7yf/SOehFM+tLZdyIqlimKbm3q6qrhjapfjuwvRbKk4XhGzOhGleGVLP3UETEvaYgqEx5VHNdt+xkHfj2FqcO91U1Pt4FTVFmLZKEjco9KeF8XVxmnctO0aqafquGVuuDtP/yqas9lVoZdPx7D3HG98OTpM1XDW/jTD1QN7+vGnf30d6qkoe947F45Vscvxf82ZYZXJ0YKr06AHrI5hdd4gUrownvqpJQvJMZvJ7zUl9GatrChRx8bUqVyf1bXfnRQePULr7SgzR7w4mWgqsGUF6EK5ntPlRr08J+Ou/8+VC9ziWCWKv6penRsPw/v9h+OYMLsNaH1v4XyqhfbEie2oKXPKJw5dxkzRnZX2Viptbx1575qp2ndCqhXraw6gElz12L15v1ImSKZmiN24eqdr9RS2mcCZZuAKcsQaLXBt2sj1YYmvPazNEQ8ntcJ79QFGzB/xTZ0aVlLCXufYbNgMSdSpRPlShRCv5Gz1eN0eakutvPwRsX5xB/nHdqfPc82vcaEm4c3KuHt5DsRZT7PH+6ltd9O/x2uxr7Fd9/iqy8Lhc3D+zpOMiYii6Oqme0zDo/+e4osmdLBu1UdyL5lrEg5hSMZ3ujiKiUpU+avh9VqU+USfj5N1Utw8kShz7CZkOOSWSGKFcyrxqyUyUTV34gZXpnH+cCRP9T0bc5YErzwbt59UL0VOqRXSzW1SmRL/XaD8df5f1TdkCzyaECbx5DC64xhaPw2KLzGi1FCFd7/HpkwdLAZSxeZERwMFCoShNHjrXjv/bgVXW2EUHiNd66wR8YiIGUFu348GjYtmbF6Z5zetOs9Dt+ULRbuwyF6epeghVfmoTt28qy662hW/5sohffbRr3VtCvvvJX9FdYUXj3Dz3O2pfAaL1YJTXilfGHJQjNGDDXjwX0TMmUOwYDBNtSoZTNUcCi8hgoHO2NAAjJLR9UmfTBpSJdIvcKAXXZ7l7QPT2xaOCzWHy6J2OkELbySPpdpQVp2H4W6VctEKbzyZRptPsOIACm8bj8P4mSHFN44wf7anSYk4T1xPLR84Y/fvWC2AC1a2dC9lxUpUhgvLhRe48WEPTIeASmZCJi6HAsn9lVlGlz+R0BuCOTTwr07NlB1285aErTwahBbdAt4rfDKG5VfFvsEx0+dU59Q7Na6rqqZkoXC66yhaOx2KLzGi09CEN5//zVhqJ9ZvZgmGd5ixYMwapwVed4xRvlCZKOCwmu8c4U9IgESACi8AF4nvPKGYv+AOap4vETRT/Dzkd/R0386Ni4YrgqxXwQGOXUcJU0ceqfn7Had2skE2FhiixdstmAEG9czElxUvEyA2eyFQGtwvDt2qc2dMd0LgwaZ8OghkD07MDIgGLVqG38AWsxe6o39ICefLNq1Md4FmwdEAiTgFgIU3miEN7IoNPceqd6urPx1cdx/HOjUQKVLmVi9G/fgSaDK6HAxBoE0KRLjyXOr03/EjXF0ntkL+XJUyqRmPHpm9cwDiKLXRw6b0L1rIpw57YXEiYF2HYLQo3cQkiXzjMNMkdQMW1AwXjr5RiR9qvCfZ/UMGuwlCZCAUQhQeKMRXpla49zFq+HqSGTew4Y1v1Y1vyxpMMpQdm0/WNLgWr6xaT2+lTTcvWNSnwNeu8qscHxRIggB46zI/ZZn3fmypCE2o5nbkAAJuJoAhTcK4ZXpyj4rmFfNW1iujjfGDeqIEkXzQb4pLp9Y3LxoBDKkS03hdfUINUj7FF6DBMKuG/FFeG02YM5MM8YGmPHkiQlv5gjBoCFWVPzWueVS7ooghdddpLkfEiCBmBBI0MIrX/v4+/J12GxBkG+bm7xMGNmvNSqULoova3RWkx3LRN8/HT6FUdOW4/bd+3gza0b07PAdDaXGWgAAIABJREFUihX4UHFmhjcmw81z16XwGi928UF4D/3ihR7eFly8EFq+0L6TFZ272ZAkifF4O9ojCq+jpLgeCZCAOwkkaOF1BmgKrzMoGr8NCq/xYuTJwnv7lgkD+1mwaUPoS6qlygRh5BgrcuT0rPKFyEYFhdd45wp7RAIkwFkadI8BCq9uhB7RAIXXeGHyROG1BgIzp1kwfowZz54BOXOFwH+4FV+V98zyBQqv8c4L9ogESCByAszw6hwZFF6dAD1kcwqv8QLlacK7b68X+va04J/LXkiaFOjkbVMlDFLKEJ8WZnjjUzR5LCQQfwhQeHXGksKrE6CHbE7hNV6gPEV4b1w3oX8fC7ZvDS1fqPhNkMrqZsvu+eULzPAa77xgj0iABJjhdckYoPC6BKvhGqXwGi4kMLrwBgYCUyZaMHm8GS9eALlyB2NYgBWly8a/D2XYjw5meI13rrBHJEACrOHVPQYovLoRekQDFF7jhcnIwrtrRyIM6GvBlX9MSJ4c6NrdhtbtrLDEs/IFZniNd16wRyRAAszwumQMUHhdgtVwjVJ4DRcSQ2Z4L18yqTrd/T+Eli9UrhqEwcOsyJwlfpYvUHiNd16wRyRAAhRel4wBCq9LsBquUQqv4UJiKOF9+RIYP9qM6VMskFKGt/MEY/hoK0qUjN/lCxRe450X7BEJkACF1yVjgMLrEqyGa5TCa7iQGEZ4t25KBL/+Fly/ZkLKlCHw7hGElm2sMId+ITjBLazhTXAh5wGTgEcQ4CwNOsNE4dUJ0EM2p/AaL1BxXcN74W8T+vhYcODn0PKFGrVsGOhvQ8ZMCad8gRle450X7BEJkAAzvC4ZAxRel2A1XKMUXsOFJM4yvE+fAuNGmTFrhgU2K/Due8EYMdqKzz5PeOULFF7jnRfsEQmQAIXXJWOAwusSrIZrlMJruJDEifCuX5sIgwdYIJ8GTpU6BD1629C0hQ2JQpO8XACwpIHDgARIwIgEWNKgMyoUXp0APWRzCq/xAuXOkoZzZ03o7WPB4YOJYDIBtevZ0H+QDRkyJOzyBWZ4jXdesEckQALM8LpkDFB4XYLVcI1SeA0XErdkeB8/NmHUcDMWzDXDZgM+zBtavlC4KMsXohoRzPAa71xhj0iABPjhCd1jgMKrG6FHNEDhNV6YXJnhDQkBVq8wY+ggM+7eNSFN2hD06mtDo6Y2eHkZj4WRekThNVI02BcSIAGNAEsadI4FCq9OgB6yOYXXeIFylfCeOW1Cj66J8dsJL1W+UL+hDf0G2JAuPcsXHBkFFF5HKHEdEiABdxOg8OokTuHVCdBDNqfwGi9Qzhbe/x6ZMGKoGYvmmxEcDHz8iZQvBKJAQYpuTKJP4Y0JLa5LAiTgLgIUXp2kKbw6AXrI5hRe4wXKWcIr5QvLFpsx3N+M+/dNKpPbt78N331vUxleLjEjQOGNGS+uTQIk4B4ChhNeW1AQjv9+Htdv3UWNSiUVhafPXiBF8qTuIRLDvVB4YwjMQ1en8BovcM4Q3lMnTfDxTow/fvdStbkNG9uU7KZOw6xubCNO4Y0tOW5HAiTgSgKGEt5LV26iXe9xuHf/IZ6/CMTpffNx/dY91G45ADMCuuOTvHlcySJWbVN4Y4XN4zai8BovZHqE98F9E4YONmP5EjMkw5u/QGj5Qr5PKbp6I03h1UuQ25MACbiCgKGEt6XPKHzy4dvo0LQGPinXXAmvLEvW7sLWPYexZIqvKxjoapPCqwufx2xM4TVeqGIjvFKbKzW6I4eZ8eihCW+8EYK+A22oW5/lC86KMIXXWSTZDgmQgDMJGEp4i1duj31rJyBJYgs+Kt00THittiAUr9wOR7fPdOaxO6UtCq9TMBq+EQqv8UIUU+E9esRLfTzizzNe6stoTZrZ0LOfDalSMavrzOhSeJ1Jk22RAAk4i4ChhPfzKh2wYf5QZMyQNpzwXrxyE406DcWBDZOdddxOa4fC6zSUhm6Iwmu88DgqvP/+a8LgAWasWRlavlCoSJD6eETejyi6rogqhdcVVNkmCZCAXgKGEt5BY+bj0tVb6NC0Opp2HYE1swfj7IWrmL5wIz4v/BH6ezfWe7xO357C63SkhmyQwmu8sEQnvEFBwLzZZoweYYZ8MS1jphAMGGRDzTo24x1MPOoRhTceBZOHQgLxiIChhPfFy0BMmrsWKzf+gGfPXyrMyZMlRf1qZdGxeQ1V6mC0hcJrtIi4pj8UXtdw1dPq64T30C+h5Qvnz3nBbAaat7LBp7cVKVLo2SO3dYQAhdcRSlyHBEjA3QQMJbzawYeEhODe/UcwmUx4I30adzOJ0f4ovDHC5bErU3iNF7rIhPf2LRMG9bdgw7pEqsPFioeWL7z3PssX3BVBCq+7SHM/JEACMSFgGOGV+Xe/rNEZG+cPM7zk2gOm8MZkuHnuuhRe48XOXnhtNmDmNAvGjzbj6VMgS9YQDPS3omr1ION1PJ73iMIbzwPMwyMBDyVgGOEVfp18J+KzgnnRsOZXHoOTwusxodLVUQqvLnwu2VgT3nWbrejjY8HFC16wJAZatbWiWw8bkiVzyW7ZaDQEKLwcIiRAAkYkYCjh9R05Bz8fOYXEFjNyZM+ExJbwNbvTRngbjiGF13AhcUmHKLwuwaqr0Tu3vDDINzHWrw/9/u8XJYIwfLQVed5h+YIusDo3pvDqBMjNSYAEXELAUMI7csoymBMlivL79d3a1HUJBD2NUnj10POcbSm8xolVYCAwbbIFk8aZ8fw5kP3NEAwaYkWlyixfMEKUKLxGiAL7QAIkEJGAoYTXE8ND4fXEqMW8zxTemDNzxRb79nqhV3cLrl31QuIkQLduIWjd8QWSJHHF3thmbAhQeGNDjduQAAm4moChhDcoKBiL1uzE1j2HcO3mXXXsObNnRs1vvkTdKqVdzSJW7VN4Y4XN4zai8MZtyK5eNcG3lwW7d4bOvlCqTBBGjbEhfz4L7j4KncKQizEIUHiNEQf2ggRIIDwBQwmvfGBi2fo9qFGpJHJky6R6eunqTazb9hPaN6luyJfZKLwJ45Si8MZNnF++BCaONasSBvnvnDlDMGiYFeUrBiG6D0/ETY+5VwovxwAJkIARCRhKeCt81wMT/Dvhg3dyhmP1+5kL6DtiNjYvHG44hhRew4XEJR2i8LoE62sb3b4lEQb6SvmCSZUsdOxqQ8cuViROHLoZhdf9MXFkjxReRyhxHRIgAXcTMJTwFqnUBgc2TEbiCF9UCwy04rPK7XF85yx384l2fxTeaBHFixUovO4L4+VLJvT0tuDAz6HlCxUqBamsbo4c4WdfoPC6LyYx2ROFNya0uC4JkIC7CBhKeOu1GYTaVUqhTuXw9bqrN+/H4jW7sH7eEHdxcXg/FF6HUXn0ihRe14dPZlwYO8qMWdMtsAYCuXIHY1iAFaXLBke6cwqv62MSmz1QeGNDjduQAAm4moChhPfIib/QuudovJUjC97KmRXyieFLV27hyvXbmODfGSWL5XM1jxi3T+GNMTKP3IDC69qwbVyfCIMHWHDzhkl9MKJLdxvatreqD0lEtVB4XRuT2LZO4Y0tOW5HAiTgSgKGEl450Nt3H2DTrl9w7cb/z9LwZiZULf+FYT83TOF15fA0TtsUXtfE4txZE3r7WHD4YGj5wrdVguA3xIps2aP/eASF1zUx0dsqhVcvQW5PAiTgCgKGE94Hjx5Dpid7I30adbz/XLuNlCmSIUO61K44ft1tUnh1I/SIBii8zg3T06fA6BEWzJ1lhs0GvJ0nWH0lrUTJyMsXIts7hde5MXFWaxReZ5FkOyRAAs4kYCjhPXj0NDr5TsCAbk1UVleW+Su2Y8r89Zg0tDM+K5jXmcfulLYovE7BaPhGKLzOC9GalWb4+5lx944JKVIA3j1saNXWCrM5Zvug8MaMl7vWpvC6izT3QwIkEBMChhLeGs19Ua9aWdSvVjbcMazc+AOWb9iLtXP8Y3JsblmXwusWzHG+Ewqv/hCcOR1avnDs19DyhWo1gjDQ34rMWaIvX2CGVz9/d7VA4XUXae6HBEggJgQMJbz5v26Jn9dPUiUM9ouUOZSp7Y3fds2OybG5ZV0Kr1swx/lOKLyxD8F/j0wIGG7GwnlmBAUB774XjBGjrfjsc8fLFyi8sefv7i0pvO4mzv2RAAk4QsBQwlu5cR+0bVwVlb8qHq7vS9buwooNP2DjgmGOHJNb16HwuhV3nO2Mwhtz9CEhwIplZgwbZMa//5qQKlUIfHrb0KylDYlCk7y6FpY06MLnso0pvC5Dy4ZJgAR0EDCU8O49cALd/Kbg/bdzIHvWjAgJCcaFf26GTks2uBNKFf9Ux6G6ZlMKr2u4Gq1VCm/MInLqpJQvJMZvJ7zUhrXq2tDfz4aMmWJXvhDZ3im8MYuJu9am8LqLNPdDAiQQEwKGEl7peMRpyXJkz4QqX3+OTG+kjclxuW1dCq/bUMfpjii8juF/cN+E4UPMWLbYjOBg4MO8oeULhYvqK1+g8DrG3whrUXiNEAX2gQRIICIBwwmvp4WIwutpEYtdfym8r+cm5QuLF5gxYqgZDx+YkDpNCHr2saFJcxu8QpO8Tl+Y4XU6Uqc0SOF1CkY2QgIk4GQChhHezbsOQrK5n+bNow5RpigLmLoM9+4/wteliqBv54YwO6Pwz8kAKbxOBmrQ5ii8UQfmxPHQ8oU/fveCyQTU+86GvgNtyJDBeeULzPAa9MSIpFsUXs+JFXtKAgmJgCGEd9n6PQiYuhxjBrRD2RIF8ejxU5Sv74MyXxRAvg/exszFm9Diu2/QuE4Fw8WGwmu4kLikQxTeV7HKi2hD/MxYtdwMyfB+/ImULwSiQEHXiq7WE2Z4XTLUdTdK4dWNkA2QAAm4gIAhhLdas35oWrcialQqqQ5R5t1dtHqnmpXBZDJhx74jmLl4M9bMHuwCBPqapPDq4+cpW1N4/xcpmVpswVyzmmrs8X8mpEsfgt79bGjY2KYyvO5aKLzuIh2z/VB4Y8aLa5MACbiHgCGEt1CF1tixbFTY54R7+k9H5ozp0b1tXUXh+q17kI9SHNk63T1UYrAXCm8MYHnwqhTe0OAdPeIFn64WnD/npWpzGzSyKdkV6XX3QuF1N3HH9kfhdYwT1yIBEnAvAUMIb5FKbbFtycgw4S1Xpxv6dW2Esl8UUDSu3byrhPfXbTPcS8eBvVF4HYAUD1ZJ6MIrnwEe1N+MdWtCv/+bv0Bo+UK+T90vutpwovAa88Si8BozLuwVCSR0AoYQ3lotB6D195VRoXRR/PrbX2jVY3S4L65JScOU+Ruwcf5Qw8WLwmu4kLikQwlVeG02YPYMC8aNSoQnT0zqRTR5IU1eTHNn+UJkQaXwumSo626UwqsbIRsgARJwAQFDCO+qzfsQMGU5vijyMY6c+BPVKpZArw7fqcM9evIseg6ZjnpVy6JNoyouQKCvSQqvPn6esnVCFN6ff/JCHx8LLl7wUl9Ga9zMpqYakynHjLBQeI0QhVf7QOE1ZlzYKxJI6AQMIbwShK17DuPwiTPIkysbGtT8KmwKsv4Bc2GzBWFwz+awmJ3wPVInR5zC62SgBm0uIQnv7VsmDOhrweaNoedboSJB6uMReT8yhuhqQ4TCa8yThcJrzLiwVySQ0AkYRnijCkRQUDASJXLRzPVOiD6F1wkQPaCJhCC81kBg+lQLJowx4/lzqM8Ay+eA5bPARlwovEaMCkDhNWZc2CsSSOgEDC+8Rg8QhdfoEXJO/+K78O7b64W+PS3457IXzGagaQsbfHrbkCqVsbK69tGk8DpnbDu7FQqvs4myPRIgAWcQoPDqpEjh1QnQQzaPr8J79aoJA/tasGNbaPlCseKh5QvvvW9c0dWGDIXXmCcPhdeYcWGvSCChE6Dw6hwBFF6dAD1k8/gmvIGBwOQJFkweb8bLl0DmLCEYMNiK6jWDPCQiAIXXmKGi8BozLuwVCSR0AhRenSOAwqsToIdsHp+EV7K5fv0suHLFBLMFaNXGCu8eNqRI4SHB+P9uUniNGS8KrzHjwl6RQEInYCjhffrsBdZu/REXr9zEy5eBr8RmWJ9WhosXhddwIXFJh+KD8F6+ZFJ1uvt/CC1f+KJEEIaPtiLPO8YvX4gsqBRelwx13Y1SeHUjZAMkQAIuIGAo4W3XexzOnLuMzwrmReLEllcO179ncxcg0NckhVcfP0/Z2pOFV2ZckJkXZAYGmYkhW/YQ+A2x4tsqnlO+QOH1lDOFszR4TqTYUxJIWAQMJbyFKrTGxgXDkD3LGx4TBQqvx4RKV0c9VXi3bEoEP18Lblw3wZIYaNveii7dbUiWTBcOQ2zMDK8hwvBKJ5jhNWZc2CsSSOgEDCW8X9f3wcoZA5EuTSqPiQuF12NCpaujnia8F/42qa+kHfg5tHyhVJkgDAuwIvdbnlm+wAyvruHr1o0pvG7FzZ2RAAk4SMBQwrvnp+PYe+A4vFvXwRvp0zh4CHG7GoU3bvm7a++eIrxPnwJjA8yYPdMCmxXIkTMEg4ZaUaGSZ5cvUHjdNdL174fCq58hWyABEnA+gTgX3sIVW4cdldlshtVqw4uXgUiS2AIvL1O4Iz66fabzCehskcKrE6CHbO4JwrtujRn+A82QTwMnSQJ06GxFx6429d/xcWFJgzGjSuE1ZlzYKxJI6ATiXHh/OnzK4RiULJbP4XXdtSKF112k43Y/Rhbec2dN8OlqwbFfQ8sXvq4QBP8RVuTIEX/KF5jhjdvxH5O9U3hjQovrkgAJuItAnAuv/YGu3LQPdauUfuXYZbqyFRv3onn9b9zFxeH9UHgdRuXRKxpReB8/NiFgmBkL5poRFAS8mSMYI8dYUbpssEezdrTzzPA6Ssq961F43cubeyMBEnCMgCGEV8oYrDYbvqzRGT+um/hKzy/8cxPNug4HSxocCyrXcj4BIwlvSAiwaoUZQ/3MuHfPpGZc6ORtQ7uOViRO7PxjN2qLFF5jRobCa8y4sFckkNAJGEJ4l63fgxGTlsImaaools8Lf4xZo30MFy9meA0XEpd0yCjCe+qkCb19EuO3E17qOCtVDsLgoVY1t25CWyi8xow4hdeYcWGvSCChEzCE8EoQnr8IxBdVO2Dp1P6vxCRpksTImT3zKy+xGSF4FF4jRMH1fYhr4f3vkQnD/M1YstCM4GAgV+5gBIyzokTJhFG+EFmEKbyuH/ex2QOFNzbUuA0JkICrCRhGeOVAAwPlkeyrX1hzNQQ97VN49dDznG3jSnilfGHpIjOGDzHjwX0TkicHvH1saNXWqj4kkZAXCq8xo0/hNWZc2CsSSOgE4lx4P6/aweEY/LJxisPrumtFCq+7SMftfuJCeE8cDy1f+OP30PKFqtWD1CeBM2dJeOULzPDG7fiPyd4pvDGhxXVJgATcRSDOhVc+NuHoUq5kQUdXddt6FF63oY7THblTeCWTO8TPjBXLzJAM79t5gjFqnBWffZ5wyxcovHE6/GO0cwpvjHBxZRIgATcRiHPhdfQ4O/efiIn+nR1d3W3rUXjdhjpOd+QO4ZXaXJliLGC4GVKzmzJlCLr3sqF5KxvM5jg9fEPunCUNhgwLKLzGjAt7RQIJnYChhPdloBVL1u7C6bOXVT2vttz99yGu3byHnzdMMly8KLyGC4lLOuRq4T16xAu9fSz480xo+ULNOjYMGGRDxkwsX4gqoBRelwx13Y1SeHUjZAMkQAIuIGAo4fUdOQfHfj+LEkXzYcOOA6j1bSmcPnsJz56/xJBeLfDBOzldgEBfkxReffw8ZWtXCe/dOyb4+5mxZmVoCvfd94IxerwVhYuyfCG6sUHhjY5Q3Pydwhs33LlXEiCB1xMwlPB+Ua0jVs7wQ/Ysb+Cret2xe8UY1fuxM1YiTeqUaPEdv7TGAR03BJwtvDLl9JyZZowNMEO+mJY6TQh69LGhSTMbEoV+IZhLNAQovMYcIhReY8aFvSKBhE7AUMJbqEJrHNg4GTLvrgjvruWjYTKZVHlDhQY98MPq8XEWr827D2LQmPkY0qslKpQuEtYPZnjjLCRu3bEzhffQL6HlC+fPecFkAurUt8HXz4YMGVi+EJOgUnhjQst961J43ceaeyIBEnCcgKGEt2GHISiY7z10al4DzbxHon61sqhS/nOcv3QN33ccisNbpjl+ZE5cc/7K7Th28iyklrhZ/W9cJryzJgeoXnt5eeH58+fo2P3Vj3A48bA8vil7Xi9fvkR7734uOyZnCO/tWyb4+VqwcX1oCvfDvMEYNT4QBQrGL9GdPMYfyZIlQ7C8hQegVceeLokLhdclWHU16srYZ8uQTFffuDEJkEDCJmAo4T311yV07T8Jq2cPwrHfz6Gb3xSkTpkCj588Q92qpdGvS6M4idZff1/B+3lyoGX3UahbtYxLhFeTN/sDFPFt0d54n1OOkyBE2GlkvFwpV3qE1xoIzJpuwbjRZjx7BqRNF4Le/Wxo2NgGr9B31OLNMmfq6DDRtT8oV0gvhddYw8bVsafwGive7A0JeBoBQwmvwAsJCVFlDLJcunITp/66iCwZM6BogQ/inG2LbgEuE97ZU0apY4+4uEIU4hykEzowY+IIlQmPuEim1xWZ8dgK788/eaGntwX/XPZSclu/oQ19+9uQLn38yupqcYjsRkTO55Ydejgh6uGboPA6HamuBl0dewqvrvBwYxJI8AQMJ7wSEVtQEG7ffaBeXjPSEpnwBgU7R1yG+A+O9FB9+w8wEgLD9GXoEP9IbxCkg65g5uVlQkhwCByN9tWrgHdXYP360Ju3AgVCMGMmUNB4305xakzdOY6FrMh0cCQ3ik49KDbmEAFXxz6RV+i5xIUESIAEYkPAUMIrpQvDJi7Blj0HERQUjNP75uP+w8fo4T8NAb5tkSFd6tgco9O2iUx4bz944ZT2Z0wKiFTg2nbu5ZT241sj0yeOjPSQrFYrOnX3dfrhZkidBI+eBsIW9HrlDXwJTJlkxoSxZrx4AaTPEIJ+A4LQ4HubekEtvi+RxUWktE0n59fxmhOZkCaFBf/+FxjfsXrE8bk69pnTJfUIDuwkCZCAMQkYSnhlHl55Max90+po0N5fCa/MwTt43AK8eBGI8YM7xilFV5Y0uLr+LU7BuWDnU8cNhcViCdeyqx6dy04cKWnYvTMR+vex4Mo/JlW+0KipTdXqypRjCWVxZ201SxqMNapcHXuWNBgr3uwNCXgaAUMJb6maXbB+3hCkS5MKH5VuqoRXlv+ePEOF+j44uHlqnPJ1pfDKgckPhvZmu9ls5gtr0URb3gi3l942nXu7bHy8TnivXjGhV3cL9v8QOvtC/gKhsy/k/SjhiK49eLl5s9ls6p+kztpVdegUXpcN91g37MrYU3hjHRZuSAIkICVwIZG9KRVHaGQe3p83TEaypInDCe/DR0/wVb1uOLp9Zpz0rHargfj78nXYbEFI5OUFk5cJI/u1RoXSRcF5eOMkJG7faWTC+/IlMGGMGdMmWxAYCGTMGIJ+A22oXS9hlC+4PQgRdkjhjesIRL5/zsNrzLiwVySQ0AkYSnjb9ByDPLmywbt1HeT/uqXK8N68/S+GTVwMW1Awpo3wNly8KLyGC4lLOhRReLdtToSBvhZcv2aC2Qw0aW5TX0pLlSphZnVdAj2aRim8cUE9+n1SeKNnxDVIgATcT8BQwnvt5l019+65C1dhtQUhZYpkePL0OfJ9+DbGDmyPbAabtUHCReF1/6CNiz1qwvvXWaCPjwUHfg4tXyhUJAijx1vx3vsUXXfHhcLrbuKO7Y/C6xgnrkUCJOBeAoYSXu3Q5QMUV67fhpfJhJzZM+Oj93O7l0oM9kbhjQEsD141ReKk8PUNwvRpZtisQKbMIRgw2IYatUJrVbm4nwCF1/3MHdkjhdcRSlyHBEjA3QQMKbzuhqBnfxRePfQ8Y9sN6xJhiF9i3LgOmC1Ay9ZWdOtpQ4oUntH/+NpLCq8xI0vhNWZc2CsSSOgEDCG8Uq/ryPLbrtmOrObWdSi8bsXt1p2dO2tCbx8LDh8MLV/4rHgwAsYFIs87LF9wayCi2BmF1whReLUPFF5jxoW9IoGETsAQwrt1z+Fwceg5ZDq6t6mLzBnTh/v3b8oVM1y8KLyGC4nuDj1+bMLoEWbMn2OGzK6VNVsIxo41oVyFF7BG8+EJ3TtnA//X3pnH2VT/f/x1mRkKEYXSov1bvlq0CCmVqCyVUijZsmTf930d2WUrFJVCK5G0+1ZUfFPf9vq2SRvZScydmd/jnPmZL5maO3POufd93ef9q3LO+/P+PF8fes7H554TMQGEN2JUUb0Q4Y0qbgaDAAQiJGBCeP/ca8VrWuiZuSN01mknRTiN2F2G8MaOfRAjP7UoSSOHJWnzppCSU6Q296apW8+wTjmhsLbv2ofwBgE9nzUR3nyCC/g2hDdgwJSHAATyRQDhzRe2/92E8HoEaOT2Tz/JOr7w77VZxxeqXZGu+yalqfxpWccXInnTmpGpJEwbCK/NqBFem7nQFQQSnQDC63EFILweAcb49p07Qho7OkmPzktSerp00smZGjYyTdfXST+kM4Q3xkHlMDzCay8TpyOE12YudAWBRCeA8HpcAQivR4Axuj0zU1q4IEmjhydp69aQUlKk9p3S1Ll7WIUKHd4UwhujoP5mWITXXiYIr81M6AoCEDDyauGHF644JIsJDyxWs4a1dVzJ4of89xaNbjCXGcJrLpJcG/row5B6dkvRx/8p4F57ba10jUpN08mn/PXTFxDeXLFG/QKEN+rIIxqQHd6IMHERBCAQZQImdnjrNO0b0bSXP5oa0XXRvAjhjSZtb2Nt2xrS6BFJ7s5uRoZzfCHDFd2atTNyLYzw5ooo6hcgvFFHHtGACG9EmLgIAhCIMgETwhvlOfs6HMLrK85Aijly+9j8JKWOStKO7SEVLix16hZ2jzA4Rxki+SC8kVCK7jUIb3R5RzoawhspKa6DAASiSQDh9Ugb4fUp0lRHAAAgAElEQVQIMODb178fUq+uKfrs06zjC86X0UaMTtOJ5fL28giEN+Cg8lEe4c0HtCjcgvBGATJDQAACeSaA8OYZ2aE3ILweAQZ0+5YtIY0YkiTnubrOF9ROLe+8JS1NV1TP/fhCTi0hvAEF5aEswusBXoC3IrwBwqU0BCCQbwIIb77RZd2I8HoE6PPtzqPFnDekjUtN0q6dIR19tNS1R9h9gYTzIon8fhDe/JIL7j6ENzi2XiojvF7ocS8EIBAUAYTXI1mE1yNAH29/Z3UB9+URX32ZdXyhbv10DR+dpjJl83Z8IaeWEF4fg/KpFMLrE0ifyyC8PgOlHAQg4AuBmAvvV99ujHgiFl81jPBGHF9gFzqvAR46MFnPPZP1lrTTz8jQuElpurxq/o4vILyBReVrYYTXV5y+FUN4fUNJIQhAwEcCMRfeCjWaRzydT96YF/G10boQ4Y0W6cPHCYel2bOSNWlckvbskYoWzVS3Xum6p22akpL87YsdXn95+lEN4fWDov81EF7/mVIRAhDwTiDmwrtz9+8RzSItLaxSxx4T0bXRvAjhjSbt/4311psF1K9nsr75Ouv4wi23hjVkRFjHl/Z+fIEd3thkmtdREd68EovO9QhvdDgzCgQgkDcCMRfeSNrdvWevnJdTrHpmSiSXR/UahDequPXTjyENGZisF57POr5w1tkZGj85TZdc5t/xBYQ3upnmdzSEN7/kgr0P4Q2WL9UhAIH8ETAlvD/8tEmjpjymT7/8Tvv2p2XPaO8f+3TGqSfq2YdG5m+WAd6F8AYI96DSafulmdOTNXVikvbulYodk6lefcNq3iqsglnuG+iHIw2B4s1XcYQ3X9gCvwnhDRwxA0AAAvkgYEp47+k5TkWOKqw6NatoxKT5GtarpT754lu9vfZjzUztpmOLF8vHFIO9BeENlq9T/Y3XCqh/72R9/10BhULSbXeENWhYWKVKBXN8IacZIbzB55zXERDevBKLzvUIb3Q4MwoEIJA3AqaE99Ib2uq1JyepWNGjVfOOHnpl0QR3NivfWKtVaz7Q6H6t8za7KFyN8AYH+YcfQhrUN1kvr8zawj33vAyNm7xfF1WKnugemB3CG1zO+a2M8OaXXLD3IbzB8qU6BCCQPwKmhPfyuu21YsFYdye3VqOeWvbIGKWkJCszM1NV6nXQO8tm5G+WAd6F8PoPd98+6f5JSZpxf7Kcfy5eIlN9+ofVtHlYBbK+oxb1D8IbdeS5Dojw5oooJhcgvDHBzqAQgEAuBEwJb5dB92vX7t81dWRn9R45S2VLl9Kdt1yrf3/0laY//CxfWkuA5fziCwXdZ+r+sCHkHl9odGdYAwaHdWzJ6O/qHowb4bW3+BBee5k4HSG8NnOhKwgkOgFTwrt1+y6Nnfa4BnW7Wz/+8ps69JuknzdtVaGUZA3u3kw3X3+FubzY4fUnku++DbnndFe9nnV84Z/nZ2j8pP2qeEFsRffA7BBef3L2swrC6ydN/2ohvP6xpBIEIOAfAVPC++dpOUcZftm8TSWOKaKkggWVnOzz2wR84IjweoPoPHFh8oQkPTAjWc6TGJyd3P6Dwmp8V9jd4bXyQXitJPG/PhBee5mww2szE7qCAAQkU8J78BfVDg5n+47duqnFAI40HGEr9vklBTVsULJ+/inkns298+6wK7vHFLexq3swboTX3uJDeO1lgvDazISuIAABI8LrPHZs9dqP9djTL+uuW687LJcfft6k99Z/zpfWjpAV+/V/Q+rVLVnvrsk6vnDhRVlPXzivgj3RPYAc4bW3+BBee5kgvDYzoSsIQMCI8H793Y96/uU1mvvEctWocuFhuRQunKJ611XVlZdfYC4zjjREHsmePdKEscma+2CSwmHpuOMyNWBoWA3vsHV8IacZIbyR5xytKxHeaJHO2zic4c0bL66GAASiQ8DUkYZpDz2rji1vic7MfRoF4Y0M5LNPJWn4kCRt+jXkvhmtWYuweg8Iq1gxu7u6B88M4Y0s52hehfBGk3bkYyG8kbPiSghAIHoETAmvM+3P/7tB3274WX/s238YhVtuqB49MhGOhPD+PahPPwmpb89k/Xtt1vGFiy9N1/jJaTr7nPgQ3QOzQ3gj/A0RxcsQ3ijCzsNQCG8eYHEpBCAQNQKmhHf8rEWat+hFlTnu2ByfyPDi4/dFDUykAyG8OZPatSuk+0Ylaf7DSUpPl0qXydSgoWE1aBiOFK2p6xBeU3G4zSC89jJxOkJ4beZCVxBIdAKmhPeqBl00Z0IvnXXaSXGTC8J7aFSZmdLihUkaPSxJv/0WUlKS1LJ1WD37pqlIkbiJ9bBGEV572SG89jJBeG1mQlcQgICRL60dCKJO075a/mhqXOWC8P4vro8+dI4vpOiD9Vnv/61cJV3jJqXpjDPj6/hCTgsQ4bX32xLhtZcJwmszE7qCAASMCe/Y6U/oon+epVpXXRI32SC80ratIaWOStLjjyYpI0Mqe0KmhoxIU/2b0+Mmx9waRXhzIxT9X0d4o888khE50hAJJa6BAASiTcDUkYYBqXP00qp1OrFsKZ1QupRCf3rV1szUbtHmk+t4iSy8zvGFBY8kubLrSG9yitS6XZq69wrrqKNyRRdXFyC89uJCeO1lwg6vzUzoCgIQMLbDO27mQhV0Xrn1F5/ubW83l1miCu/697OOL3z8n6y8ql2Rrvsmpan8afF/fCGnRYbwmvutx5fW7EXidsQOr9FgaAsCCU7A1A5vPGaRaMK7ZUtIo4YmuV9Mc3Z4y52UqWEj03RD3SPn+ALCGx+/E9nhtZkTwmszF7qCQKITMCW8GRmZWrLyLT334lv68Zff9MqiCe7zeOcvXqlWTW5UkvPGAmOfRBFe52zuvIeSNG5MknbuCCklRbq3Y5q69AirUCFjoQTQDju8AUD1WBLh9QgwoNsR3oDAUhYCEPBEwJTwznl8uRY+96ruuOkaTZ79lD55Y55+27pDbXqNV7VLK6pHO440eEo7nzeve6+A+/KIzz7NOr5w1dXpGjshTSefcmQeX2CHN58LJcq3IbxRBh7hcAhvhKC4DAIQiCoBU8J7fZPemj66i84oX04VajR3hdf5/PDTJt3VcZRWPTMlqnAiGexI3uHdvCmkYYOT5LwW2Pmcckqmho9J03W1j+zjCwhvJCs/9tcgvLHPIKcOEF6budAVBBKdgCnhvahWa61dMcs9unCw8DrHGqrU66D1L802l9eRKLzhsDT3wSRNvC9Ju3eHVLiw1KFLWB27pLlHGRLxw5EGe6kjvPYycTpCeG3mQlcQSHQCpoT31nsGq/WddXX91ZdlC29mZqZmL1jmPq7sqdnDzOV1pAnvO6sLqFe3ZH3zddbxhdo3pGtkappOLJc4xxfY4TX32yzHhhBemzkhvDZzoSsIJDoBU8K7Zt0n6jzoflU4p7zWfvC5rql2kb78ZqO2bt+paaO7qvJF55rL60gR3l9/CWnIgGQ9vyTri4Gnls/Q6PvSVOOaDHPMY9EQO7yxoP73YyK89jJhh9dmJnQFAQgYew6vE4jzJbWlL72tDRs3KVQgpFPLlVH92tVUskQxk3nFu/Cm7ZcemJGsKROT9Pvvcl8Y4Tx5oV37NPdFEnyyCCC89lYCwmsvE4TXZiZ0BQEIGBTeeAslnoX3jdcKqH/vZH3/XdbxhTr10jViTJrKlE3s4ws5rUGE197vTITXXiYIr81M6AoCEDAivFPmPB1RFl3uuTWi66J5UTwK708/hjSoX7JefCHr+MLpZ2RozPg0XVGd4wt/tXYQ3mj+ropsLIQ3Mk7RvoozvNEmzngQgEAkBEyc4XWeyFCieFFV+udZKlQoRc4X1XL6TBjSPpI5RfWaeBLe/ful6VOTNW1ykv74QypSROrWK6zW7dKUlPXkMT5/QQDhtbc0EF57mTgdIbw2c6ErCCQ6ARPC++Lr77nndp0vqlW5pILq16qmqy6/QMnJ9i0sXoT3pRcLakj/ZG3YEHLX/M0N0jV0ZJqOL83xhUj+EEB4I6EU3WsQ3ujyjnQ0hDdSUlwHAQhEk4AJ4T0w4a3bd2nFa+9o6crV7ssmrr+msurXqqoLK5wZTSZ5Gsu68H73bcg9p7vq9azjC2ednaHU8Wm6vCrHF/ISNMKbF1rRuRbhjQ7nvI6C8OaVGNdDAALRIGBKeA+e8LcbftbzL6/WC6++q1BIqlermto3uykaTPI0hlXh3bdPmjw+SbOmJ8s5ylCsWKZ69g2rxT1hFcxyXz55IIDw5gFWlC5FeKMEOo/DILx5BMblEIBAVAiYFV7nHO+76z/Ty6vW6YVX39FJJ5bWkw8OjQqUvAxiUXhfeL6ghg5K1o8bQ+4PCw0ahjVkRFilSnF8IS/ZHnwtwptfcsHdh/AGx9ZLZYTXCz3uhQAEgiJgTnidnd0lK9/WspdXKz0jQ3VrVtXN11fTGeXLBcXAU11Lwvv1f0Pq1zNZb7+VtYV77nlZxxcuuYzjC55C5jm8XvEFcj/CGwhWz0URXs8IKQABCARAwITw7ti5Rytef1fPvfiWvvpmo2pWv9h92USViyuoQIGsL1lZ/VgQ3j17pEnjkjT7gWSF06Rjimeqd7+wmrUMq0DWI3b5eCTADq9HgAHcjvAGANWHkgivDxApAQEI+E7AhPBeWLOVihY5WtUu/aeuuKyijj6qcI4TvbZ6Jd8BeC0Ya+F97pmCGj44Wc6rgZ3jC3c0Dmvg0LCOLcnxBa/ZHnw/wusnTX9qIbz+cPS7CsLrN1HqQQACfhAwIbx1mvaNaC7LH02N6LpoXhQr4f3yi5D69kzWu2uyji/883zn+MJ+XVQJ0Q0if4Q3CKreaiK83vgFdTfCGxRZ6kIAAl4ImBBeLxOI9b3RFt5du0IaNyZJ8x9KUjgsdye374Cw7rw77O7w8gmGAMIbDFcvVRFeL/SCuxfhDY4tlSEAgfwTQHjzz869M1rC67x87qlFSRo1LEmbN4fcs7lNmoY1YHDYPbPLJ1gCCG+wfPNTHeHND7Xg70F4g2fMCBCAQN4JILx5Z3bIHdEQ3k8/CalX1xR9sD7rG2gXXpR1fKHiBYiux/givh3hjRhV1C5EeKOGOk8DIbx5wsXFEIBAlAggvB5BBym827eHNGZkkh6bn6SMDLnP0e0/JOx+MY3jCx6Dy+PtCG8egUXhcoQ3CpDzMQTCmw9o3AIBCAROAOH1iDgI4Q2FQpo4JU2jRyRp69aQ+2a0u1uE1WdA2H1jGp/oE0B4o888txER3twIxebXEd7YcGdUCEDg7wnEXHidVwdH8gmnh1W/VrVILo3qNX4L797thdWoUUjr1mVN4+JLM3TfxDT941xeHhHVYP80GMIbS/o5j43w2svE6QjhtZkLXUEg0QnEXHivvq1rdgbOzua2Hbu1f3+aihcronB6uvb8/oeOKpyiE8scp6XzR5vLy2/hLZJUWOXLh5SSkuk+T/fW28Pm5pyIDSG89lJHeO1lgvDazISuIAABKebCe3AIi59/Q19+/YM6tWyg4scUcX9p85btmvDAYl18/tlqWLeGucz8Ft6yxxbW6tUhlTn5DxUpyvEFK4EjvFaS+F8fCK+9TBBem5nQFQQgYEx4a9zaVS88lnrYm9a2bt+lW1oO1KpnppjLLAjhdV6n/MvWvcrAd83kjfCaiSK7EYTXXiYIr81M6AoCEDAmvFXqttcTMwer/MllD8nmi69/UPMuY7Rm2QxzmSG85iIJpCGENxCsnooivJ7wBXYzZ3gDQ0thCEDAAwFTRxqGTZyvVWs+UL3rqurEMqXkbHD+9MtvWvrSal1xWUWN7NPKw1SDuRXhDYartaoIr7VEJITXXibs8NrMhK4gAAFjO7xp4XQtXvqaXlq1Tpt+26b9+8M6/rgSuvLyC3RP4xuVkpJsLjOE11wkgTSE8AaC1VNRhNcTvsBuZoc3MLQUhgAEPBAwtcPrYR4xuxXhjRn6qA6M8EYVd0SDIbwRYYr6RQhv1JEzIAQgEAEBU8KbkZGpJSvf0nMvvqUff/lNryyaoD/27df8xSvVqsmNSnLewGDsg/AaCySgdhDegMB6KIvweoAX4K0Ib4BwKQ0BCOSbgCnhnfP4ci187lXdcdM1mjz7KX3yxjz9tnWH2vQar2qXVlSPdrfne6JB3YjwBkXWVl2E11YeTjcIr71MnI4QXpu50BUEEp2AKeG9vklvTR/dRWeUL6cKNZq7wut8fvhpk+7qOIrHkiX6ao3h/BHeGML/i6ERXnuZILw2M6ErCEDA2JfWLqrVWmtXzHKPLhwsvM6xhir1Omj9S7PNZcYOr7lIAmkI4Q0Eq6eiCK8nfIHdzA5vYGgpDAEIeCBgaof31nsGq/WddXX91ZdlC29mZqZmL1jmPrnhqdnDPEw1mFsR3mC4WquK8FpLhCMN9hLJ6gjhtZoMfUEgsQmYEt416z5R50H3q8I55bX2g891TbWL9OU3G7V1+05NG91VlS8611xaCK+5SAJpCOENBKunouzwesIX2M0Ib2BoKQwBCHggYEp4nXk4X1Jb+tLb2rBxk0IFQjq1XBnVr11NJUsU8zDNnG/d8OMm9R8zW5999b3KlT1Ow3u31IUVzjzs4kb3DtfnX30vhULurx1T9Gj969mp7j8jvL7HYrIgwmsvFoTXXibs8NrMhK4gAAFjZ3gXP/+Gbq9X47Bc9vz+hxYtfU0tG93oa2ZNO41yn/7Qqkkd9w1vo6c+ppVPjFdy0qGPP6vTtK+mDO+kM08rd9j4CK+vkZgthvDaiwbhtZcJwmszE7qCAASMCG9aWlhp4bCuvKVz9s7pweF8/f3PatF1jNa9+KBvmW3ZtlPXN+mlNctmZD/f97bWQ9SnQ2NdeuE/DhnnqgZdtOiBISp7fEmE17cE4qsQwmsvL4TXXiYIr81M6AoCEDAivE8896pS739c4fT0v8yk6iX/1OzxPX3L7P2PvtLwifP13MMjs2v2HD5TlSudq4Z1D91ldp4ecWXl8/X+R1+q5LHHqHub23VVlQvc+37Z+odvPTmFSh9bWAVC0qZtfygj09fSFPNAoFTxQtq5e7/S0gnFA0Zfb3WE95giydqyc7+vdSnmjUDxosnan5ahvfv++s/z/IxQtmTh/NzGPRCAAARcAmbO8O79Y7+q1e+gx2cMOiyawoVSdEq5MirgmKBPn9XrPtaU2U+7O7cHPgNS5+jsM05Ws4a1s/+b8/a3QffNVc0rL9YVl52vt977j3qPmKWl88fohNIllZHprwAV+P9zwn7X9QlbwpZxciETe/GTi71MQnL+nM6Uv38ySgf+bLQ3YzqCAATigYAZ4XVg7d+fpj17/1B6eoaOK1nc5ff9xl9VtMhRKnXsMb7yXP/xVxo4dq6WP5qaXbfzoKmqXvn8w3Z4/zxwy25j1eDGK1X3uip8ac3XVOwW40iDvWw40mAvE6cjntJgMxe6gkCiEzAlvM5jyToNnKLB3Zupfq1qbjbzFr2o6fOe0/2jOuvySuf5lte2HbtU8/YeenvpNDk7yM7H+XLaiN4tVani2dnj/L53n7785odDnt5wd+fRurPBdapd41KE17dEbBdCeO3lg/DaywThtZkJXUEAAoaONDhh3NJyoO646Ro1uumaQ7JZvPR1LVzymp6ZO8LXzFr1uE8Xn3+O+7KLlW+8pylzntaKBWPdL7Ete2WNK9gpKcm6tmE3TRrWUVdcVlFvvfeReg2fqWWPprq7zjylwddIzBZDeO1Fg/DaywThtZkJXUEAAsaE98Lr7tFbz93vHmE4+OPsxl59Wzd98PIcXzP7+dct6jPqAX3yxXc6+cTSGtX3HvelF87HeWLE5OEd3d3eN9/9SONmLtSvm7fqpBOOV+8OjbNfgoHw+hqJ2WIIr71oEF57mSC8NjOhKwhAwJjw1r27n9rdXV91a1Y5JJsFz7ysRUte19L5o81lhvCaiySQhhDeQLB6KorwesIX2M2c4Q0MLYUhAAEPBEyd4X3t7fXqPnS6zjn9ZJU74XhlZmbIeQbvhh9/dV/8cOBRYB7m6/utCK/vSE0WRHjtxYLw2suEHV6bmdAVBCBgbIfXCeTXzdv0/MurtfGnzW4+J5crrXrXVVXp40qYzAvhNRmL700hvL4j9VwQ4fWMMJAC7PAGgpWiEICARwKmdng9ziUmtyO8McEe9UER3qgjz3VAhDdXRDG5AOGNCXYGhQAEciEQc+GtWr+Dpo3qqkoVz5Lzz3/3Wb10urlAEV5zkQTSEMIbCFZPRRFeT/gCuxnhDQwthSEAAQ8EYi68r775viqdf5aOLV5Mzj//3efa6pU8TDWYWxHeYLhaq4rwWktEQnjtZeJ0hPDazIWuIJDoBGIuvPEeAMIb7wlG1j/CGxmnaF6F8EaTduRjIbyRs+JKCEAgegRiLrzDJsyLaLZp4XSN7NMqomujeRHCG03asRsL4Y0d+78aGeG1lwk7vDYzoSsIQMDAUxr6jHwgO4eMzAy9sfoD9+UOp5Qro/T0DH2z4Sdt2bZTN157uYZ0b2YuM4TXXCSBNITwBoLVU1GE1xO+wG5mhzcwtBSGAAQ8EIj5Du/BvQ+f9IguOO8M3VS72iFTeuTJlfp+468a1O1uD1MN5laENxiu1qoivNYS4QyvvUSyOkJ4rSZDXxBIbAKmhPeyG9vp7SXTlJycdEgqu/fsdV8tvHbFLHNpIbzmIgmkIYQ3EKyeirLD6wlfYDcjvIGhpTAEIOCBgCnhvbZhdw3t2VzVK59/yJReefPfGjN1gV59cqKHqQZzK8IbDFdrVRFea4mww2svEXZ4rWZCXxCAgIEzvAeHsHDJaxo5+VFVPPd0nVimlDIzpZ9+/U0fffaNendorGYNa5vLDOE1F0kgDSG8gWD1VJQdXk/4AruZHd7A0FIYAhDwQMDUDq8zjy+/2aiXV63Vr79t0/60sEqXKqErL79Al1xwjodpBncrwhscW0uVEV5LaWT1gvDay8TpCOG1mQtdQSDRCZgTXieQcHq6ft28TeXKHmc+H4TXfES+NIjw+oLR1yIIr684fSuG8PqGkkIQgICPBEwJ767dv2v01AVa/uoa95Fkn7wxT1u371KvETN138B2KnXsMT5O3Z9SCK8/HK1XQXjtJYTw2suEHV6bmdAVBCBg7AzvwLFztXnLdrVvfrOatB/hCu/ve/dp+KT5+uOP/Zo8vKO5zBBec5EE0hDCGwhWT0URXk/4AruZHd7A0FIYAhDwQMDUDu9VDbrouYdH6tjixVShRnNXeJ3Pzt2/q3ajnlqzbIaHqQZzK8IbDFdrVRFea4lwhtdeIlkdIbxWk6EvCCQ2AVPCe3HtNnpryTQdVTjlEOHdvmO3at7RXetefNBcWgivuUgCaQjhDQSrp6Ls8HrCF9jNCG9gaCkMAQh4IGBKeNv2nqAzTj1R3do01IXX3ePu8P786xaNnvqYwukZmpnazcNUg7kV4Q2Gq7WqCK+1RNjhtZcIO7xWM6EvCEDA2BnejT9vVveh0/Xl1z8oLZyuokWOkvOWNee5vBOHtNeJBp/agPAmxm8jhNdezuzw2suEIw02M6ErCEDAmPAeCOSjz7/Vhh9/VYFQSKeUK6MK55Q3mxXCazYaXxtDeH3F6UsxhNcXjL4X4UiD70gpCAEI+EDAzJEG59m7V97SWUvnjdZxJYv7MLXolEB4o8M51qMgvLFO4PDxEV57mbDDazMTuoIABIzt8HYaOFWXVzpPdzaoGTfZILxxE5WnRhFeT/gCuRnhDQSr56Ls8HpGSAEIQCAAAmZ2eJ25Oc/hfeu9j5SSnKSTy5VWSnLyIVPmS2sBrABKRkQA4Y0IU1QvQnijijviwRDeiFFxIQQgEEUCpoR37PQnlFSwoEKhnAl0b3t7FNFENhQ7vJFxiverEF57CSK89jJxOkJ4beZCVxBIdAKmhDcew0B44zG1vPeM8OadWdB3ILxBE85ffYQ3f9y4CwIQCJaAGeH95Ivv9Mbq9crIzFT1yufrwgpnBjtzn6ojvD6BNF4G4bUXEMJrLxN2eG1mQlcQgICRL6299vZ6dRk0VWeWL+ee2/30q+80rGcLNbjxSvMZIbzmI/KlQYTXF4y+FkF4fcXpWzF2eH1DSSEIQMBHAiZ2eBu2GarqlSuqc6tb3ak9/9JqjZm2QKuXTvdxqsGUQniD4WqtKsJrLRHetGYvkayOEF6rydAXBBKbgAnhvbh2Gy1+YIjOKF/OTcN5y1qlWvfotScn6fhSJUwnhPCajse35hBe31D6VogdXt9Q+loI4fUVJ8UgAAGfCJgQ3go1muvVJyeq7PEls6d1yfVt9MzcEe6b1ix/EF7L6fjXG8LrH0u/KiG8fpH0tw7C6y9PqkEAAv4QQHg9ckR4PQKMk9sRXntBIbz2MnE6Qnht5kJXEEh0AmaEt23TeipW5OjsPKbMeUrNbr9eJY4pmv3fWjS6wVxeCK+5SAJpCOENBKunogivJ3yB3YzwBoaWwhCAgAcCJoS3TtO+EU1h+aOpEV0XzYsQ3mjSjt1YCG/s2P/VyAivvUzY4bWZCV1BAAJGHksWz0EgvPGcXuS9I7yRs4rWlQhvtEjnbRx2ePPGi6shAIHoEDCxwxudqQYzCsIbDFdrVRFea4nwWDJ7iWR1hPBaTYa+IJDYBBBej/kjvB4BxsntCK+9oNjhtZcJwmszE7qCAAQ40uB5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZkJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZnXtG8EAABtASURBVEJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZkJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZkJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZkJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZkJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZkJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZkJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8ntcAwusZYVwUQHjtxYTw2ssE4bWZCV1BAAIIr+c1gPB6RhgXBRBeezEhvPYyQXhtZkJXEIAAwut5DSC8nhHGRQGE115MCK+9TBBem5nQFQQggPB6XgMIr2eEcVEA4bUXE8JrLxOE12YmdAUBCCC8Ea2BDT9uUv8xs/XZV9+rXNnjNLx3S11Y4Uz3XoQ3IoRxfxHCay9ChNdeJgivzUzoCgIQQHgjWgNNO41StUsrqlWTOlq15gONnvqYVj4xXslJBRHeiAjG/0UIr70MEV57mSC8NjOhKwhAAOHNdQ1s2bZT1zfppTXLZiipYEH3+ttaD1GfDo116YX/8FV4584Yr3A4rAIFCmT31bpj71x7TNQLZk+7T6FQSJmZmYHzQngjW2VOJgc+GRkZbjbtuvSL7OY8XoXw5hFYwJcHnf2JpY4KeAaUhwAEjmQCocyDbeFInmk+5/b+R19p+MT5eu7hkdkVeg6fqcqVzlXDujV8E94HpqYeIroHBkN4cw7u4P+5HnxFULwQ3sh+A82ZPu6QH0Ccuxzxbdu5b2QF8nAVwpsHWFG4NOjsEd4ohMgQEDiCCSC8uYS7et3HmjL7aS16YEj2lQNS5+jsM05Ws4a1tS8tw5flMXbMyMNEwSnct/8gX+ofaUVSR4/4yykFwSwlqYDC6RnK+N9m8pGG1Jf55JSLswvfp99AX+ofXKRASEoqGNL+MKH4DjcfBYPOvlDy//7mKx/tcQsEIJDgBBDeXBbA+o+/0sCxc7X80dTsKzsPmqrqlc93d3j9+gwbNizHUkOG/E+0/RrrSKgzfPjwHH9AcOYGs9glzDqOHftYj0z2sU6A8SEAgb8jgPDmsj627dilmrf30NtLp6lwoRT36jpN+2pE75aqVPFsbdm535cVNn1yao4C17FbMOcffWk6hkWmThiV4xGQffv2qUffob53VqJoinb/vl9hfzb0fe/PSsFpk8Yc1oqzw9uhq/9HGpIKSEWPStb2PWlWpp/QfQSdfaljsv785QMBCEAgPwQQ3giotepxny4+/xy1vrOuVr7xnqbMeVorFox1v8Tm12PJZkwapeTk5MO6CepMagTTNn1JTl9Yc77s16p9z0D65gxvZFhzOlvt/BDSsYf/R3M4wxtZJtG6KujsOcMbrSQZBwJHJgGEN4Jcf/51i/qMekCffPGdTj6xtEb1vUcVzinv3umX8B5o44D4tu3Um/OiEWQzbcIIFSpUyL0yyB8OEN4IwjjoEkd+0tLS1L7bgLzdmIerEd48wIripUFlj/BGMUSGgsARSADh9Riq38Jb9tjCKlAgpF+27kV4PWbj5+0Ir580/amF8PrD0e8qzvGf/Wnp+n1fuq+lEV5fcVIMAglHAOH1GDnC6xFgnNyO8NoLCuG1l4nTEcJrMxe6gkCiE0B4Pa4AhNcjwDi5HeG1FxTCay8ThNdmJnQFAQjwpjXPawDh9YwwLgogvPZiQnjtZYLw2syEriAAAYTX8xpAeD0jjIsCCK+9mBBee5kgvDYzoSsIQADh9bwGEF7PCOOiAMJrLyaE114mCK/NTOgKAhBAeD2vAYTXM8K4KIDw2osJ4bWXCcJrMxO6ggAEEF7PawDh9YwwLgogvPZiQnjtZYLw2syEriAAAYTX8xpAeD0jjIsCCK+9mBBee5kgvDYzoSsIQADh9bwGEF7PCOOiAMJrLyaE114mCK/NTOgKAhBAeD2vAYTXM8K4KIDw2osJ4bWXCcJrMxO6ggAEEF7PawDh9YwwLgogvPZiQnjtZYLw2syEriAAAYTX8xpAeD0jjIsCCK+9mBBee5kgvDYzoSsIQADh9bwGEF7PCOOiAMJrLyaE114mCK/NTOgKAhBAeD2vAYTXM8K4KIDw2osJ4bWXCcJrMxO6ggAEEF7PawDh9YwwLgogvPZiQnjtZYLw2syEriAAAYSXNQABCEAAAhCAAAQgcIQTCGVmZmYe4XNkehCAAAQgAAEIQAACCUwA4U3g8Jk6BCAAAQhAAAIQSAQCCG8ipMwcIQABCEAAAhCAQAITQHgNhT97wTLNX7xS4fR03Xjt5RrQ+S4VLFjAUIeJ18rX3/2ooRPm64uvN+i4ksXV895GuqbaRYkHwuiM31v/uVp0S9Xzj4zR6aecYLTLxGnrkSdXas7jy5WWFtbNN1RX7/aNFAqFEgcAM4UABMwSQHiNRPPOvz/VwPvmav6UfiperIju7TtJN15bWY1vvtZIh4nZxk0tBui2OlfpzgbX6e21H6v70Gn617P366jCKYkJxNCs9+9PU+P2I7R5y3bNm9IP4Y1xNu+8/6mGTZivhyf3UaGUZHUaMFV9OjZRxX+cFuPOGB4CEIAAT2kwswaGT3pEJ5QuqdZ31nV7en31ene3d97kvmZ6TLRGnJ32Z1e8qVtuqK6kggXd6Veuc6+efHCYTilXOtFwmJvv9IeflfOV25f+tU6Th3dEeGOcUN/RD6pSxbN1e70aMe6E4SEAAQgcToAdXiOrolWP+9Topmt03ZWXuB19u+Fnteg2Vm88PdlIh7Tx0WffqMvg+/XKookqUIC/po3livjuh1/UdfA0LX5giG5rMxThjWUY/z92g1aDdMM1lfXi6+9p9569uql2NbVvfrOBzmgBAhCAADu8ZtbAnR1Gqm3Terry8gvcnn765Tfd3HKg3nthlpkeE7mRjT9vVpte4zWo692qckmFREZhYu4tu41V27vrq/JF56p+8wEIr4FUat7RQ+edfarGDminPb/vVbMuY9StTUPVrH6xge5oAQIQSHQC7PAaWQH39BynBjdc6Z7bdT5ffP2D2vaewA6vgXycLLoMul99OzZRjaoXGugosVt47sW3tO7DLzSyTysXBMJrYz04O7wdWzbI/lLnrEeW6retOzSwa1MbDdIFBCCQ0AQQXiPxj5ryqEocU1QdWtzidvTCq+/q6eWrNHdibyMdJmYbP/y0Sa17jtfofq1VqeJZiQnB2Kw7DZyq9z/6UgULZD3BZPvO3SpW9GiN6nMPP5DEMKvOg6bq6qoXuWfenc/MR5Zox8497g+KfCAAAQjEmgDCG+sE/n9853/gvUfM0iNT+6tIkaPUpud43V7/at1a50ojHSZmG827puqO+le7ZxP52CTADq+NXF7+1zrNnL9ED0/qq7RwWE07jVafDo35IcRGPHQBgYQngPAaWgLznWdYLlimtHC6br7+Cvd/FjzDMnYBOed2azfupeTkpEOaGD/4Xs4lxi6Ww0ZGeO2EMenBJ/XU8lVKTkpSw7pXZf+NlZ0O6QQCEEhUAghvoibPvCEAAQhAAAIQgECCEEB4EyRopgkBCEAAAhCAAAQSlQDCm6jJM28IQAACEIAABCCQIAQQ3gQJmmlCAAIQgAAEIACBRCWA8CZq8swbAhCAAAQgAAEIJAgBhDdBgmaaEIAABCAAAQhAIFEJILyJmjzzhgAEIAABCEAAAglCAOFNkKCZJgQgAAEIQAACEEhUAghvoibPvCEAAQhAAAIQgECCEEB4EyRopgkBCEAAAhCAAAQSlQDCm6jJM28IQAACEIAABCCQIAQQ3gQJmmlCAAIQgAAEIACBRCWA8CZq8swbAhCAAAQgAAEIJAgBhDdBgmaaEIAABCAAAQhAIFEJILyJmjzzhgAEIAABCEAAAglCAOFNkKCZJgQgAAEIQAACEEhUAghvoibPvAMj8Oqb72vQuLlavXR6YGP8XeHMzEx1GXy/3nz3Iw3udrduuaF6IH00aDVIt9a5Snc2qPm39fftT1OlWq216IEh+uc5px127c7dv6tK3fZ67uGROuu0kwLplaIQgAAEIJDYBBDexM7/iJ99naZ9tWv371r2aKqOKXp09nw//uJb3dVxlD54eY7vDGItvJ999b1uaz1Ezz40UuVPKqOUlORD5jhswjwtfv4NPTZtgC7651mH/Nol17fR3Il9dMF5Z+TKxWF4fMkSKnP8sQhvrrS4AAIQgAAEYkkA4Y0lfcYOnIAjvNt27NKN11yugV2bxp3whtPTlVSwYJ44vfPvT9Wu78S/lHlHeFeuWquyx5fUkw8OU8GCBbLr50V4I23K0g5vfnhGOk+ugwAEIAABuwQQXrvZ0JkPBBzhbVi3hqbMfVoLpg3QeWeXd6v+eYfXua7FHTfotrpXub/+1bcbdXOLgVqzbIYrnJfe0FYTh7bXQ0+s0A8/bdIFFc5Uv05NNHzSI/r6ux9VutSxmjy8o04oU0rODu/QCQ+rR7s7NGXOU9r7x37VqHqhhnRvrqMKp7j1Fzzzih5etELbd+zSqSeVVZd7btWVl1/g/tqt9wxW3ZpV9MwL/1L5U07Q/SM7H0Zi7Qefa8KsRfr6+591XMlj1ODGK9WqcR29vfZjdR40Vfv3p7lj9et0l26tc+Uh9zvCm5ycrDff/VBNbqmpprfV+kvh/bs+Dz7SsHnLdvUfM0cffPKVTj6xtDv3Nr3G65XFE1WyRDH3SMP4wfdq3qIX9eW3G3XGqScqtX8bnXlaOR040jCmf+tsvk5Ozr+fdMLxbm9/Nd8CBUJy5vP73n0aO7Bt9jyq1u+gEb1a6drqlXLk+fizr2reohXatGW7SpcqoWa3X5/r0QwfliMlIAABCEAgRgQQ3hiBZ9joEHBEtkfb27X+4/9q7Yef6/Hpg+RIUl6Et1BKsitstWtcqrED22nPnr2q1binypYupTnje+m4ksXVqsdYnXPGKerbsYkrvL1GzFTN6herT8cm2r5zt9r2Gq86NauoW5uG+tc7H2rQfQ9pxphuOufMk/Xmu/9R96EztOThkTqlXBk1ajfMlcAhPZrrH2eeouLFihwC69fN21S7SS8N6HKXbqpVTd9s+Flte09Qq8Y36u6GtbVm3Se6t9+kv93hLVQoRdUuragew6Zr+aOpOr5UCXeMg3d4c+vzYOF15DYcTtf4Ie3dIyR9Rj2gjz77RquemaJiRY92+V1ywTnunI4vWVxdB0/T0UcXdmX+gPBWPPd0jep7jyvIfUc96P73J2YMUm7zzU14/8xz82/b1bDtUD0+faDOPv1kffrld66cz5/aX2efzhni6PzOZBQIQAAC0SWA8EaXN6NFmYAjvN3b3K4ql1RQvbv7qe3d9XV7vRr5El5HUK+qkrUL2+je4Tr/3NPVv/Nd7r9PmfO0vvh6gyuxjvA6u6wvLRyvcmWPc3992kPP6pU3/+1+Matdn4mq+I/T1KHFLdk0HGF16jn/zant7IA68pfTZ87jy7X8lTXuGd0Dn8mzn9LqdR9r8QNDIxJe51xvv053un06Qj9u0L2HCW9ufR4QXmdX3BHlmanddcVlFd06S1a+rf5jZh8ivM4YN15b2f31p5at0rzFL2rZI2OyhXdkn1bZX7B7d/1natltrN5eMk1PLV/1t/PNVXj/xPPDT79W866pWv7IGJ34//mkp2cccrQjysuU4SAAAQhAIGACCG/AgCkfWwIHhNf5q+2Vb6x1//p7+WOp+vGX3w750trfHWk4sMP71OxhOvesU90JNesyxpW71nfWdf991iNL3b92nzuxtyu8PYbPOGSH1TmeMG7GQveIxI139dH3G389DMxNtatpdL/WrvBeXfUitW1aL0d4Q8fP045duzVpWMfsX396+b/cIw6rn5+eJ+H9+dctqtesn6aP6abKF517yA5vbn0eEF7nh4DajXvphcfG6tSTyrg9Occ86jcfcIjwLpw5WM4urvN5/qXV7jGTVxZNyBbex2cMyv6ynHNs5PomvfXM3BF64tlX/3a+kQjvwTwdue2fOttdD86uc/XK57s75SWKF43tYmV0CEAAAhAIjADCGxhaClsgcLDwOv207jleZUuXVKObrtGdHUdmS+mfhffLbzbqlpZZZ3gPCO/Tc4a7RwwiEd7eI2fp3ysfzEbw7Io35ezCOn/FX/fufu4us3P8IKePI7y1r7pULRrd8JfCu3P3Hk0c2iH7150d04kPLnYfhRbJkYYDO7xOgdkLlmnpyrf1zEMjVbVee82Z0NsVz9z6PCC8jjDecGdvrVgw1j2S4XycYxbOjvrBRxoOfixZTsK7cNYQd+fb+Wz8ebMr0c5xC+fc79/NN0fhrddBI3pnneH9K57fbvhZr69erxWvvadfNm3RollDsnd8LaxdeoAABCAAAf8IILz+saSSQQJ/Fl5nZ/XmlgPVr2MTjb5/QbbwOl8Uc/5qvvHN17qzWLXmQ7XvNynfwuscFXj9qckqfVzW2VjnSINzVteRPqduyRLHyPkr/AMfZ6e1zPEl3fPFuQnvQwtfcHdIDz7SMH7WIq378As5u6h5Fd60tLDL5Obrr9DDC1do5tjurvDm1ucB4b3lhit06Q3t3N3tyyud505p2ctr3HO8eRFe50tnzpf1nM+BIw1rV8zSwiWv/e18U6c9rk2/bcv+AeD3vX+4/Uwd0TlH4U0Lp7vnsA/s6DrPLXYe41a/djU1+4sfQgwubVqCAAQgAIE8EEB48wCLS+OPwJ+F15mBc9726eWr3L9KP/Ac3h7DZrhPNrh/VBf3qQrOl7kc6c3vDq9zpKF+rarq3b6x+1i0Ft3G6o76V7tHIJwvg3UbMl2Th3dS1Usq6INP/uvK5czUbqpU8exchfe3rTtUq1FP9zFr9a6rqs//u8F9DFnnlg10x03X5Fl4HSaOJHcaOEWhUCh7hze3Pg/+0lrj9iNU4piiGjeonTtf5/zu+x99lSfhrVTxLN03sJ37JbdeI2YpPT1dD47rqdzmu2jJa3rwsWVaMm+UihY5SpMefFKPPvWSey45px3eJ5571X1KhvOFOecJGd9t/EUtuqZqSPdmuuaKSvG3yOkYAhCAAARyJYDw5oqIC+KZQE7C+8e+/arXrL+cR2kdEF7nr+D7jHxAW7ftdJ+64Hy5rdOAKXpryf06+qjC7lMGIj3S8NKqdRo3c6HaNa2vaQ8/4wq088QG561nB14C8djTL7tf2nJk7sQypdTmrnruDqvzyW2H17nGeYva+FkLteHHTTqhdEn38WLOG88cYc3rDu+BfJ0nRax84z0dfJb27/o8WHidnfN+ox/Ul9/8oDNPO0kdmt+idn0m6F/PTnUl9M9vWjv4SMP2HbtV7aaO7mPfps9bop9+2awK55zmPrbMecxbbvN1HknWZ+Qs/eezb9xd26a31tL8xS+qY8sG7pM1/szTOcPrPC7u+ZdXa9uO3e4TKpwjJgfOY8fzeqd3CEAAAhDImQDCy8qAAAQ8E8jIyFRaOOyed3Y+zu5uy26pev+lOe4xDT4QgAAEIACBWBJAeGNJn7EhcIQQcHaHnS+WOUcSnDOxA1Jnuy+3yOmlGUfIlJkGBCAAAQjEEQGEN47ColUIWCXgHM1wHpf23gefKSmpoPvlNecZxc7xED4QgAAEIACBWBNAeGOdAONDAAIQgAAEIAABCARKAOENFC/FIQABCEAAAhCAAARiTQDhjXUCjA8BCEAAAhCAAAQgECgBhDdQvBSHAAQgAAEIQAACEIg1AYQ31gkwPgQgAAEIQAACEIBAoAQQ3kDxUhwCEIAABCAAAQhAINYEEN5YJ8D4EIAABCAAAQhAAAKBEkB4A8VLcQhAAAIQgAAEIACBWBNAeGOdAONDAAIQgAAEIAABCARKAOENFC/FIQABCEAAAhCAAARiTQDhjXUCjA8BCEAAAhCAAAQgECgBhDdQvBSHAAQgAAEIQAACEIg1AYQ31gkwPgQgAAEIQAACEIBAoAQQ3kDxUhwCEIAABCAAAQhAINYEEN5YJ8D4EIAABCAAAQhAAAKBEkB4A8VLcQhAAAIQgAAEIACBWBNAeGOdAONDAAIQgAAEIAABCARKAOENFC/FIQABCEAAAhCAAARiTQDhjXUCjA8BCEAAAhCAAAQgECgBhDdQvBSHAAQgAAEIQAACEIg1AYQ31gkwPgQgAAEIQAACEIBAoAQQ3kDxUhwCEIAABCAAAQhAINYEEN5YJ8D4EIAABCAAAQhAAAKBEkB4A8VLcQhAAAIQgAAEIACBWBNAeGOdAONDAAIQgAAEIAABCARKAOENFC/FIQABCEAAAhCAAARiTQDhjXUCjA8BCEAAAhCAAAQgECgBhDdQvBSHAAQgAAEIQAACEIg1AYQ31gkwPgQgAAEIQAACEIBAoAQQ3kDxUhwCEIAABCAAAQhAINYEEN5YJ8D4EIAABCAAAQhAAAKBEkB4A8VLcQhAAAIQgAAEIACBWBNAeGOdAONDAAIQgAAEIAABCARKAOENFC/FIQABCEAAAhCAAARiTQDhjXUCjA8BCEAAAhCAAAQgECgBhDdQvBSHAAQgAAEIQAACEIg1AYQ31gkwPgQgAAEIQAACEIBAoAT+D9gDZ2ujkUTAAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load and clean the dataset\n",
    "cscs_data = pd.read_csv(\"/home/jovyan/STA130/COURSE PROJECT/CSCS_data_anon.csv\", low_memory=False)\n",
    "columns_of_interest = [\"CONNECTION_neighbours_name_num\", \"WELLNESS_self_rated_mental_health\"]\n",
    "cscs_data = cscs_data[columns_of_interest]\n",
    "\n",
    "# Drop rows with 'Presented but no response' and convert 'NaN' values in CONNECTION_neighbours_name_num to 0\n",
    "cscs_data = cscs_data[\n",
    "    ~cscs_data[\"CONNECTION_neighbours_name_num\"].isin([\"Presented but no response\"]) &\n",
    "    ~cscs_data[\"WELLNESS_self_rated_mental_health\"].isin([\"Presented but no response\"])\n",
    "]\n",
    "\n",
    "# Map the `CONNECTION_neighbours_name_num` categories to numerical values\n",
    "cscs_data[\"CONNECTION_neighbours_name_num\"] = cscs_data[\"CONNECTION_neighbours_name_num\"].map({\n",
    "    '5 or more': 6,\n",
    "    '1–2': 1.5,\n",
    "    '3–4': 3.5,\n",
    "    None: 0\n",
    "}).fillna(0)\n",
    "\n",
    "# Map `WELLNESS_self_rated_mental_health` to binary outcome: 1 for positive, 0 for negative\n",
    "cscs_data[\"mental_health_binary\"] = cscs_data[\"WELLNESS_self_rated_mental_health\"].map({\n",
    "    'Excellent': 1,\n",
    "    'Very good': 1,\n",
    "    'Good': 1,\n",
    "    'Fair': 0,\n",
    "    'Poor': 0\n",
    "})\n",
    "\n",
    "# Drop any remaining rows with NaN values in the new binary column\n",
    "cscs_data = cscs_data.dropna()\n",
    "\n",
    "# Define the linear model based on the provided slope and intercept\n",
    "intercept = 0.5977\n",
    "slope = 0.1764\n",
    "\n",
    "# Calculate the predicted mental health scores using the linear equation\n",
    "cscs_data['predicted_mental_health'] = intercept + slope * cscs_data['CONNECTION_neighbours_name_num']\n",
    "\n",
    "# Plotting the observed data points and the linear model prediction\n",
    "fig = go.Figure()\n",
    "\n",
    "# Scatter plot for the actual binary outcomes\n",
    "fig.add_trace(go.Scatter(x=cscs_data[\"CONNECTION_neighbours_name_num\"],\n",
    "                         y=cscs_data[\"mental_health_binary\"],\n",
    "                         mode='markers',\n",
    "                         name='Observed Data',\n",
    "                         marker=dict(color='gray', opacity=0.6)))\n",
    "\n",
    "# Add the linear model prediction line\n",
    "fig.add_trace(go.Scatter(x=cscs_data[\"CONNECTION_neighbours_name_num\"],\n",
    "                         y=cscs_data[\"predicted_mental_health\"],\n",
    "                         mode='lines',\n",
    "                         name=\"Predicted Mental Health (Linear Model)\",\n",
    "                         line=dict(color=\"blue\", width=2)))\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title=\"Linear Model: Predicted Mental Health by Number of Neighbours\",\n",
    "    xaxis_title=\"Number of Neighbours\",\n",
    "    yaxis_title=\"Predicted Mental Health Score\",\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show(renderer=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfd948",
   "metadata": {},
   "source": [
    "### 3.3 Interpreting Logistic Regression as Multivariate Linear Regression\n",
    "\n",
    "Logistic regression is not as simple as multivariate linear regression due to the log-odds transformation. However, for simplicity and ease of interpretation, we will treat logistic regression models as if they were multivariate linear regression models.\n",
    "\n",
    "#### Logistic Regression Model in Simple Terms\n",
    "In logistic regression, instead of directly predicting the outcome as in linear regression, we predict the **log-odds** of the event happening. But for interpretation, we'll treat the model like a linear regression, where the outcome is a continuous variable.\n",
    "\n",
    "The logistic regression equation is:\n",
    "$[\n",
    "\\text{logit}(P(Y = 1)) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
    "$]\n",
    "Where:\n",
    "- $( P(Y = 1) $) is the probability of the event occurring (e.g., \"positive mental health\").\n",
    "- $( \\beta_0 $) is the intercept (baseline).\n",
    "- $( \\beta_1, \\beta_2, \\dots, \\beta_n $) are the coefficients representing how each predictor influences the log-odds.\n",
    "- $( X_1, X_2, \\dots, X_n $) are the predictor variables (e.g., age, number of neighbors known).\n",
    "\n",
    "#### Interpreting as a Linear Regression\n",
    "While logistic regression uses log-odds, we will interpret the coefficients as if the model were linear:\n",
    "1. **Intercept ($(\\beta_0$))**: This is the baseline log-odds of the outcome when all predictors are 0. If the coefficients were in a linear regression, this would just be the starting value for the prediction.\n",
    "2. **Coefficient ($(\\beta_1, \\beta_2, \\dots, \\beta_n$))**: These represent how much the outcome (the log-odds) changes for each one-unit increase in the corresponding predictor variable. In simpler terms, if these were linear regression coefficients, you would add $(\\beta_1$) to the outcome for each one-unit increase in $(X_1$), and so on.\n",
    "\n",
    "For example:\n",
    "- If $( \\beta_1 $) is positive, it suggests that as $( X_1 $) (e.g., number of neighbors) increases, the log-odds of having positive mental health also increase.\n",
    "- If $( \\beta_2 $) is negative, it suggests that as $( X_2 $) (e.g., age) increases, the log-odds of having positive mental health decrease.\n",
    "\n",
    "#### Making Predictions\n",
    "For prediction, we can use the fitted model coefficients as follows:\n",
    "1. Calculate the predicted log-odds:\n",
    "   $[\n",
    "   \\text{logit}(P(Y = 1)) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots\n",
    "   $]\n",
    "2. Convert the log-odds to a probability (between 0 and 1) using the logistic function:\n",
    "   $[\n",
    "   P(Y = 1) = \\frac{1}{1 + e^{-\\text{logit}(P(Y = 1))}}\n",
    "   $]\n",
    "   \n",
    "This provides the probability that the event (e.g., positive mental health) occurs.\n",
    "\n",
    "#### Summary\n",
    "By interpreting the logistic regression model as if it were linear regression, we simplify the understanding of how each predictor influences the outcome. The coefficients tell us how changes in the predictors are associated with the log-odds of the outcome, and we can convert those log-odds into probabilities for making predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309ac769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.610360\n",
      "         Iterations 5\n",
      "                            Logit Regression Results                            \n",
      "================================================================================\n",
      "Dep. Variable:     mental_health_binary   No. Observations:                 8423\n",
      "Model:                            Logit   Df Residuals:                     8420\n",
      "Method:                             MLE   Df Model:                            2\n",
      "Date:                  Fri, 15 Nov 2024   Pseudo R-squ.:                 0.02475\n",
      "Time:                          00:51:48   Log-Likelihood:                -5141.1\n",
      "converged:                         True   LL-Null:                       -5271.5\n",
      "Covariance Type:              nonrobust   LLR p-value:                 2.189e-57\n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Intercept                          0.9078      0.042     21.771      0.000       0.826       0.990\n",
      "CONNECTION_neighbours_name_num     0.1518      0.015     10.426      0.000       0.123       0.180\n",
      "age_binary                        -0.4857      0.049     -9.828      0.000      -0.583      -0.389\n",
      "==================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the dataset\n",
    "cscs_data = pd.read_csv(\"/home/jovyan/STA130/COURSE PROJECT/CSCS_data_anon.csv\", low_memory=False)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = [\"CONNECTION_neighbours_name_num\", \"WELLNESS_self_rated_mental_health\", \"DEMO_age\"]\n",
    "cscs_data = cscs_data[columns_of_interest]\n",
    "\n",
    "# Drop rows with 'Presented but no response' and convert 'NaN' values in CONNECTION_neighbours_name_num to 0\n",
    "cscs_data = cscs_data[\n",
    "    ~cscs_data[\"CONNECTION_neighbours_name_num\"].isin([\"Presented but no response\"]) &\n",
    "    ~cscs_data[\"WELLNESS_self_rated_mental_health\"].isin([\"Presented but no response\"])\n",
    "]\n",
    "\n",
    "# Map the `CONNECTION_neighbours_name_num` categories to numerical values\n",
    "cscs_data[\"CONNECTION_neighbours_name_num\"] = cscs_data[\"CONNECTION_neighbours_name_num\"].map({\n",
    "    '5 or more': 6,\n",
    "    '1–2': 1.5,\n",
    "    '3–4': 3.5,\n",
    "    None: 0\n",
    "}).fillna(0)\n",
    "\n",
    "# Map `WELLNESS_self_rated_mental_health` to binary outcome: 1 for positive, 0 for negative\n",
    "cscs_data[\"mental_health_binary\"] = cscs_data[\"WELLNESS_self_rated_mental_health\"].map({\n",
    "    'Excellent': 1,\n",
    "    'Very good': 1,\n",
    "    'Good': 1,\n",
    "    'Fair': 0,\n",
    "    'Poor': 0\n",
    "})\n",
    "\n",
    "# Create binary age variable: 1 for age 40+ and 0 for age below 40 using 'DEMO_age'\n",
    "cscs_data[\"age_binary\"] = (cscs_data[\"DEMO_age\"] >= 40).astype(int)\n",
    "\n",
    "# Drop any remaining rows with NaN values in the new binary column\n",
    "cscs_data = cscs_data.dropna()\n",
    "\n",
    "# Fit a logistic regression model with both predictors\n",
    "log_reg_formula = 'mental_health_binary ~ CONNECTION_neighbours_name_num + age_binary'\n",
    "log_reg_fit = smf.logit(log_reg_formula, data=cscs_data).fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(log_reg_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "379f3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OLS Regression Results                             \n",
      "================================================================================\n",
      "Dep. Variable:     mental_health_binary   R-squared:                       0.035\n",
      "Model:                              OLS   Adj. R-squared:                  0.034\n",
      "Method:                   Least Squares   F-statistic:                     100.5\n",
      "Date:                  Fri, 15 Nov 2024   Prob (F-statistic):           6.52e-64\n",
      "Time:                          00:51:57   Log-Likelihood:                -5371.2\n",
      "No. Observations:                  8423   AIC:                         1.075e+04\n",
      "Df Residuals:                      8419   BIC:                         1.078e+04\n",
      "Df Model:                             3                                         \n",
      "Covariance Type:              nonrobust                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Intercept                          0.6829      0.009     72.895      0.000       0.665       0.701\n",
      "CONNECTION_neighbours_name_num     0.0482      0.004     12.122      0.000       0.040       0.056\n",
      "age_binary                        -0.0634      0.012     -5.355      0.000      -0.087      -0.040\n",
      "interaction                       -0.0360      0.005     -6.636      0.000      -0.047      -0.025\n",
      "==============================================================================\n",
      "Omnibus:                   127702.290   Durbin-Watson:                   2.018\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1369.269\n",
      "Skew:                          -0.715   Prob(JB):                    4.65e-298\n",
      "Kurtosis:                       1.637   Cond. No.                         7.37\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "cscs_data = pd.read_csv(\"/home/jovyan/STA130/COURSE PROJECT/CSCS_data_anon.csv\", low_memory=False)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = [\"CONNECTION_neighbours_name_num\", \"WELLNESS_self_rated_mental_health\", \"DEMO_age\"]\n",
    "cscs_data = cscs_data[columns_of_interest]\n",
    "\n",
    "# Drop rows with 'Presented but no response' and convert 'NaN' values in CONNECTION_neighbours_name_num to 0\n",
    "cscs_data = cscs_data[\n",
    "    ~cscs_data[\"CONNECTION_neighbours_name_num\"].isin([\"Presented but no response\"]) &\n",
    "    ~cscs_data[\"WELLNESS_self_rated_mental_health\"].isin([\"Presented but no response\"])\n",
    "]\n",
    "\n",
    "# Map the `CONNECTION_neighbours_name_num` categories to numerical values\n",
    "cscs_data[\"CONNECTION_neighbours_name_num\"] = cscs_data[\"CONNECTION_neighbours_name_num\"].map({\n",
    "    '5 or more': 6,\n",
    "    '1–2': 1.5,\n",
    "    '3–4': 3.5,\n",
    "    None: 0\n",
    "}).fillna(0)\n",
    "\n",
    "# Map `WELLNESS_self_rated_mental_health` to binary outcome: 1 for positive, 0 for negative\n",
    "cscs_data[\"mental_health_binary\"] = cscs_data[\"WELLNESS_self_rated_mental_health\"].map({\n",
    "    'Excellent': 1,\n",
    "    'Very good': 1,\n",
    "    'Good': 1,\n",
    "    'Fair': 0,\n",
    "    'Poor': 0\n",
    "})\n",
    "\n",
    "# Create binary age variable: 1 for age 40+ and 0 for age below 40 using 'DEMO_age'\n",
    "cscs_data[\"age_binary\"] = (cscs_data[\"DEMO_age\"] >= 40).astype(int)\n",
    "\n",
    "# Drop any remaining rows with NaN values in the new binary column\n",
    "cscs_data = cscs_data.dropna()\n",
    "\n",
    "# Create interaction term between CONNECTION_neighbours_name_num and age_binary\n",
    "cscs_data[\"interaction\"] = cscs_data[\"CONNECTION_neighbours_name_num\"] * cscs_data[\"age_binary\"]\n",
    "\n",
    "# Fit a linear regression model with interaction terms\n",
    "linear_reg_formula = 'mental_health_binary ~ CONNECTION_neighbours_name_num + age_binary + interaction'\n",
    "linear_reg_fit = smf.ols(linear_reg_formula, data=cscs_data).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(linear_reg_fit.summary())\n",
    "\n",
    "# Now let's generate the predictions\n",
    "# Create a range of values for CONNECTION_neighbours_name_num to predict across\n",
    "connection_range = np.linspace(cscs_data[\"CONNECTION_neighbours_name_num\"].min(),\n",
    "                               cscs_data[\"CONNECTION_neighbours_name_num\"].max(), 100)\n",
    "\n",
    "# Create DataFrames for the predictions, one for age_binary = 1 and one for age_binary = 0\n",
    "df_40_plus = pd.DataFrame({\n",
    "    'CONNECTION_neighbours_name_num': connection_range,\n",
    "    'age_binary': np.ones(100),\n",
    "    'interaction': connection_range * 1  # interaction term for age_binary = 1\n",
    "})\n",
    "\n",
    "df_below_40 = pd.DataFrame({\n",
    "    'CONNECTION_neighbours_name_num': connection_range,\n",
    "    'age_binary': np.zeros(100),\n",
    "    'interaction': connection_range * 0  # interaction term for age_binary = 0\n",
    "})\n",
    "\n",
    "# Generate predictions for both groups\n",
    "predictions_40_plus = linear_reg_fit.predict(df_40_plus)\n",
    "predictions_below_40 = linear_reg_fit.predict(df_below_40)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(connection_range, predictions_40_plus, label='Age 40+ (age_binary = 1)', color='blue')\n",
    "plt.plot(connection_range, predictions_below_40, label='Below Age 40 (age_binary = 0)', color='orange')\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Predicted Mental Health (Binary) vs. Number of Neighbors\")\n",
    "plt.xlabel(\"Number of Neighbors (CONNECTION_neighbours_name_num)\")\n",
    "plt.ylabel(\"Predicted Mental Health Binary (0 or 1)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34e31b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.619589\n",
       "1        0.619589\n",
       "2        0.619589\n",
       "3        0.619589\n",
       "4        0.971952\n",
       "           ...   \n",
       "11424    0.619589\n",
       "11425    0.619589\n",
       "11426    0.619589\n",
       "11427    0.682940\n",
       "11430    0.682940\n",
       "Length: 8423, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg_fit.predict(cscs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f0780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OLS Regression Results                             \n",
      "================================================================================\n",
      "Dep. Variable:     mental_health_binary   R-squared:                       0.035\n",
      "Model:                              OLS   Adj. R-squared:                  0.034\n",
      "Method:                   Least Squares   F-statistic:                     100.5\n",
      "Date:                  Fri, 15 Nov 2024   Prob (F-statistic):           6.52e-64\n",
      "Time:                          01:02:18   Log-Likelihood:                -5371.2\n",
      "No. Observations:                  8423   AIC:                         1.075e+04\n",
      "Df Residuals:                      8419   BIC:                         1.078e+04\n",
      "Df Model:                             3                                         \n",
      "Covariance Type:              nonrobust                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Intercept                          0.6829      0.009     72.895      0.000       0.665       0.701\n",
      "CONNECTION_neighbours_name_num     0.0482      0.004     12.122      0.000       0.040       0.056\n",
      "age_binary                        -0.0634      0.012     -5.355      0.000      -0.087      -0.040\n",
      "interaction                       -0.0360      0.005     -6.636      0.000      -0.047      -0.025\n",
      "==============================================================================\n",
      "Omnibus:                   127702.290   Durbin-Watson:                   2.018\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1369.269\n",
      "Skew:                          -0.715   Prob(JB):                    4.65e-298\n",
      "Kurtosis:                       1.637   Cond. No.                         7.37\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "cscs_data = pd.read_csv(\"/home/jovyan/STA130/COURSE PROJECT/CSCS_data_anon.csv\", low_memory=False)\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = [\"CONNECTION_neighbours_name_num\", \"WELLNESS_self_rated_mental_health\", \"DEMO_age\"]\n",
    "cscs_data = cscs_data[columns_of_interest]\n",
    "\n",
    "# Drop rows with 'Presented but no response' and convert 'NaN' values in CONNECTION_neighbours_name_num to 0\n",
    "cscs_data = cscs_data[\n",
    "    ~cscs_data[\"CONNECTION_neighbours_name_num\"].isin([\"Presented but no response\"]) &\n",
    "    ~cscs_data[\"WELLNESS_self_rated_mental_health\"].isin([\"Presented but no response\"])\n",
    "]\n",
    "\n",
    "# Map the `CONNECTION_neighbours_name_num` categories to numerical values\n",
    "cscs_data[\"CONNECTION_neighbours_name_num\"] = cscs_data[\"CONNECTION_neighbours_name_num\"].map({\n",
    "    '5 or more': 6,\n",
    "    '1–2': 1.5,\n",
    "    '3–4': 3.5,\n",
    "    None: 0\n",
    "}).fillna(0)\n",
    "\n",
    "# Map `WELLNESS_self_rated_mental_health` to binary outcome: 1 for positive, 0 for negative\n",
    "cscs_data[\"mental_health_binary\"] = cscs_data[\"WELLNESS_self_rated_mental_health\"].map({\n",
    "    'Excellent': 1,\n",
    "    'Very good': 1,\n",
    "    'Good': 1,\n",
    "    'Fair': 0,\n",
    "    'Poor': 0\n",
    "})\n",
    "\n",
    "# Create binary age variable: 1 for age 40+ and 0 for age below 40 using 'DEMO_age'\n",
    "cscs_data[\"age_binary\"] = (cscs_data[\"DEMO_age\"] >= 40).astype(int)\n",
    "\n",
    "# Drop any remaining rows with NaN values in the new binary column\n",
    "cscs_data = cscs_data.dropna()\n",
    "\n",
    "# Create interaction term between CONNECTION_neighbours_name_num and age_binary\n",
    "cscs_data[\"interaction\"] = cscs_data[\"CONNECTION_neighbours_name_num\"] * cscs_data[\"age_binary\"]\n",
    "\n",
    "# Fit a linear regression model with interaction terms\n",
    "linear_reg_formula = 'mental_health_binary ~ CONNECTION_neighbours_name_num + age_binary + interaction'\n",
    "linear_reg_fit = smf.ols(linear_reg_formula, data=cscs_data).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(linear_reg_fit.summary())\n",
    "\n",
    "# Generate predictions (in log-odds space) and convert to probabilities\n",
    "log_odds_predictions = linear_reg_fit.predict(cscs_data)\n",
    "probabilities = 1 / (1 + np.exp(-log_odds_predictions))  # Applying the logistic function\n",
    "\n",
    "# Create a range of values for CONNECTION_neighbours_name_num to predict across\n",
    "connection_range = np.linspace(cscs_data[\"CONNECTION_neighbours_name_num\"].min(),\n",
    "                               cscs_data[\"CONNECTION_neighbours_name_num\"].max(), 100)\n",
    "\n",
    "# Create DataFrames for the predictions, one for age_binary = 1 and one for age_binary = 0\n",
    "df_40_plus = pd.DataFrame({\n",
    "    'CONNECTION_neighbours_name_num': connection_range,\n",
    "    'age_binary': np.ones(100),\n",
    "    'interaction': connection_range * 1  # interaction term for age_binary = 1\n",
    "})\n",
    "\n",
    "df_below_40 = pd.DataFrame({\n",
    "    'CONNECTION_neighbours_name_num': connection_range,\n",
    "    'age_binary': np.zeros(100),\n",
    "    'interaction': connection_range * 0  # interaction term for age_binary = 0\n",
    "})\n",
    "\n",
    "# Generate predictions for both groups and convert to probabilities\n",
    "log_odds_40_plus = linear_reg_fit.predict(df_40_plus)\n",
    "probabilities_40_plus = 1 / (1 + np.exp(-log_odds_40_plus))  # Convert to probability\n",
    "\n",
    "log_odds_below_40 = linear_reg_fit.predict(df_below_40)\n",
    "probabilities_below_40 = 1 / (1 + np.exp(-log_odds_below_40))  # Convert to probability\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(connection_range, probabilities_40_plus, label='Age 40+ (age_binary = 1)', color='blue')\n",
    "plt.plot(connection_range, probabilities_below_40, label='Below Age 40 (age_binary = 0)', color='orange')\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Predicted Probability of Positive Mental Health vs. Number of Neighbors\")\n",
    "plt.xlabel(\"Number of Neighbors (CONNECTION_neighbours_name_num)\")\n",
    "plt.ylabel(\"Predicted Probability of Positive Mental Health\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ac555",
   "metadata": {},
   "source": [
    "### 4. Explain the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" while at the same time \"many of the *coefficients* are larger than 10 while having *strong* or *very strong evidence against* the *null hypothesis* of 'no effect'\"<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _How do we simultaneously interpret **hypothesis testing** results regarding **coefficient estimates** based on **p-values** and **R-squared** \"the proportion of variation in (outcome) $y$ explained by the model ($\\hat y_i$)\"? How can both be meaningfully understood at the same time? Do they address different aspects of a model?_\n",
    ">    \n",
    "> _As introduced in the previous homework, **R-squared** is_\n",
    ">\n",
    "> _$$R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i-\\hat y)^2}{\\sum_{i=1}^n(y_i-\\bar y)^2}$$_\n",
    ">    \n",
    "> _which describes the **explanatory power** of a model; whereas, **p-values** allow us to characterize **evidence against** a **null hypothesis**, and **coefficients** in a **multiple linear regression** context allow us to interpret the relationship between the **outcome** and a **predictor variable** \"with all other **predictor variables** 'held constant'\". Are these concepts thus contradictory or conflictual in some manner?_\n",
    "\n",
    "|p-value|Evidence|\n",
    "|-|-|\n",
    "|$$p > 0.1$$|No evidence against the null hypothesis|\n",
    "|$$0.1 \\ge p > 0.05$$|Weak evidence against the null hypothesis|\n",
    "|$$0.05 \\ge p > 0.01$$|Moderate evidence against the null hypothesis|\n",
    "|$$0.01 \\ge p > 0.001$$|Strong evidence against the null hypothesis|\n",
    "|$$0.001 \\ge p$$|Very strong evidence against the null hypothesis|\n",
    "    \n",
    "> _In `formula='HP ~ Q(\"Sp. Def\") * C(Generation)'` the `Q` stands for \"quote\" and is needed to access column names when they have a \"space\" in their name, while the `C` indicates a **categorical** use of what is actually an **integer** valued column. Despite technically being **continuous** numbers, **integer** often simply indicate categories which should not necessarily be treated as an incremental **continuous predictor variable**. Remember, a model such as $\\beta_0 + \\beta_1 x$ means for each unit increase in $x$ the outcome increases \"on average\" by $\\beta_1$; so, if $x$ takes on the values `1` through `6` as the `Generation` **predictor variable** here does, then this means the average value for \"Generation 1\" must be $\\beta_0 + \\beta_1$ while for \"Generation 2\" it must be $\\beta_0 + 2\\times \\beta_1$ (and so on up to \"Generation 6\" which must be $\\beta_0 + 6\\times \\beta_1$). This might be a very strange restriction to place on something that is really actually a **categorical predictor variable**. You can see in the given model fit below how this six-level **categorical predictor variable** is actually appropriately treated in the specification of the **linear form** using \"Generation 1\" for the \"baseline\" and **binary indicators** to model the \"contrast\" (\"offsets\") for the other \"Generations\"; and, how these are in turn used in the context of the **interaction** considered by the model specification._ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee4ccce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d4a80b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:06:59</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     01:06:59     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        01:06:59   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355e6cc",
   "metadata": {},
   "source": [
    "## 4. General Answer\n",
    "\n",
    "To understand the seeming contradiction between having a low $( R^2 $) (indicating the model explains only a small portion of the outcome variability) and having statistically significant coefficients (suggesting strong evidence that predictors influence the outcome), it's essential to unpack what each metric—$( R^2 $) and p-values for coefficients—tells us about the model's performance and the predictor variables.\n",
    "\n",
    "### 1. $( R^2 $) and Overall Model Explanatory Power\n",
    "The $( R^2 $) statistic measures the proportion of the total variance in the outcome variable $( y $) that can be explained by the model. An $( R^2 $) of 17.6% indicates that the model accounts for only 17.6% of the variability in $( y $), meaning the model doesn’t capture most of the factors influencing $( y $). This low explanatory power suggests that there are other factors (predictors or underlying processes) outside the model that contribute to variations in $( y $).\n",
    "\n",
    "### 2. P-Values and Evidence Against the Null Hypothesis for Coefficients\n",
    "In contrast, p-values for individual coefficients help determine whether specific predictors have statistically significant relationships with $( y $), with all other predictors held constant. If many of the p-values are low (below typical thresholds like 0.05 or 0.01), we have strong evidence that these predictors are associated with $( y $) in a way that is unlikely to be due to chance alone. This implies that each predictor has a measurable effect on the outcome in the context of the model, even if this combined influence doesn’t explain much of $( y $)’s overall variation.\n",
    "\n",
    "### Reconciling the Low $( R^2 $) and Significant Coefficients\n",
    "Here’s how these concepts coexist:\n",
    "\n",
    "- **Distinct Interpretations**: The $( R^2 $) value reflects the overall goodness of fit of the entire model but doesn’t speak to the significance of individual predictors. A low $( R^2 $) tells us that the model’s predictors don’t collectively explain much of the outcome’s variance. Meanwhile, the p-values for coefficients give us insight into the individual predictors’ impact on the outcome, even if these impacts are small or context-specific.\n",
    "\n",
    "- **Addressing Different Questions**: $( R^2 $) answers “how much of the variability in $( y $) can this model explain?” while p-values for coefficients answer “is there evidence that this predictor has a non-zero effect on $( y $)?” Thus, a model can have a low $( R^2 $) while still having statistically significant predictors, as these metrics address different aspects of model interpretation.\n",
    "\n",
    "- **Potential for Minor Predictive Contributions**: In practice, predictors with large coefficients and low p-values may indeed affect $( y $), but the overall relationship might be weak or overshadowed by unexplained variability. For instance, if each significant predictor explains a small, specific part of $( y $) but is still statistically reliable, the model can yield strong coefficient estimates without achieving high explanatory power overall.\n",
    "\n",
    "### When Both Insights are Valuable\n",
    "These two metrics together suggest that while individual predictors are statistically relevant to $( y $), many factors impacting $( y $) are missing from the model. This situation commonly arises in complex systems where multiple, nuanced factors are at play, or in studies with limited data on potentially influential variables. In such cases, both the low $( R^2 $) and significant p-values provide insights that could guide further research to identify additional predictors that might better explain the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf7734",
   "metadata": {},
   "source": [
    "## Data Specific Answer\n",
    "### Interpreting Model Results: Interaction Terms, Low $( R^2 $), and Significant Coefficients\n",
    "\n",
    "With the provided Pokémon data and updated regression model using the interaction term between *Special Defense* (`Sp. Def`) and *Generation* (`C(Generation)`), there’s additional context to consider when interpreting the results. Here’s how it may adjust the interpretation of the low $( R^2 $) and significant coefficients, focusing specifically on the interaction term and its implications:\n",
    "\n",
    "#### 1. Effects of Interaction Term on Interpretation of Predictors\n",
    "The addition of an interaction term, $( \\text{Q(\"Sp. Def\")} * \\text{C(Generation)} $), allows us to examine how the relationship between *Special Defense* and *HP* differs across Pokémon generations. Including this interaction may result in some of the coefficients (such as the interaction coefficients for *Special Defense* by *Generation*) being statistically significant. This implies that the effect of *Special Defense* on *HP* varies based on the *Generation* category, even if the model’s overall explanatory power remains low.\n",
    "\n",
    "#### 2. Potential Implications for Model Explanatory Power (Low $( R^2 $))\n",
    "The model might still have a low $( R^2 $), indicating that while *Special Defense*, *Generation*, and their interactions have statistically significant effects, there are likely many other factors contributing to the variability in *HP*. The low $( R^2 $) suggests that these predictors—while statistically significant—don’t capture the bulk of what influences *HP* across all Pokémon species. It’s common for models with categorical interactions to explain only specific aspects of the outcome rather than capturing the overall variation.\n",
    "\n",
    "#### 3. Understanding the Role of Categorical and Interaction Terms\n",
    "With categorical predictors (like *Generation*) and interactions in the model, each level of the *Generation* category can have a unique influence on *HP*. The coefficients for these categories and interactions reflect how *Special Defense* influences *HP* differently across generations, yet the overall contribution of these predictors remains limited relative to other unexplored variables that influence *HP* in Pokémon data.\n",
    "\n",
    "#### Revised Answer Summary\n",
    "This additional complexity reinforces the interpretation that:\n",
    "\n",
    "- **Predictor Significance**: The model’s significant coefficients (including the interaction terms) indicate that *Special Defense* and *Generation* meaningfully affect *HP*, even if they don’t explain much of the total variability. This points to reliable relationships in the data, though they’re limited in scope.\n",
    "  \n",
    "- **Low $( R^2 $)**: The low $( R^2 $) persists because *HP* depends on many other attributes not included in the model. Thus, the model captures specific relationships rather than providing a comprehensive explanation of *HP* variability.\n",
    "\n",
    "Adding the interaction term doesn't contradict the original answer; it highlights the nuanced insights into how specific predictors influence the outcome while underscoring the distinct meanings of $( R^2 $) and coefficient significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614675a5",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "### Pre-lecture VS Post-lecture HW\n",
    "    \n",
    "Feel free to work on the \"Post-lecture\" HW below if you're making good progress and want to continue: in this case the \"Post-lecture\" HW just builds on the \"Post-lecture\" HW, introducing and extending the considerations available in the **multiple linear regression context**. That said, as \"question 3\" above hopefully suggests and reminds you, the **course project** is well upon us, and prioritizing work on that (even over the homework) may very well be indicated at this point...\n",
    "\n",
    "*The benefits of continue would are that (a) it might be fun to try to tackle the challenge of working through some problems without additional preparation or guidance; and (b) this is a very valable skill to be comfortable with; and (c) it will let you build experience interacting with ChatBots (and beginning to understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a392bc2",
   "metadata": {},
   "source": [
    "## \"Post-lecture\" HW [*submission along with \"Pre-lecture\" HW is due prior to next TUT*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edb40a",
   "metadata": {},
   "source": [
    "### 5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _Recall from the previous week's homework that the **R-squared** \"the proportion of variation in (outcome) $y$ explained by the model ($\\hat y_i$)\" quantity (re-introduced in the previous problem) can be calculated as `np.corrcoef(y,fitted_model.fittedvalues)[0,1]**2` (as well as several other ways in the **simple linear regression** context). The **squared correlation** between the outcome $y$ and it's **fitted values** $\\hat y$ is the most generally useful formulation of **R-squared** since this can be use in the **multiple linear regression** context._\n",
    "> \n",
    "> _This question therefore thus addresses the question of model **generalizability** on the basis of \"in sample\" and \"out of sample\" **model performance** (measured by **R-squared**)._\n",
    "> \n",
    "> - _The **squared correlation** between the **outcomes** $y$ and their **fitted values** $\\hat y$ is an \"in sample\" **model performance** metric since the $\\hat y$ \"predictions\" for the $y$ **outcomes** are based on using those already **observed outcomes** to fit the model to generate the $\\hat y$._  \n",
    "> \n",
    "> - _If we instead calculate **squared correlation** between **outcomes** $y$ that were not used to fit the model and their corresponding $\\hat y$ **predictions** (which are indeed now actually **predictions** as opposed to **fitted values**), then we are now  calculating an \"out of sample\" **model performance** metric._\n",
    "> \n",
    "> _When an \"out of sample\" metric performs more poorly than a comparitive \"in sample\" metric, then the **predictions** of the **fitted model** are not **generalizing** to data being the dataset the model is fit on. In this case we say the model is **overfit** (to the data its fit was based on). The purpose of using different **training** and **testing** datasets is to consider \"in sample\" versus \"out of sample\" **model performance** in order to try to confirm that the model is not **overfit** and that the **predictions** do indeed seem to **generalizable** beyond the dataset used for **model fitting**._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4dd90ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788a1fd",
   "metadata": {},
   "source": [
    "### Code Summary:\n",
    "\n",
    "This code prepares the Pokémon data for training a model by splitting it into two parts: one for training and one for testing. Here's what it does step-by-step:\n",
    "\n",
    "1. **Set Up a 50/50 Split Size**:\n",
    "   - `fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)` calculates half of the total number of rows in the dataset (`pokeaman`) to determine the size of the training set.\n",
    "\n",
    "2. **Handle Missing Data**:\n",
    "   - `pokeaman.fillna('None', inplace=True)` replaces any missing values (NaN) in the dataset, specifically in the \"Type 2\" column, with the word 'None' so that no data is left missing.\n",
    "\n",
    "3. **Ensure Consistent Randomness**:\n",
    "   - `np.random.seed(130)` sets the random seed to 130, ensuring that every time the data is split, it is split in the exact same way.\n",
    "\n",
    "4. **Split the Data into Training and Testing Sets**:\n",
    "   - `pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=fifty_fifty_split_size)` splits the dataset (`pokeaman`) into two parts: \n",
    "     - `pokeaman_train` (half of the data for training)\n",
    "     - `pokeaman_test` (the other half for testing).\n",
    "\n",
    "5. **Show the Training Data**:\n",
    "   - `pokeaman_train` displays the training dataset that will be used to teach the model.\n",
    "\n",
    "In short, this code splits the Pokémon dataset into two parts: one for training and one for testing, while also handling any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69a8b379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:33:45</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     01:33:45     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        01:33:45   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f77f3",
   "metadata": {},
   "source": [
    "### Code Summary:\n",
    "\n",
    "This code is fitting a linear regression model to predict the Pokémon's **HP (Health Points)** based on the **Attack** and **Defense** values from the training dataset. Here's a breakdown of what each part of the code is doing:\n",
    "\n",
    "1. **Model Specification**:\n",
    "   - `model_spec3 = smf.ols(formula='HP ~ Attack + Defense', data=pokeaman_train)`:\n",
    "     - This defines a linear regression model where we want to predict `HP` (the outcome or dependent variable) using `Attack` and `Defense` as the predictor (independent) variables.\n",
    "     - `smf.ols()` is a function from the `statsmodels` library that creates an ordinary least squares (OLS) regression model.\n",
    "\n",
    "2. **Fit the Model**:\n",
    "   - `model3_fit = model_spec3.fit()`:\n",
    "     - This line fits the model to the `pokeaman_train` data. Essentially, it finds the best-fit line that predicts `HP` based on the values of `Attack` and `Defense` in the training data.\n",
    "\n",
    "3. **Model Summary**:\n",
    "   - `model3_fit.summary()`:\n",
    "     - This provides a detailed summary of the fitted model, including statistics like the coefficients for the predictor variables (`Attack` and `Defense`), the R-squared value, p-values, and other diagnostics that tell us how well the model fits the data and the significance of the predictors.\n",
    "\n",
    "### What it's doing:\n",
    "This code builds a linear model to understand how the Pokémon's **HP** depends on its **Attack** and **Defense** stats. The summary output will show if those two predictors are statistically significant and how much they explain the variation in the Pokémon's HP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac52c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127a55d",
   "metadata": {},
   "source": [
    "### Code Summary:\n",
    "\n",
    "This code is calculating and printing two types of **R-squared values**: one for the training data (in-sample) and another for the test data (out-of-sample). These values are used to evaluate how well the linear regression model fits both the training data and the new, unseen test data.\n",
    "\n",
    "1. **Making Predictions**:\n",
    "   - `yhat_model3 = model3_fit.predict(pokeaman_test)`:\n",
    "     - This line uses the trained model (`model3_fit`) to make predictions on the **test data** (`pokeaman_test`). These predictions are the model's best guess for the HP values of Pokémon in the test dataset.\n",
    "     - The predictions are stored in `yhat_model3`.\n",
    "\n",
    "2. **Extracting the Actual HP Values**:\n",
    "   - `y = pokeaman_test.HP`:\n",
    "     - This stores the actual **HP** values from the test data (`pokeaman_test`) in the variable `y`. These are the real values that the model is trying to predict.\n",
    "\n",
    "3. **'In Sample' R-squared**:\n",
    "   - `print(\"'In sample' R-squared:    \", model3_fit.rsquared)`:\n",
    "     - This prints the **R-squared value** for the training data (in-sample), which tells us how much of the variability in **HP** can be explained by the model when it's applied to the training data.\n",
    "     - The `rsquared` attribute of `model3_fit` gives this value directly.\n",
    "\n",
    "4. **'Out of Sample' R-squared**:\n",
    "   - `print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)`:\n",
    "     - This prints the **R-squared value** for the test data (out-of-sample), which tells us how well the model can predict the **HP** values of new Pokémon that it hasn’t seen before.\n",
    "     - It calculates this by first computing the correlation between the actual (`y`) and predicted (`yhat_model3`) HP values, and then squaring this correlation to get the **R-squared**.\n",
    "\n",
    "### What it's doing:\n",
    "- **In-sample R-squared** measures how well the model fits the data it was trained on.\n",
    "- **Out-of-sample R-squared** measures how well the model generalizes to new data (the test set).\n",
    "- These values help assess the performance of the model both on the data it has seen and on unseen data, indicating its ability to make accurate predictions for future or unseen Pokémon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cfa038c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.23e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:36:00</td>     <th>  Log-Likelihood:    </th> <td> -1738.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3603.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   337</td>      <th>  BIC:               </th> <td>   3855.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    62</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td>  521.5715</td> <td>  130.273</td> <td>    4.004</td> <td> 0.000</td> <td>  265.322</td> <td>  777.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                                                <td>   -6.1179</td> <td>    2.846</td> <td>   -2.150</td> <td> 0.032</td> <td>  -11.716</td> <td>   -0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                                           <td>   -8.1938</td> <td>    2.329</td> <td>   -3.518</td> <td> 0.000</td> <td>  -12.775</td> <td>   -3.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                                         <td>-1224.9610</td> <td>  545.105</td> <td>   -2.247</td> <td> 0.025</td> <td>-2297.199</td> <td> -152.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                                          <td>   -6.1989</td> <td>    2.174</td> <td>   -2.851</td> <td> 0.005</td> <td>  -10.475</td> <td>   -1.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]</th>                                        <td> -102.4030</td> <td>   96.565</td> <td>   -1.060</td> <td> 0.290</td> <td> -292.350</td> <td>   87.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense</th>                                                   <td>    0.0985</td> <td>    0.033</td> <td>    2.982</td> <td> 0.003</td> <td>    0.034</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]</th>                                 <td>   14.6361</td> <td>    6.267</td> <td>    2.336</td> <td> 0.020</td> <td>    2.310</td> <td>   26.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                                            <td>   -7.2261</td> <td>    2.178</td> <td>   -3.318</td> <td> 0.001</td> <td>  -11.511</td> <td>   -2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]</th>                                          <td>  704.8798</td> <td>  337.855</td> <td>    2.086</td> <td> 0.038</td> <td>   40.309</td> <td> 1369.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                                                     <td>    0.1264</td> <td>    0.038</td> <td>    3.351</td> <td> 0.001</td> <td>    0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]</th>                                   <td>    5.8648</td> <td>    2.692</td> <td>    2.179</td> <td> 0.030</td> <td>    0.570</td> <td>   11.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed</th>                                                    <td>    0.1026</td> <td>    0.039</td> <td>    2.634</td> <td> 0.009</td> <td>    0.026</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]</th>                                  <td>   -6.9266</td> <td>    3.465</td> <td>   -1.999</td> <td> 0.046</td> <td>  -13.742</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed</th>                                             <td>   -0.0016</td> <td>    0.001</td> <td>   -2.837</td> <td> 0.005</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]</th>                           <td>   -0.0743</td> <td>    0.030</td> <td>   -2.477</td> <td> 0.014</td> <td>   -0.133</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                                                     <td>   -5.3982</td> <td>    1.938</td> <td>   -2.785</td> <td> 0.006</td> <td>   -9.211</td> <td>   -1.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\")</th>                                   <td> -282.2496</td> <td>  126.835</td> <td>   -2.225</td> <td> 0.027</td> <td> -531.738</td> <td>  -32.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                                              <td>    0.1094</td> <td>    0.034</td> <td>    3.233</td> <td> 0.001</td> <td>    0.043</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\")</th>                            <td>   12.6503</td> <td>    5.851</td> <td>    2.162</td> <td> 0.031</td> <td>    1.141</td> <td>   24.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\")</th>                                             <td>    0.0628</td> <td>    0.028</td> <td>    2.247</td> <td> 0.025</td> <td>    0.008</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                           <td>    3.3949</td> <td>    1.783</td> <td>    1.904</td> <td> 0.058</td> <td>   -0.112</td> <td>    6.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\")</th>                                      <td>   -0.0012</td> <td>    0.000</td> <td>   -2.730</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                    <td>   -0.1456</td> <td>    0.065</td> <td>   -2.253</td> <td> 0.025</td> <td>   -0.273</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                                               <td>    0.0624</td> <td>    0.031</td> <td>    2.027</td> <td> 0.043</td> <td>    0.002</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                             <td>   -3.2219</td> <td>    1.983</td> <td>   -1.625</td> <td> 0.105</td> <td>   -7.122</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>                                        <td>   -0.0014</td> <td>    0.001</td> <td>   -2.732</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                      <td>   -0.0695</td> <td>    0.033</td> <td>   -2.100</td> <td> 0.036</td> <td>   -0.135</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\")</th>                                       <td>   -0.0008</td> <td>    0.000</td> <td>   -1.743</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                     <td>    0.0334</td> <td>    0.021</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\")</th>                                <td> 1.629e-05</td> <td> 6.92e-06</td> <td>    2.355</td> <td> 0.019</td> <td> 2.68e-06</td> <td> 2.99e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>              <td>    0.0008</td> <td>    0.000</td> <td>    2.433</td> <td> 0.015</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                                                     <td>   -8.3636</td> <td>    2.346</td> <td>   -3.565</td> <td> 0.000</td> <td>  -12.978</td> <td>   -3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Atk\")</th>                                   <td>  850.5436</td> <td>  385.064</td> <td>    2.209</td> <td> 0.028</td> <td>   93.112</td> <td> 1607.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                                              <td>    0.1388</td> <td>    0.040</td> <td>    3.500</td> <td> 0.001</td> <td>    0.061</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Atk\")</th>                            <td>    2.1809</td> <td>    1.136</td> <td>    1.920</td> <td> 0.056</td> <td>   -0.054</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Atk\")</th>                                             <td>    0.0831</td> <td>    0.038</td> <td>    2.162</td> <td> 0.031</td> <td>    0.007</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                           <td>   -7.3121</td> <td>    3.376</td> <td>   -2.166</td> <td> 0.031</td> <td>  -13.953</td> <td>   -0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Atk\")</th>                                      <td>   -0.0014</td> <td>    0.001</td> <td>   -2.480</td> <td> 0.014</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                    <td>   -0.0434</td> <td>    0.022</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                                               <td>    0.1011</td> <td>    0.035</td> <td>    2.872</td> <td> 0.004</td> <td>    0.032</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                             <td>  -12.6343</td> <td>    5.613</td> <td>   -2.251</td> <td> 0.025</td> <td>  -23.674</td> <td>   -1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>                                        <td>   -0.0018</td> <td>    0.001</td> <td>   -3.102</td> <td> 0.002</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                      <td>    0.0151</td> <td>    0.009</td> <td>    1.609</td> <td> 0.109</td> <td>   -0.003</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Atk\")</th>                                       <td>   -0.0012</td> <td>    0.001</td> <td>   -1.860</td> <td> 0.064</td> <td>   -0.002</td> <td> 6.62e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                     <td>    0.1210</td> <td>    0.054</td> <td>    2.260</td> <td> 0.024</td> <td>    0.016</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Atk\")</th>                                <td> 2.125e-05</td> <td>  9.1e-06</td> <td>    2.334</td> <td> 0.020</td> <td> 3.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>              <td> 6.438e-06</td> <td> 7.69e-05</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                        <td>    0.1265</td> <td>    0.033</td> <td>    3.821</td> <td> 0.000</td> <td>    0.061</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                      <td>   -5.0544</td> <td>    2.506</td> <td>   -2.017</td> <td> 0.044</td> <td>   -9.983</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                 <td>   -0.0021</td> <td>    0.001</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>               <td>   -0.0346</td> <td>    0.017</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.069</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                <td>   -0.0012</td> <td>    0.000</td> <td>   -2.406</td> <td> 0.017</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0446</td> <td>    0.025</td> <td>    1.794</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                         <td> 1.973e-05</td> <td> 7.28e-06</td> <td>    2.710</td> <td> 0.007</td> <td> 5.41e-06</td> <td>  3.4e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.957</td> <td> 0.051</td> <td>-2.56e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                  <td>   -0.0013</td> <td>    0.000</td> <td>   -2.740</td> <td> 0.006</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                <td>    0.0841</td> <td>    0.040</td> <td>    2.125</td> <td> 0.034</td> <td>    0.006</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                           <td> 2.379e-05</td> <td> 7.85e-06</td> <td>    3.030</td> <td> 0.003</td> <td> 8.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>         <td> 2.864e-05</td> <td> 7.73e-05</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                          <td> 1.284e-05</td> <td> 7.46e-06</td> <td>    1.721</td> <td> 0.086</td> <td>-1.83e-06</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0008</td> <td>    0.000</td> <td>   -2.085</td> <td> 0.038</td> <td>   -0.002</td> <td>-4.68e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                   <td> -2.53e-07</td> <td>  1.1e-07</td> <td>   -2.292</td> <td> 0.023</td> <td> -4.7e-07</td> <td>-3.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>-1.425e-06</td> <td> 1.14e-06</td> <td>   -1.249</td> <td> 0.212</td> <td>-3.67e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.2e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                   &        HP        & \\textbf{  R-squared:         } &     0.467   \\\\\n",
       "\\textbf{Model:}                                                           &       OLS        & \\textbf{  Adj. R-squared:    } &     0.369   \\\\\n",
       "\\textbf{Method:}                                                          &  Least Squares   & \\textbf{  F-statistic:       } &     4.764   \\\\\n",
       "\\textbf{Date:}                                                            & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.23e-21   \\\\\n",
       "\\textbf{Time:}                                                            &     01:36:00     & \\textbf{  Log-Likelihood:    } &   -1738.6   \\\\\n",
       "\\textbf{No. Observations:}                                                &         400      & \\textbf{  AIC:               } &     3603.   \\\\\n",
       "\\textbf{Df Residuals:}                                                    &         337      & \\textbf{  BIC:               } &     3855.   \\\\\n",
       "\\textbf{Df Model:}                                                        &          62      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                                 &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                        &     521.5715  &      130.273     &     4.004  &         0.000        &      265.322    &      777.821     \\\\\n",
       "\\textbf{Legendary[T.True]}                                                &      -6.1179  &        2.846     &    -2.150  &         0.032        &      -11.716    &       -0.520     \\\\\n",
       "\\textbf{Attack}                                                           &      -8.1938  &        2.329     &    -3.518  &         0.000        &      -12.775    &       -3.612     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                                         &   -1224.9610  &      545.105     &    -2.247  &         0.025        &    -2297.199    &     -152.723     \\\\\n",
       "\\textbf{Defense}                                                          &      -6.1989  &        2.174     &    -2.851  &         0.005        &      -10.475    &       -1.923     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]}                                        &    -102.4030  &       96.565     &    -1.060  &         0.290        &     -292.350    &       87.544     \\\\\n",
       "\\textbf{Attack:Defense}                                                   &       0.0985  &        0.033     &     2.982  &         0.003        &        0.034    &        0.164     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]}                                 &      14.6361  &        6.267     &     2.336  &         0.020        &        2.310    &       26.963     \\\\\n",
       "\\textbf{Speed}                                                            &      -7.2261  &        2.178     &    -3.318  &         0.001        &      -11.511    &       -2.942     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]}                                          &     704.8798  &      337.855     &     2.086  &         0.038        &       40.309    &     1369.450     \\\\\n",
       "\\textbf{Attack:Speed}                                                     &       0.1264  &        0.038     &     3.351  &         0.001        &        0.052    &        0.201     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]}                                   &       5.8648  &        2.692     &     2.179  &         0.030        &        0.570    &       11.160     \\\\\n",
       "\\textbf{Defense:Speed}                                                    &       0.1026  &        0.039     &     2.634  &         0.009        &        0.026    &        0.179     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]}                                  &      -6.9266  &        3.465     &    -1.999  &         0.046        &      -13.742    &       -0.111     \\\\\n",
       "\\textbf{Attack:Defense:Speed}                                             &      -0.0016  &        0.001     &    -2.837  &         0.005        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]}                           &      -0.0743  &        0.030     &    -2.477  &         0.014        &       -0.133    &       -0.015     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                                                     &      -5.3982  &        1.938     &    -2.785  &         0.006        &       -9.211    &       -1.586     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\")}                                   &    -282.2496  &      126.835     &    -2.225  &         0.027        &     -531.738    &      -32.761     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                                              &       0.1094  &        0.034     &     3.233  &         0.001        &        0.043    &        0.176     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\")}                            &      12.6503  &        5.851     &     2.162  &         0.031        &        1.141    &       24.160     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\")}                                             &       0.0628  &        0.028     &     2.247  &         0.025        &        0.008    &        0.118     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\")}                           &       3.3949  &        1.783     &     1.904  &         0.058        &       -0.112    &        6.902     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\")}                                      &      -0.0012  &        0.000     &    -2.730  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")}                    &      -0.1456  &        0.065     &    -2.253  &         0.025        &       -0.273    &       -0.018     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                                               &       0.0624  &        0.031     &     2.027  &         0.043        &        0.002    &        0.123     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\")}                             &      -3.2219  &        1.983     &    -1.625  &         0.105        &       -7.122    &        0.678     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}                                        &      -0.0014  &        0.001     &    -2.732  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                      &      -0.0695  &        0.033     &    -2.100  &         0.036        &       -0.135    &       -0.004     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\")}                                       &      -0.0008  &        0.000     &    -1.743  &         0.082        &       -0.002    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                     &       0.0334  &        0.021     &     1.569  &         0.117        &       -0.008    &        0.075     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\")}                                &    1.629e-05  &     6.92e-06     &     2.355  &         0.019        &     2.68e-06    &     2.99e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}              &       0.0008  &        0.000     &     2.433  &         0.015        &        0.000    &        0.001     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                                                     &      -8.3636  &        2.346     &    -3.565  &         0.000        &      -12.978    &       -3.749     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Atk\")}                                   &     850.5436  &      385.064     &     2.209  &         0.028        &       93.112    &     1607.975     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                                              &       0.1388  &        0.040     &     3.500  &         0.001        &        0.061    &        0.217     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Atk\")}                            &       2.1809  &        1.136     &     1.920  &         0.056        &       -0.054    &        4.416     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Atk\")}                                             &       0.0831  &        0.038     &     2.162  &         0.031        &        0.007    &        0.159     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                           &      -7.3121  &        3.376     &    -2.166  &         0.031        &      -13.953    &       -0.671     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Atk\")}                                      &      -0.0014  &        0.001     &    -2.480  &         0.014        &       -0.003    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                    &      -0.0434  &        0.022     &    -2.010  &         0.045        &       -0.086    &       -0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                                               &       0.1011  &        0.035     &     2.872  &         0.004        &        0.032    &        0.170     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                             &     -12.6343  &        5.613     &    -2.251  &         0.025        &      -23.674    &       -1.594     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}                                        &      -0.0018  &        0.001     &    -3.102  &         0.002        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                      &       0.0151  &        0.009     &     1.609  &         0.109        &       -0.003    &        0.034     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Atk\")}                                       &      -0.0012  &        0.001     &    -1.860  &         0.064        &       -0.002    &     6.62e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                     &       0.1210  &        0.054     &     2.260  &         0.024        &        0.016    &        0.226     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Atk\")}                                &    2.125e-05  &      9.1e-06     &     2.334  &         0.020        &     3.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}              &    6.438e-06  &     7.69e-05     &     0.084  &         0.933        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                        &       0.1265  &        0.033     &     3.821  &         0.000        &        0.061    &        0.192     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                      &      -5.0544  &        2.506     &    -2.017  &         0.044        &       -9.983    &       -0.126     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                 &      -0.0021  &        0.001     &    -3.606  &         0.000        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}               &      -0.0346  &        0.017     &    -1.992  &         0.047        &       -0.069    &       -0.000     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                &      -0.0012  &        0.000     &    -2.406  &         0.017        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0446  &        0.025     &     1.794  &         0.074        &       -0.004    &        0.093     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                         &    1.973e-05  &     7.28e-06     &     2.710  &         0.007        &     5.41e-06    &      3.4e-05     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &       0.0005  &        0.000     &     1.957  &         0.051        &    -2.56e-06    &        0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                  &      -0.0013  &        0.000     &    -2.740  &         0.006        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                &       0.0841  &        0.040     &     2.125  &         0.034        &        0.006    &        0.162     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                           &    2.379e-05  &     7.85e-06     &     3.030  &         0.003        &     8.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}         &    2.864e-05  &     7.73e-05     &     0.370  &         0.711        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                          &    1.284e-05  &     7.46e-06     &     1.721  &         0.086        &    -1.83e-06    &     2.75e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0008  &        0.000     &    -2.085  &         0.038        &       -0.002    &    -4.68e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                   &    -2.53e-07  &      1.1e-07     &    -2.292  &         0.023        &     -4.7e-07    &    -3.59e-08     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &   -1.425e-06  &     1.14e-06     &    -1.249  &         0.212        &    -3.67e-06    &     8.19e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.2e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.467\n",
       "Model:                            OLS   Adj. R-squared:                  0.369\n",
       "Method:                 Least Squares   F-statistic:                     4.764\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           4.23e-21\n",
       "Time:                        01:36:00   Log-Likelihood:                -1738.6\n",
       "No. Observations:                 400   AIC:                             3603.\n",
       "Df Residuals:                     337   BIC:                             3855.\n",
       "Df Model:                          62                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                          521.5715    130.273      4.004      0.000     265.322     777.821\n",
       "Legendary[T.True]                                                   -6.1179      2.846     -2.150      0.032     -11.716      -0.520\n",
       "Attack                                                              -8.1938      2.329     -3.518      0.000     -12.775      -3.612\n",
       "Attack:Legendary[T.True]                                         -1224.9610    545.105     -2.247      0.025   -2297.199    -152.723\n",
       "Defense                                                             -6.1989      2.174     -2.851      0.005     -10.475      -1.923\n",
       "Defense:Legendary[T.True]                                         -102.4030     96.565     -1.060      0.290    -292.350      87.544\n",
       "Attack:Defense                                                       0.0985      0.033      2.982      0.003       0.034       0.164\n",
       "Attack:Defense:Legendary[T.True]                                    14.6361      6.267      2.336      0.020       2.310      26.963\n",
       "Speed                                                               -7.2261      2.178     -3.318      0.001     -11.511      -2.942\n",
       "Speed:Legendary[T.True]                                            704.8798    337.855      2.086      0.038      40.309    1369.450\n",
       "Attack:Speed                                                         0.1264      0.038      3.351      0.001       0.052       0.201\n",
       "Attack:Speed:Legendary[T.True]                                       5.8648      2.692      2.179      0.030       0.570      11.160\n",
       "Defense:Speed                                                        0.1026      0.039      2.634      0.009       0.026       0.179\n",
       "Defense:Speed:Legendary[T.True]                                     -6.9266      3.465     -1.999      0.046     -13.742      -0.111\n",
       "Attack:Defense:Speed                                                -0.0016      0.001     -2.837      0.005      -0.003      -0.001\n",
       "Attack:Defense:Speed:Legendary[T.True]                              -0.0743      0.030     -2.477      0.014      -0.133      -0.015\n",
       "Q(\"Sp. Def\")                                                        -5.3982      1.938     -2.785      0.006      -9.211      -1.586\n",
       "Legendary[T.True]:Q(\"Sp. Def\")                                    -282.2496    126.835     -2.225      0.027    -531.738     -32.761\n",
       "Attack:Q(\"Sp. Def\")                                                  0.1094      0.034      3.233      0.001       0.043       0.176\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\")                               12.6503      5.851      2.162      0.031       1.141      24.160\n",
       "Defense:Q(\"Sp. Def\")                                                 0.0628      0.028      2.247      0.025       0.008       0.118\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\")                               3.3949      1.783      1.904      0.058      -0.112       6.902\n",
       "Attack:Defense:Q(\"Sp. Def\")                                         -0.0012      0.000     -2.730      0.007      -0.002      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")                       -0.1456      0.065     -2.253      0.025      -0.273      -0.018\n",
       "Speed:Q(\"Sp. Def\")                                                   0.0624      0.031      2.027      0.043       0.002       0.123\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\")                                -3.2219      1.983     -1.625      0.105      -7.122       0.678\n",
       "Attack:Speed:Q(\"Sp. Def\")                                           -0.0014      0.001     -2.732      0.007      -0.002      -0.000\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         -0.0695      0.033     -2.100      0.036      -0.135      -0.004\n",
       "Defense:Speed:Q(\"Sp. Def\")                                          -0.0008      0.000     -1.743      0.082      -0.002       0.000\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         0.0334      0.021      1.569      0.117      -0.008       0.075\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\")                                 1.629e-05   6.92e-06      2.355      0.019    2.68e-06    2.99e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                  0.0008      0.000      2.433      0.015       0.000       0.001\n",
       "Q(\"Sp. Atk\")                                                        -8.3636      2.346     -3.565      0.000     -12.978      -3.749\n",
       "Legendary[T.True]:Q(\"Sp. Atk\")                                     850.5436    385.064      2.209      0.028      93.112    1607.975\n",
       "Attack:Q(\"Sp. Atk\")                                                  0.1388      0.040      3.500      0.001       0.061       0.217\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Atk\")                                2.1809      1.136      1.920      0.056      -0.054       4.416\n",
       "Defense:Q(\"Sp. Atk\")                                                 0.0831      0.038      2.162      0.031       0.007       0.159\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Atk\")                              -7.3121      3.376     -2.166      0.031     -13.953      -0.671\n",
       "Attack:Defense:Q(\"Sp. Atk\")                                         -0.0014      0.001     -2.480      0.014      -0.003      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")                       -0.0434      0.022     -2.010      0.045      -0.086      -0.001\n",
       "Speed:Q(\"Sp. Atk\")                                                   0.1011      0.035      2.872      0.004       0.032       0.170\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Atk\")                               -12.6343      5.613     -2.251      0.025     -23.674      -1.594\n",
       "Attack:Speed:Q(\"Sp. Atk\")                                           -0.0018      0.001     -3.102      0.002      -0.003      -0.001\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                          0.0151      0.009      1.609      0.109      -0.003       0.034\n",
       "Defense:Speed:Q(\"Sp. Atk\")                                          -0.0012      0.001     -1.860      0.064      -0.002    6.62e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                         0.1210      0.054      2.260      0.024       0.016       0.226\n",
       "Attack:Defense:Speed:Q(\"Sp. Atk\")                                 2.125e-05    9.1e-06      2.334      0.020    3.34e-06    3.92e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")               6.438e-06   7.69e-05      0.084      0.933      -0.000       0.000\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                            0.1265      0.033      3.821      0.000       0.061       0.192\n",
       "Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                         -5.0544      2.506     -2.017      0.044      -9.983      -0.126\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                    -0.0021      0.001     -3.606      0.000      -0.003      -0.001\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  -0.0346      0.017     -1.992      0.047      -0.069      -0.000\n",
       "Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                   -0.0012      0.000     -2.406      0.017      -0.002      -0.000\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0446      0.025      1.794      0.074      -0.004       0.093\n",
       "Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                          1.973e-05   7.28e-06      2.710      0.007    5.41e-06     3.4e-05\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           0.0005      0.000      1.957      0.051   -2.56e-06       0.001\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                     -0.0013      0.000     -2.740      0.006      -0.002      -0.000\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    0.0841      0.040      2.125      0.034       0.006       0.162\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                            2.379e-05   7.85e-06      3.030      0.003    8.34e-06    3.92e-05\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          2.864e-05   7.73e-05      0.370      0.711      -0.000       0.000\n",
       "Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                           1.284e-05   7.46e-06      1.721      0.086   -1.83e-06    2.75e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0008      0.000     -2.085      0.038      -0.002   -4.68e-05\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    -2.53e-07    1.1e-07     -2.292      0.023    -4.7e-07   -3.59e-08\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\") -1.425e-06   1.14e-06     -1.249      0.212   -3.67e-06    8.19e-07\n",
       "==============================================================================\n",
       "Omnibus:                      214.307   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2354.664\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.174   Cond. No.                     1.20e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.2e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53985f",
   "metadata": {},
   "source": [
    "### Code Summary:\n",
    "\n",
    "This code is setting up and fitting a **multiple linear regression model** with an extremely complex formula, incorporating various **interaction terms** between the predictor variables. Here's a step-by-step explanation of what it's doing:\n",
    "\n",
    "1. **Creating the Linear Formula**:\n",
    "   - `model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'`:\n",
    "     - This defines a linear regression formula where the **HP** (target variable) is being predicted by the following predictor variables: **Attack**, **Defense**, **Speed**, and **Legendary**.\n",
    "     - The asterisk (`*`) indicates interaction terms between the variables. In a regression context, an interaction term allows the effect of one variable to depend on the value of another. For example, **Attack** might affect **HP** differently depending on the value of **Speed**.\n",
    "     - This first part of the formula includes **main effects** for the variables **Attack**, **Defense**, **Speed**, and **Legendary**, and also their **pairwise interaction terms** (i.e., the effect of combinations like **Attack * Defense**, **Attack * Speed**, etc.).\n",
    "\n",
    "   - `model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'`:\n",
    "     - This adds more interaction terms between the **Sp. Def** (Special Defense) and **Sp. Atk** (Special Attack) variables. The `Q()` is used to handle columns with spaces in their names (e.g., \"Sp. Def\").\n",
    "     - The asterisk (`*`) again indicates that not only should the main effects of **Sp. Def** and **Sp. Atk** be included, but also their interactions with all other variables defined earlier in the formula.\n",
    "\n",
    "   - **Note about Complexity**:\n",
    "     - The comment in the code warns about adding even more interaction terms, particularly ones involving categorical variables like **Generation** or **Type 1** and **Type 2**, which would drastically increase the number of possible interactions and cause computational problems. Adding these terms would create a huge model with thousands of interactions, which could overwhelm your computer and make the model unmanageable.\n",
    "\n",
    "2. **Specifying and Fitting the Model**:\n",
    "   - `model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)`:\n",
    "     - This sets up the **OLS (Ordinary Least Squares)** regression model using the specified formula and the training data (`pokeaman_train`).\n",
    "     - The formula defines how **HP** is predicted by the variables and their interactions.\n",
    "\n",
    "   - `model4_fit = model4_spec.fit()`:\n",
    "     - This fits the model to the data, meaning it calculates the coefficients for the variables in the formula that best explain **HP** based on the training data.\n",
    "\n",
    "3. **Model Summary**:\n",
    "   - `model4_fit.summary()`:\n",
    "     - This generates a summary of the model’s performance, which includes the coefficients for each predictor variable, the R-squared value, p-values, and other statistical details about the model’s fit.\n",
    "\n",
    "### What it's doing:\n",
    "- This code is **building a very complex regression model** that includes a large number of **interaction terms** between variables, including multiple Pokémon attributes like **Attack**, **Defense**, **Speed**, and **Legendary** status, as well as **Special Defense** and **Special Attack**.\n",
    "- The purpose is to explore how these variables, both individually and in combination, affect the target variable **HP**.\n",
    "- The formula is intentionally complex, with multiple interactions, and is aimed at discovering intricate relationships between the variables.\n",
    "- However, the code also includes a warning about adding even more interactions, which could lead to an overwhelming computational load.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61d225cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4cc0e",
   "metadata": {},
   "source": [
    "### Code Summary:\n",
    "\n",
    "This last bit of code is evaluating the performance of the **model4** regression model using both the **in-sample** and **out-of-sample** R-squared values. Here's a step-by-step breakdown of what the code is doing:\n",
    "\n",
    "1. **Predicting 'HP' on Test Data**:\n",
    "   - `yhat_model4 = model4_fit.predict(pokeaman_test)`:\n",
    "     - This line generates the predicted **HP** values (`yhat_model4`) for the Pokémon in the **test set** (`pokeaman_test`) using the **fitted model** (`model4_fit`).\n",
    "     - The `predict()` method uses the coefficients calculated during model fitting to predict the **HP** of each Pokémon based on the values of the predictor variables in the test set.\n",
    "\n",
    "2. **Extracting True 'HP' Values**:\n",
    "   - `y = pokeaman_test.HP`:\n",
    "     - This stores the actual **HP** values of the Pokémon in the **test set** into the variable `y`, so that they can be used for comparison against the predicted values.\n",
    "\n",
    "3. **'In-sample' R-squared**:\n",
    "   - `print(\"'In sample' R-squared:    \", model4_fit.rsquared)`:\n",
    "     - This prints the **R-squared** value for the **training data** (in-sample R-squared).\n",
    "     - **In-sample R-squared** measures how well the model fits the data it was trained on. It is calculated as the proportion of the variance in **HP** that is explained by the predictor variables in the model, using the training data.\n",
    "\n",
    "4. **'Out-of-sample' R-squared**:\n",
    "   - `print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model4)[0,1]**2)`:\n",
    "     - This calculates the **out-of-sample R-squared**, which measures how well the model predicts the **HP** values for new, unseen data (i.e., the test data).\n",
    "     - It uses the **correlation coefficient** between the actual **HP** values (`y`) and the predicted values (`yhat_model4`) from the test set. The correlation coefficient is squared (`**2`) to give the **R-squared** value.\n",
    "     - **Out-of-sample R-squared** is a measure of the model's ability to generalize and make accurate predictions on data it hasn't seen before.\n",
    "\n",
    "### What it's doing:\n",
    "- **In-sample R-squared** shows how well the model fits the training data, indicating the proportion of variance in **HP** explained by the model.\n",
    "- **Out-of-sample R-squared** assesses the model's ability to generalize to unseen data (test data), providing insight into how well it will perform on future Pokémon data.\n",
    "- The code compares the model's fit to the training data with its predictive performance on the test data, offering both an **internal** (training) and **external** (test) measure of model quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e45cc",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "\n",
    "The results from the five code cells illustrate the performance of different models based on **in-sample** and **out-of-sample** R-squared values. Here's a summary of the key observations:\n",
    "\n",
    "1. **In-sample vs. Out-of-sample R-squared**:\n",
    "   - **In-sample R-squared** indicates how well the model fits the **training data**, i.e., how much of the variation in the outcome variable (HP) is explained by the predictor variables used in the model.\n",
    "   - **Out-of-sample R-squared** measures how well the model predicts the **test data**, i.e., how much of the variation in HP is explained by the predictor variables when applied to new, unseen data.\n",
    "\n",
    "2. **Model Performance Comparison**:\n",
    "   - In one model, the **in-sample R-squared** was `0.4671` and the **out-of-sample R-squared** was drastically lower at `0.0025`. This suggests that while the model fits the training data reasonably well, it **fails to generalize** to the test data, indicating **overfitting**. Overfitting occurs when a model captures noise or specific patterns in the training data that don't apply to new data.\n",
    "   - In another model, the **in-sample R-squared** was `0.1477` and the **out-of-sample R-squared** was `0.2121`. These R-squared values are more comparable, suggesting that the model is performing **more consistently** across both the training and test datasets. This indicates a **better balance** between model complexity and generalization, and it is **less likely to be overfitting** than the previous model.\n",
    "\n",
    "### Key Takeaways:\n",
    "- The second model is a **super-fitted model** that performs well on the training data but struggles with **generalization** on the test data, as evidenced by the drastically different R-squared values.\n",
    "- The first model shows **more balanced performance**, with similar R-squared values for both in-sample and out-of-sample, suggesting it may generalize better to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecc5b7",
   "metadata": {},
   "source": [
    "### 6. Work with a ChatBot to understand how the *model4_linear_form* (*linear form* specification of  *model4*) creates new *predictor variables* as the columns of the so-called \"design matrix\" *model4_spec.exog* (*model4_spec.exog.shape*) used to predict the *outcome variable*  *model4_spec.endog* and why the so-called *multicollinearity* in this \"design matrix\" (observed in *np.corrcoef(model4_spec.exog)*) contribues to the lack of \"out of sample\" *generalization* of *predictions* from *model4_fit*; then, explain this consisely in your own works<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The **overfitting** observed in the previous problem is a question of **model complexity** relative to the amount of information or evidence available in a given dataset (or we could just say \"the amount of data in a dataset\"). The **model fit** for  `model4` resulted in an **overfit model** because the specification of its **linear form** was overly complex (relative to the the amount of available data). Indeed, `model4` is rediculously complex as can be seen from `model4_fit.summary()`. This in turn allowed the **model fit** to \"detect\" idiosyncratic associations spuriously present specifically in the **training** dataset but which did not **generalize** to the **testing** dataset. If a model is too **complex** then it will find and model \"patterns\" in a **training** dataset which are actually just accidental \"noise\" from the random sampling nature of the dataset. The simpler (or more parsimoneous) `model3` on the other hand was able to identify **predictive associations** in the **training** dataset which did **generalize** to the **testing** dataset. This is because `model3` only sought to understand simpler **predictive association** for which there was enough data in the **training** dataset to reliably detect and obviously identify. And these patterns were indeed sufficiently \"real\" in the sense that they were present and **generalized** into the **testing** dataset as well as the **training** dataset. So they could be \"found\" in the **training** and then used in (**generalized** to) the **testing** dataset._\n",
    "> \n",
    "> _This question therefore addresses the topic of the **evidence** a given dataset provides for the **predictive associations** detected by a **fitted model**. It should be increasingly clear at this point that evidence for a model can be addressed using **coefficent hypothesis testing** in the context of **multiple linear regression**, but that examinations of \"in sample\" versus \"out of sample\" **model performance** metrics are what in fact are directly designed to address this question of **generalizability**. That said, this question introduces another consideration of **multicollinearity** as something that affects the **generalizability** of **model fits** in the **multiple linear regression** context. A good question that a ChatBot could help you understand is (a) \"why is **generalizability** more uncertain if two **predictor variables** are highly **correlated**?\" and (b) \"why is **generalizability** more uncertain if multiple **predictor variables** are highly **multicollinear**?\"_\n",
    ">\n",
    "> _The four code cells below are not necessary for answering this question; however, they introduce two very practical helpful tools for the **multiple linear regression** context that are immediately relevant for this question. The first is the so-called **condition number** (of a \"design matrix\") which provides a very simple diagnostic which can serve as a measure the degree of **multicollinearity** that is present in a **model fit**. If this number is \"very large\" then there is a large degree of **multicollinearity** and suggests room for doubt regarding the **generalizability** of the **fitted model**. The second tool(s) are the `center` and `scale` functions. It is best practice to \"center and scale\" **continuous predictor variables** (but not **indicator variables**) in the **multiple linear regression** context as is done below. While \"centering and scaling\" does make interpreting the predictions on the original scale of the data slighly more complicated, it also must be done in order to get a \"true\" evaluation of the degree of **multicollinearity** present in a **model fit** using the **condition number** of the model (\"design matrix\"). The examples below show that the **condition number** reported by a **fitted model** are \"artificially inflacted\" if \"centering and scaling\" is not used. Specically, they show that the **condition number** of `model3_fit` is really `1.66` (as opposed to the \"very large\" `343` which is reported without \"centering and scaling\"); whereas, the **condition number** for `model4_fit` is \"very (VERY) large\" irrespective of \"centering and scaling\", showing that the overwheling presense of **multicollinearity** in `model4_fit` is in fact a very real thing.  Indeed, we have already seen that `model4_fit` is grossly **overfit** and does not remotely **generalize** beyond its **training** dataset. Without knowing this, however, the comically large **condition number** for `model4_fit` (after \"centering and scaling\") makes it abundantly clear that we should have great doubts about the likely **generalizability** of `model4_fit` (even without examining specific aspects of **multicollinearity** directly or examining \"in sample\" versus \"out of sample\" **model performance** comparisions)._\n",
    ">\n",
    "> - _The \"specific aspects of **multicollinearity**\" reference above refer to understanding and attributing the detrmimental affects of specific **predictor variables** towards **multicollinearity**. This can be done using so-called **variance inflation factors**, but this is beyond the scope of STA130. We should be aware that the presence of excessive **multicollinearity** as indicated by a large **condition number** for a (\"centered and scaled\") **fitted model** raises grave concerns regarding the potential **generalizability** of the model._\n",
    ">\n",
    "> _The `np.corrcoef(model4_spec.exog)` examination of the **correlations** of a \"design matrix\" considered in ths problems prompt is analogous to the examination of the **correlations** present in a dataset that might considered when initially examining the **predictor variables** of a dataset, such as `pokeaman.iloc[:,4:12].corr()`. Indeed, such an examination is often the first step in examining the potential presence of **multicollinearity** among the **predictor variables** of a dataset. However, these are consideration of **pairwise correlation**, whereas **multicollinearity** generalizes this notion to the full collection of **predictor variables** together. A **condition number** for a \"centered and scale\" version of a **fit model** can therefore be viewed as serving the analogous purposes of a multivariate generalization of **pairwise correlation**._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66dd4227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:56:08</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     01:56:08     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        01:56:08   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dedf01b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:56:09</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>              <td>   69.3025</td> <td>    1.186</td> <td>   58.439</td> <td> 0.000</td> <td>   66.971</td> <td>   71.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Attack))</th>  <td>    8.1099</td> <td>    1.340</td> <td>    6.051</td> <td> 0.000</td> <td>    5.475</td> <td>   10.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Defense))</th> <td>    2.9496</td> <td>    1.340</td> <td>    2.201</td> <td> 0.028</td> <td>    0.315</td> <td>    5.585</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    1.66</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}         &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}                &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}                  & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}                  &     01:56:09     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:}      &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}          &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}              &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}              &      69.3025  &        1.186     &    58.439  &         0.000        &       66.971    &       71.634     \\\\\n",
       "\\textbf{scale(center(Attack))}  &       8.1099  &        1.340     &     6.051  &         0.000        &        5.475    &       10.745     \\\\\n",
       "\\textbf{scale(center(Defense))} &       2.9496  &        1.340     &     2.201  &         0.028        &        0.315    &        5.585     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     1.66  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        01:56:09   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Intercept                 69.3025      1.186     58.439      0.000      66.971      71.634\n",
       "scale(center(Attack))      8.1099      1.340      6.051      0.000       5.475      10.745\n",
       "scale(center(Defense))     2.9496      1.340      2.201      0.028       0.315       5.585\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         1.66\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patsy import center, scale\n",
    "\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))' \n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cb64e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.54e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.663  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.54e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary' \n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "\n",
    "# The condition number is still bad even after centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5f19d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just as the condition number was very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a75a5",
   "metadata": {},
   "source": [
    "### Summary of Code\n",
    "\n",
    "The provided code demonstrates the application of **centering** and **scaling** to predictor variables in regression models, aimed at improving model stability and reducing issues related to **multicollinearity**.\n",
    "\n",
    "1. **First Model (`model3_linear_form_center_scale`)**:\n",
    "   - **Centering** and **scaling** are applied to the `Attack` and `Defense` predictors.\n",
    "   - This reduces the **Condition Number** from 343.0 (without centering and scaling) to 1.66, indicating improved stability of the model.\n",
    "\n",
    "2. **Second Model (`model4_linear_form_CS`)**:\n",
    "   - A more complex model is fitted with interactions between multiple predictors, all centered and scaled (except for the `Legendary` variable).\n",
    "   - Despite centering and scaling, the **Condition Number** remains extremely high at **2,250,000,000,000,000**, signaling that multicollinearity still exists in the model.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Centering** and **scaling** improve model stability by normalizing predictors.\n",
    "- A **high Condition Number** indicates potential instability, even after centering and scaling, if multicollinearity is present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf35cd0",
   "metadata": {},
   "source": [
    "### Explanation of the Condition Number and its Impact on Model Stability\n",
    "\n",
    "The **Condition Number (Cond. No.)** is a metric that indicates how sensitive a regression model is to small changes or errors in the input data. A **high Condition Number** suggests that the model might be unstable or overly sensitive to the predictors, potentially leading to unreliable results. In the context of the code provided, the **Condition Number was 343.0** before **centering and scaling** the data, indicating that the model might have been struggling with multicollinearity or large differences in the scale of the predictors.\n",
    "\n",
    "**Centering and scaling** the data involves two key steps:\n",
    "1. **Centering**: Shifting the data so the mean of each predictor is zero.\n",
    "2. **Scaling**: Adjusting the data so each predictor has a similar range or unit of measurement.\n",
    "\n",
    "These steps help to normalize the data, preventing certain predictors from dominating the model due to their larger magnitudes. After centering and scaling, the Condition Number typically decreases, leading to a more stable model that is less prone to errors. This means that the model becomes less \"wobbly\" and can make more reliable predictions, with reduced sensitivity to small variations in the data.\n",
    "\n",
    "In summary, centering and scaling improve the model's stability, making it less susceptible to multicollinearity and improving its ability to generalize to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577c7dd",
   "metadata": {},
   "source": [
    "### 7. Discuss with a ChatBot the rationale and principles by which *model5_linear_form* is  extended and developed from *model3_fit* and *model4_fit*; *model6_linear_form* is  extended and developed from *model5_linear_form*; and *model7_linear_form* is  extended and developed from *model6_linear_form*; then, explain this breifly and consisely in your own words<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _We again include the **condition number** for the \"centered and scaled\" version of `model7_fit` to show that **multicollinearity** does not appear to be a major concern for this model (and the same would be true regarding `model6_fit` if the analogous \"centered and scaled\" version of the model was considered). While it is true that the **condition number** of `15.4` observed for `model7_fit` is perhaps \"large\", this would not be considered \"vary large\"._\n",
    ">\n",
    "> - _Regarding **condition numbers**, a ChatBot gave me cutoffs of `<30` not a big problem, up to `<300` maybe an issue, up to `<1000` definitely **multicollinearity**, and beyond that is pretty much likely to be \"serious\" problems with **multicollinearity**. Personally, cutoffs around `10`, `100`, and `1000` seem about right to me._\n",
    ">\n",
    "> _This question addresses the **model building** exercise using both an **evidence** based approach using **coefficient hypothesis testing** as well as examinations of **generalizability** using comparisions of \"in sample\" versus \"out of sample\" **model performance** metrics. Through these tools, different models were considered, extended, and developed, finally arriving at `model7_fit`. When we feel we can improve the **model performance** in a **generalizable** manner, then all relatively underperforming models are said to be **underfit**, meaning that they do not leverage all the **predictive associations** available to improve **predictions**._\n",
    "> \n",
    "> _While the previous \"Question 6\" above introduced and explored the impact of **multicollinearity** in the **multiple linear regression** context_ \n",
    ">     \n",
    "> - _(whereby \"the effects\" of multiple **predictor variables** are \"tangled up\" and therefore do not allow the model to reliably determine contribution attributions between the **predictor variables**, which potentially leads to poor **estimation** of their \"effects\" in the model, which in turn is the problematic state of affairs which leads to a lack of **generalizability** in such high **multicollinearity** settings)_\n",
    "> \n",
    "> _there is still the (actually even more important) consideration of the actual **evidence** of **predictive associations**. The question is whether or not there is sufficient **evidence** in the data backing up the **estimated** fit of the **linear form** specification. Quantifying the **evidence** for a **estimated** model is a separate question from the problem of **multicollinearity**, the assessment of which is actually the primary purpose of **multiple linear regression** methodology._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5486d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>9.48e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:03:08</td>     <th>  Log-Likelihood:    </th> <td> -1765.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3624.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   353</td>      <th>  BIC:               </th> <td>   3812.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    46</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                  <td>   10.1046</td> <td>   14.957</td> <td>    0.676</td> <td> 0.500</td> <td>  -19.312</td> <td>   39.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>          <td>   -3.2717</td> <td>    4.943</td> <td>   -0.662</td> <td> 0.508</td> <td>  -12.992</td> <td>    6.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>         <td>    9.2938</td> <td>    4.015</td> <td>    2.315</td> <td> 0.021</td> <td>    1.398</td> <td>   17.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>         <td>    2.3150</td> <td>    3.915</td> <td>    0.591</td> <td> 0.555</td> <td>   -5.385</td> <td>   10.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>         <td>    4.8353</td> <td>    4.149</td> <td>    1.165</td> <td> 0.245</td> <td>   -3.325</td> <td>   12.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>         <td>   11.4838</td> <td>    3.960</td> <td>    2.900</td> <td> 0.004</td> <td>    3.696</td> <td>   19.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>         <td>    4.9206</td> <td>    4.746</td> <td>    1.037</td> <td> 0.300</td> <td>   -4.413</td> <td>   14.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Dark]</th>     <td>   -1.4155</td> <td>    6.936</td> <td>   -0.204</td> <td> 0.838</td> <td>  -15.057</td> <td>   12.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Dragon]</th>   <td>    0.8509</td> <td>    6.900</td> <td>    0.123</td> <td> 0.902</td> <td>  -12.720</td> <td>   14.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Electric]</th> <td>   -6.3641</td> <td>    6.537</td> <td>   -0.974</td> <td> 0.331</td> <td>  -19.220</td> <td>    6.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fairy]</th>    <td>   -1.9486</td> <td>   10.124</td> <td>   -0.192</td> <td> 0.847</td> <td>  -21.859</td> <td>   17.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fighting]</th> <td>    7.0308</td> <td>    7.432</td> <td>    0.946</td> <td> 0.345</td> <td>   -7.586</td> <td>   21.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fire]</th>     <td>    3.0779</td> <td>    6.677</td> <td>    0.461</td> <td> 0.645</td> <td>  -10.055</td> <td>   16.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Flying]</th>   <td>   -2.1231</td> <td>   22.322</td> <td>   -0.095</td> <td> 0.924</td> <td>  -46.025</td> <td>   41.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ghost]</th>    <td>    5.7343</td> <td>    8.488</td> <td>    0.676</td> <td> 0.500</td> <td>  -10.960</td> <td>   22.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Grass]</th>    <td>    3.3275</td> <td>    5.496</td> <td>    0.605</td> <td> 0.545</td> <td>   -7.481</td> <td>   14.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ground]</th>   <td>    9.5118</td> <td>    7.076</td> <td>    1.344</td> <td> 0.180</td> <td>   -4.404</td> <td>   23.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ice]</th>      <td>   -0.9313</td> <td>    7.717</td> <td>   -0.121</td> <td> 0.904</td> <td>  -16.108</td> <td>   14.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Normal]</th>   <td>   18.4816</td> <td>    5.312</td> <td>    3.479</td> <td> 0.001</td> <td>    8.034</td> <td>   28.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Poison]</th>   <td>    8.3411</td> <td>    7.735</td> <td>    1.078</td> <td> 0.282</td> <td>   -6.871</td> <td>   23.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Psychic]</th>  <td>    1.8061</td> <td>    6.164</td> <td>    0.293</td> <td> 0.770</td> <td>  -10.317</td> <td>   13.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Rock]</th>     <td>   -3.8558</td> <td>    6.503</td> <td>   -0.593</td> <td> 0.554</td> <td>  -16.645</td> <td>    8.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Steel]</th>    <td>   -4.0053</td> <td>    8.044</td> <td>   -0.498</td> <td> 0.619</td> <td>  -19.826</td> <td>   11.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Water]</th>    <td>    9.7988</td> <td>    5.166</td> <td>    1.897</td> <td> 0.059</td> <td>   -0.361</td> <td>   19.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Dark]</th>     <td>    5.8719</td> <td>   15.185</td> <td>    0.387</td> <td> 0.699</td> <td>  -23.993</td> <td>   35.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Dragon]</th>   <td>   13.2777</td> <td>   14.895</td> <td>    0.891</td> <td> 0.373</td> <td>  -16.016</td> <td>   42.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Electric]</th> <td>   14.3228</td> <td>   17.314</td> <td>    0.827</td> <td> 0.409</td> <td>  -19.728</td> <td>   48.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fairy]</th>    <td>    2.8426</td> <td>   14.268</td> <td>    0.199</td> <td> 0.842</td> <td>  -25.218</td> <td>   30.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fighting]</th> <td>    1.9741</td> <td>   14.089</td> <td>    0.140</td> <td> 0.889</td> <td>  -25.735</td> <td>   29.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fire]</th>     <td>    0.2001</td> <td>   15.730</td> <td>    0.013</td> <td> 0.990</td> <td>  -30.736</td> <td>   31.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Flying]</th>   <td>    6.7292</td> <td>   13.581</td> <td>    0.495</td> <td> 0.621</td> <td>  -19.980</td> <td>   33.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ghost]</th>    <td>  -10.9402</td> <td>   15.895</td> <td>   -0.688</td> <td> 0.492</td> <td>  -42.201</td> <td>   20.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Grass]</th>    <td>    2.5119</td> <td>   14.540</td> <td>    0.173</td> <td> 0.863</td> <td>  -26.084</td> <td>   31.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ground]</th>   <td>   13.6042</td> <td>   13.655</td> <td>    0.996</td> <td> 0.320</td> <td>  -13.250</td> <td>   40.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ice]</th>      <td>   19.7950</td> <td>   15.068</td> <td>    1.314</td> <td> 0.190</td> <td>   -9.840</td> <td>   49.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.None]</th>     <td>    7.6068</td> <td>   13.162</td> <td>    0.578</td> <td> 0.564</td> <td>  -18.279</td> <td>   33.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Normal]</th>   <td>   17.3191</td> <td>   17.764</td> <td>    0.975</td> <td> 0.330</td> <td>  -17.618</td> <td>   52.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Poison]</th>   <td>    0.7770</td> <td>   14.575</td> <td>    0.053</td> <td> 0.958</td> <td>  -27.887</td> <td>   29.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Psychic]</th>  <td>    4.2480</td> <td>   14.174</td> <td>    0.300</td> <td> 0.765</td> <td>  -23.628</td> <td>   32.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Rock]</th>     <td>    6.8858</td> <td>   16.221</td> <td>    0.424</td> <td> 0.671</td> <td>  -25.017</td> <td>   38.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Steel]</th>    <td>  -11.9623</td> <td>   14.973</td> <td>   -0.799</td> <td> 0.425</td> <td>  -41.409</td> <td>   17.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Water]</th>    <td>    5.8097</td> <td>   14.763</td> <td>    0.394</td> <td> 0.694</td> <td>  -23.225</td> <td>   34.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                     <td>    0.2508</td> <td>    0.051</td> <td>    4.940</td> <td> 0.000</td> <td>    0.151</td> <td>    0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                    <td>   -0.0096</td> <td>    0.060</td> <td>   -0.160</td> <td> 0.873</td> <td>   -0.127</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                      <td>   -0.1538</td> <td>    0.051</td> <td>   -2.998</td> <td> 0.003</td> <td>   -0.255</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>               <td>    0.3484</td> <td>    0.059</td> <td>    5.936</td> <td> 0.000</td> <td>    0.233</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>               <td>    0.1298</td> <td>    0.051</td> <td>    2.525</td> <td> 0.012</td> <td>    0.029</td> <td>    0.231</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>286.476</td> <th>  Durbin-Watson:     </th> <td>   1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5187.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.807</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>19.725</td>  <th>  Cond. No.          </th> <td>9.21e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 9.21e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}             &        HP        & \\textbf{  R-squared:         } &     0.392   \\\\\n",
       "\\textbf{Model:}                     &       OLS        & \\textbf{  Adj. R-squared:    } &     0.313   \\\\\n",
       "\\textbf{Method:}                    &  Least Squares   & \\textbf{  F-statistic:       } &     4.948   \\\\\n",
       "\\textbf{Date:}                      & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  9.48e-19   \\\\\n",
       "\\textbf{Time:}                      &     02:03:08     & \\textbf{  Log-Likelihood:    } &   -1765.0   \\\\\n",
       "\\textbf{No. Observations:}          &         400      & \\textbf{  AIC:               } &     3624.   \\\\\n",
       "\\textbf{Df Residuals:}              &         353      & \\textbf{  BIC:               } &     3812.   \\\\\n",
       "\\textbf{Df Model:}                  &          46      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}           &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                  &      10.1046  &       14.957     &     0.676  &         0.500        &      -19.312    &       39.521     \\\\\n",
       "\\textbf{Legendary[T.True]}          &      -3.2717  &        4.943     &    -0.662  &         0.508        &      -12.992    &        6.449     \\\\\n",
       "\\textbf{C(Generation)[T.2]}         &       9.2938  &        4.015     &     2.315  &         0.021        &        1.398    &       17.189     \\\\\n",
       "\\textbf{C(Generation)[T.3]}         &       2.3150  &        3.915     &     0.591  &         0.555        &       -5.385    &       10.015     \\\\\n",
       "\\textbf{C(Generation)[T.4]}         &       4.8353  &        4.149     &     1.165  &         0.245        &       -3.325    &       12.995     \\\\\n",
       "\\textbf{C(Generation)[T.5]}         &      11.4838  &        3.960     &     2.900  &         0.004        &        3.696    &       19.272     \\\\\n",
       "\\textbf{C(Generation)[T.6]}         &       4.9206  &        4.746     &     1.037  &         0.300        &       -4.413    &       14.254     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Dark]}     &      -1.4155  &        6.936     &    -0.204  &         0.838        &      -15.057    &       12.226     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Dragon]}   &       0.8509  &        6.900     &     0.123  &         0.902        &      -12.720    &       14.422     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Electric]} &      -6.3641  &        6.537     &    -0.974  &         0.331        &      -19.220    &        6.491     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fairy]}    &      -1.9486  &       10.124     &    -0.192  &         0.847        &      -21.859    &       17.962     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fighting]} &       7.0308  &        7.432     &     0.946  &         0.345        &       -7.586    &       21.648     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fire]}     &       3.0779  &        6.677     &     0.461  &         0.645        &      -10.055    &       16.210     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Flying]}   &      -2.1231  &       22.322     &    -0.095  &         0.924        &      -46.025    &       41.779     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ghost]}    &       5.7343  &        8.488     &     0.676  &         0.500        &      -10.960    &       22.429     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Grass]}    &       3.3275  &        5.496     &     0.605  &         0.545        &       -7.481    &       14.136     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ground]}   &       9.5118  &        7.076     &     1.344  &         0.180        &       -4.404    &       23.428     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ice]}      &      -0.9313  &        7.717     &    -0.121  &         0.904        &      -16.108    &       14.246     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Normal]}   &      18.4816  &        5.312     &     3.479  &         0.001        &        8.034    &       28.929     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Poison]}   &       8.3411  &        7.735     &     1.078  &         0.282        &       -6.871    &       23.554     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Psychic]}  &       1.8061  &        6.164     &     0.293  &         0.770        &      -10.317    &       13.930     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Rock]}     &      -3.8558  &        6.503     &    -0.593  &         0.554        &      -16.645    &        8.933     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Steel]}    &      -4.0053  &        8.044     &    -0.498  &         0.619        &      -19.826    &       11.816     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Water]}    &       9.7988  &        5.166     &     1.897  &         0.059        &       -0.361    &       19.959     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Dark]}     &       5.8719  &       15.185     &     0.387  &         0.699        &      -23.993    &       35.737     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Dragon]}   &      13.2777  &       14.895     &     0.891  &         0.373        &      -16.016    &       42.571     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Electric]} &      14.3228  &       17.314     &     0.827  &         0.409        &      -19.728    &       48.374     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fairy]}    &       2.8426  &       14.268     &     0.199  &         0.842        &      -25.218    &       30.903     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fighting]} &       1.9741  &       14.089     &     0.140  &         0.889        &      -25.735    &       29.683     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fire]}     &       0.2001  &       15.730     &     0.013  &         0.990        &      -30.736    &       31.136     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Flying]}   &       6.7292  &       13.581     &     0.495  &         0.621        &      -19.980    &       33.438     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ghost]}    &     -10.9402  &       15.895     &    -0.688  &         0.492        &      -42.201    &       20.321     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Grass]}    &       2.5119  &       14.540     &     0.173  &         0.863        &      -26.084    &       31.108     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ground]}   &      13.6042  &       13.655     &     0.996  &         0.320        &      -13.250    &       40.459     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ice]}      &      19.7950  &       15.068     &     1.314  &         0.190        &       -9.840    &       49.430     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.None]}     &       7.6068  &       13.162     &     0.578  &         0.564        &      -18.279    &       33.493     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Normal]}   &      17.3191  &       17.764     &     0.975  &         0.330        &      -17.618    &       52.256     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Poison]}   &       0.7770  &       14.575     &     0.053  &         0.958        &      -27.887    &       29.441     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Psychic]}  &       4.2480  &       14.174     &     0.300  &         0.765        &      -23.628    &       32.124     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Rock]}     &       6.8858  &       16.221     &     0.424  &         0.671        &      -25.017    &       38.788     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Steel]}    &     -11.9623  &       14.973     &    -0.799  &         0.425        &      -41.409    &       17.485     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Water]}    &       5.8097  &       14.763     &     0.394  &         0.694        &      -23.225    &       34.845     \\\\\n",
       "\\textbf{Attack}                     &       0.2508  &        0.051     &     4.940  &         0.000        &        0.151    &        0.351     \\\\\n",
       "\\textbf{Defense}                    &      -0.0096  &        0.060     &    -0.160  &         0.873        &       -0.127    &        0.108     \\\\\n",
       "\\textbf{Speed}                      &      -0.1538  &        0.051     &    -2.998  &         0.003        &       -0.255    &       -0.053     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}               &       0.3484  &        0.059     &     5.936  &         0.000        &        0.233    &        0.464     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}               &       0.1298  &        0.051     &     2.525  &         0.012        &        0.029    &        0.231     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 286.476 & \\textbf{  Durbin-Watson:     } &    1.917  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5187.327  \\\\\n",
       "\\textbf{Skew:}          &   2.807 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  19.725 & \\textbf{  Cond. No.          } & 9.21e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 9.21e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.392\n",
       "Model:                            OLS   Adj. R-squared:                  0.313\n",
       "Method:                 Least Squares   F-statistic:                     4.948\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           9.48e-19\n",
       "Time:                        02:03:08   Log-Likelihood:                -1765.0\n",
       "No. Observations:                 400   AIC:                             3624.\n",
       "Df Residuals:                     353   BIC:                             3812.\n",
       "Df Model:                          46                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "Intercept                     10.1046     14.957      0.676      0.500     -19.312      39.521\n",
       "Legendary[T.True]             -3.2717      4.943     -0.662      0.508     -12.992       6.449\n",
       "C(Generation)[T.2]             9.2938      4.015      2.315      0.021       1.398      17.189\n",
       "C(Generation)[T.3]             2.3150      3.915      0.591      0.555      -5.385      10.015\n",
       "C(Generation)[T.4]             4.8353      4.149      1.165      0.245      -3.325      12.995\n",
       "C(Generation)[T.5]            11.4838      3.960      2.900      0.004       3.696      19.272\n",
       "C(Generation)[T.6]             4.9206      4.746      1.037      0.300      -4.413      14.254\n",
       "C(Q(\"Type 1\"))[T.Dark]        -1.4155      6.936     -0.204      0.838     -15.057      12.226\n",
       "C(Q(\"Type 1\"))[T.Dragon]       0.8509      6.900      0.123      0.902     -12.720      14.422\n",
       "C(Q(\"Type 1\"))[T.Electric]    -6.3641      6.537     -0.974      0.331     -19.220       6.491\n",
       "C(Q(\"Type 1\"))[T.Fairy]       -1.9486     10.124     -0.192      0.847     -21.859      17.962\n",
       "C(Q(\"Type 1\"))[T.Fighting]     7.0308      7.432      0.946      0.345      -7.586      21.648\n",
       "C(Q(\"Type 1\"))[T.Fire]         3.0779      6.677      0.461      0.645     -10.055      16.210\n",
       "C(Q(\"Type 1\"))[T.Flying]      -2.1231     22.322     -0.095      0.924     -46.025      41.779\n",
       "C(Q(\"Type 1\"))[T.Ghost]        5.7343      8.488      0.676      0.500     -10.960      22.429\n",
       "C(Q(\"Type 1\"))[T.Grass]        3.3275      5.496      0.605      0.545      -7.481      14.136\n",
       "C(Q(\"Type 1\"))[T.Ground]       9.5118      7.076      1.344      0.180      -4.404      23.428\n",
       "C(Q(\"Type 1\"))[T.Ice]         -0.9313      7.717     -0.121      0.904     -16.108      14.246\n",
       "C(Q(\"Type 1\"))[T.Normal]      18.4816      5.312      3.479      0.001       8.034      28.929\n",
       "C(Q(\"Type 1\"))[T.Poison]       8.3411      7.735      1.078      0.282      -6.871      23.554\n",
       "C(Q(\"Type 1\"))[T.Psychic]      1.8061      6.164      0.293      0.770     -10.317      13.930\n",
       "C(Q(\"Type 1\"))[T.Rock]        -3.8558      6.503     -0.593      0.554     -16.645       8.933\n",
       "C(Q(\"Type 1\"))[T.Steel]       -4.0053      8.044     -0.498      0.619     -19.826      11.816\n",
       "C(Q(\"Type 1\"))[T.Water]        9.7988      5.166      1.897      0.059      -0.361      19.959\n",
       "C(Q(\"Type 2\"))[T.Dark]         5.8719     15.185      0.387      0.699     -23.993      35.737\n",
       "C(Q(\"Type 2\"))[T.Dragon]      13.2777     14.895      0.891      0.373     -16.016      42.571\n",
       "C(Q(\"Type 2\"))[T.Electric]    14.3228     17.314      0.827      0.409     -19.728      48.374\n",
       "C(Q(\"Type 2\"))[T.Fairy]        2.8426     14.268      0.199      0.842     -25.218      30.903\n",
       "C(Q(\"Type 2\"))[T.Fighting]     1.9741     14.089      0.140      0.889     -25.735      29.683\n",
       "C(Q(\"Type 2\"))[T.Fire]         0.2001     15.730      0.013      0.990     -30.736      31.136\n",
       "C(Q(\"Type 2\"))[T.Flying]       6.7292     13.581      0.495      0.621     -19.980      33.438\n",
       "C(Q(\"Type 2\"))[T.Ghost]      -10.9402     15.895     -0.688      0.492     -42.201      20.321\n",
       "C(Q(\"Type 2\"))[T.Grass]        2.5119     14.540      0.173      0.863     -26.084      31.108\n",
       "C(Q(\"Type 2\"))[T.Ground]      13.6042     13.655      0.996      0.320     -13.250      40.459\n",
       "C(Q(\"Type 2\"))[T.Ice]         19.7950     15.068      1.314      0.190      -9.840      49.430\n",
       "C(Q(\"Type 2\"))[T.None]         7.6068     13.162      0.578      0.564     -18.279      33.493\n",
       "C(Q(\"Type 2\"))[T.Normal]      17.3191     17.764      0.975      0.330     -17.618      52.256\n",
       "C(Q(\"Type 2\"))[T.Poison]       0.7770     14.575      0.053      0.958     -27.887      29.441\n",
       "C(Q(\"Type 2\"))[T.Psychic]      4.2480     14.174      0.300      0.765     -23.628      32.124\n",
       "C(Q(\"Type 2\"))[T.Rock]         6.8858     16.221      0.424      0.671     -25.017      38.788\n",
       "C(Q(\"Type 2\"))[T.Steel]      -11.9623     14.973     -0.799      0.425     -41.409      17.485\n",
       "C(Q(\"Type 2\"))[T.Water]        5.8097     14.763      0.394      0.694     -23.225      34.845\n",
       "Attack                         0.2508      0.051      4.940      0.000       0.151       0.351\n",
       "Defense                       -0.0096      0.060     -0.160      0.873      -0.127       0.108\n",
       "Speed                         -0.1538      0.051     -2.998      0.003      -0.255      -0.053\n",
       "Q(\"Sp. Def\")                   0.3484      0.059      5.936      0.000       0.233       0.464\n",
       "Q(\"Sp. Atk\")                   0.1298      0.051      2.525      0.012       0.029       0.231\n",
       "==============================================================================\n",
       "Omnibus:                      286.476   Durbin-Watson:                   1.917\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5187.327\n",
       "Skew:                           2.807   Prob(JB):                         0.00\n",
       "Kurtosis:                      19.725   Cond. No.                     9.21e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 9.21e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf7150",
   "metadata": {},
   "source": [
    "In this part, we are building a model (model5) by adding new \"ingredients\" (predictor variables) like Attack, Defense, Speed, and a few others like Sp. Def, Sp. Atk, and types of Pokémon. Each of these ingredients helps the model make better guesses about a Pokémon’s HP (hit points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a0dc7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.30015614488652215\n"
     ]
    }
   ],
   "source": [
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb82c7",
   "metadata": {},
   "source": [
    "Once we’ve built the model, we test it on data it hasn’t seen before (the test data). We check how well the model works by looking at two things:\n",
    "\n",
    "'In sample' R-squared: How well the model does with the training data.\n",
    "'Out of sample' R-squared: How well the model works on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eea79d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   24.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>2.25e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:03:10</td>     <th>  Log-Likelihood:    </th> <td> -1783.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3585.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   391</td>      <th>  BIC:               </th> <td>   3621.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                   <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                          <td>   22.8587</td> <td>    3.876</td> <td>    5.897</td> <td> 0.000</td> <td>   15.238</td> <td>   30.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Normal\")[T.True]</th> <td>   17.5594</td> <td>    3.339</td> <td>    5.258</td> <td> 0.000</td> <td>   10.994</td> <td>   24.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Water\")[T.True]</th>  <td>    9.0301</td> <td>    3.172</td> <td>    2.847</td> <td> 0.005</td> <td>    2.794</td> <td>   15.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 2)[T.True]</th>         <td>    6.5293</td> <td>    2.949</td> <td>    2.214</td> <td> 0.027</td> <td>    0.732</td> <td>   12.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 5)[T.True]</th>         <td>    8.4406</td> <td>    2.711</td> <td>    3.114</td> <td> 0.002</td> <td>    3.112</td> <td>   13.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                             <td>    0.2454</td> <td>    0.037</td> <td>    6.639</td> <td> 0.000</td> <td>    0.173</td> <td>    0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                              <td>   -0.1370</td> <td>    0.045</td> <td>   -3.028</td> <td> 0.003</td> <td>   -0.226</td> <td>   -0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                       <td>    0.3002</td> <td>    0.045</td> <td>    6.662</td> <td> 0.000</td> <td>    0.212</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                       <td>    0.1192</td> <td>    0.042</td> <td>    2.828</td> <td> 0.005</td> <td>    0.036</td> <td>    0.202</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>271.290</td> <th>  Durbin-Watson:     </th> <td>   1.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4238.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.651</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>18.040</td>  <th>  Cond. No.          </th> <td>    618.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                     &        HP        & \\textbf{  R-squared:         } &     0.333   \\\\\n",
       "\\textbf{Model:}                             &       OLS        & \\textbf{  Adj. R-squared:    } &     0.319   \\\\\n",
       "\\textbf{Method:}                            &  Least Squares   & \\textbf{  F-statistic:       } &     24.36   \\\\\n",
       "\\textbf{Date:}                              & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  2.25e-30   \\\\\n",
       "\\textbf{Time:}                              &     02:03:10     & \\textbf{  Log-Likelihood:    } &   -1783.6   \\\\\n",
       "\\textbf{No. Observations:}                  &         400      & \\textbf{  AIC:               } &     3585.   \\\\\n",
       "\\textbf{Df Residuals:}                      &         391      & \\textbf{  BIC:               } &     3621.   \\\\\n",
       "\\textbf{Df Model:}                          &           8      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                   &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                            & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                          &      22.8587  &        3.876     &     5.897  &         0.000        &       15.238    &       30.479     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Normal\")[T.True]} &      17.5594  &        3.339     &     5.258  &         0.000        &       10.994    &       24.125     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Water\")[T.True]}  &       9.0301  &        3.172     &     2.847  &         0.005        &        2.794    &       15.266     \\\\\n",
       "\\textbf{I(Generation == 2)[T.True]}         &       6.5293  &        2.949     &     2.214  &         0.027        &        0.732    &       12.327     \\\\\n",
       "\\textbf{I(Generation == 5)[T.True]}         &       8.4406  &        2.711     &     3.114  &         0.002        &        3.112    &       13.770     \\\\\n",
       "\\textbf{Attack}                             &       0.2454  &        0.037     &     6.639  &         0.000        &        0.173    &        0.318     \\\\\n",
       "\\textbf{Speed}                              &      -0.1370  &        0.045     &    -3.028  &         0.003        &       -0.226    &       -0.048     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                       &       0.3002  &        0.045     &     6.662  &         0.000        &        0.212    &        0.389     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                       &       0.1192  &        0.042     &     2.828  &         0.005        &        0.036    &        0.202     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 271.290 & \\textbf{  Durbin-Watson:     } &    1.999  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 4238.692  \\\\\n",
       "\\textbf{Skew:}          &   2.651 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  18.040 & \\textbf{  Cond. No.          } &     618.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.333\n",
       "Model:                            OLS   Adj. R-squared:                  0.319\n",
       "Method:                 Least Squares   F-statistic:                     24.36\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           2.25e-30\n",
       "Time:                        02:03:10   Log-Likelihood:                -1783.6\n",
       "No. Observations:                 400   AIC:                             3585.\n",
       "Df Residuals:                     391   BIC:                             3621.\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================================\n",
       "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------\n",
       "Intercept                             22.8587      3.876      5.897      0.000      15.238      30.479\n",
       "I(Q(\"Type 1\") == \"Normal\")[T.True]    17.5594      3.339      5.258      0.000      10.994      24.125\n",
       "I(Q(\"Type 1\") == \"Water\")[T.True]      9.0301      3.172      2.847      0.005       2.794      15.266\n",
       "I(Generation == 2)[T.True]             6.5293      2.949      2.214      0.027       0.732      12.327\n",
       "I(Generation == 5)[T.True]             8.4406      2.711      3.114      0.002       3.112      13.770\n",
       "Attack                                 0.2454      0.037      6.639      0.000       0.173       0.318\n",
       "Speed                                 -0.1370      0.045     -3.028      0.003      -0.226      -0.048\n",
       "Q(\"Sp. Def\")                           0.3002      0.045      6.662      0.000       0.212       0.389\n",
       "Q(\"Sp. Atk\")                           0.1192      0.042      2.828      0.005       0.036       0.202\n",
       "==============================================================================\n",
       "Omnibus:                      271.290   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4238.692\n",
       "Skew:                           2.651   Prob(JB):                         0.00\n",
       "Kurtosis:                      18.040   Cond. No.                         618.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5134427",
   "metadata": {},
   "source": [
    "Here, we simplify the model by removing some variables from model5 but adding Type 1 and Generation as new pieces. We are using the same idea as before but adjusting the puzzle pieces to try and make the model work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d91abe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.29572460427079933\n"
     ]
    }
   ],
   "source": [
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f7f63",
   "metadata": {},
   "source": [
    "Again, we test model6 on the new data to see if it works well. We compare the predictions to the actual values (real Pokémon HP) and check how accurate the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "210a2353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   12.16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.20e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:03:13</td>     <th>  Log-Likelihood:    </th> <td> -1769.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3579.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   380</td>      <th>  BIC:               </th> <td>   3659.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td>   95.1698</td> <td>   34.781</td> <td>    2.736</td> <td> 0.007</td> <td>   26.783</td> <td>  163.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Normal\")[T.True]</th>     <td>   18.3653</td> <td>    3.373</td> <td>    5.445</td> <td> 0.000</td> <td>   11.733</td> <td>   24.997</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Water\")[T.True]</th>      <td>    9.2913</td> <td>    3.140</td> <td>    2.959</td> <td> 0.003</td> <td>    3.117</td> <td>   15.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 2)[T.True]</th>             <td>    7.0711</td> <td>    2.950</td> <td>    2.397</td> <td> 0.017</td> <td>    1.271</td> <td>   12.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 5)[T.True]</th>             <td>    7.8557</td> <td>    2.687</td> <td>    2.923</td> <td> 0.004</td> <td>    2.572</td> <td>   13.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                 <td>   -0.6975</td> <td>    0.458</td> <td>   -1.523</td> <td> 0.129</td> <td>   -1.598</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                  <td>   -1.8147</td> <td>    0.554</td> <td>   -3.274</td> <td> 0.001</td> <td>   -2.905</td> <td>   -0.725</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                           <td>    0.0189</td> <td>    0.007</td> <td>    2.882</td> <td> 0.004</td> <td>    0.006</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                           <td>   -0.5532</td> <td>    0.546</td> <td>   -1.013</td> <td> 0.312</td> <td>   -1.627</td> <td>    0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                    <td>    0.0090</td> <td>    0.007</td> <td>    1.311</td> <td> 0.191</td> <td>   -0.004</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                     <td>    0.0208</td> <td>    0.008</td> <td>    2.571</td> <td> 0.011</td> <td>    0.005</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>              <td>   -0.0002</td> <td> 9.06e-05</td> <td>   -2.277</td> <td> 0.023</td> <td>   -0.000</td> <td>-2.82e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                           <td>   -0.7277</td> <td>    0.506</td> <td>   -1.439</td> <td> 0.151</td> <td>   -1.722</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                    <td>    0.0136</td> <td>    0.005</td> <td>    2.682</td> <td> 0.008</td> <td>    0.004</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                     <td>    0.0146</td> <td>    0.007</td> <td>    2.139</td> <td> 0.033</td> <td>    0.001</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>              <td>   -0.0002</td> <td>  5.4e-05</td> <td>   -3.383</td> <td> 0.001</td> <td>   -0.000</td> <td>-7.65e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0103</td> <td>    0.007</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.003</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>   -0.0001</td> <td> 6.71e-05</td> <td>   -2.119</td> <td> 0.035</td> <td>   -0.000</td> <td>-1.03e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0002</td> <td> 8.82e-05</td> <td>   -2.075</td> <td> 0.039</td> <td>   -0.000</td> <td>-9.62e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>  2.03e-06</td> <td> 7.42e-07</td> <td>    2.734</td> <td> 0.007</td> <td>  5.7e-07</td> <td> 3.49e-06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.34e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                         &        HP        & \\textbf{  R-squared:         } &     0.378   \\\\\n",
       "\\textbf{Model:}                                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.347   \\\\\n",
       "\\textbf{Method:}                                &  Least Squares   & \\textbf{  F-statistic:       } &     12.16   \\\\\n",
       "\\textbf{Date:}                                  & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.20e-29   \\\\\n",
       "\\textbf{Time:}                                  &     02:03:13     & \\textbf{  Log-Likelihood:    } &   -1769.5   \\\\\n",
       "\\textbf{No. Observations:}                      &         400      & \\textbf{  AIC:               } &     3579.   \\\\\n",
       "\\textbf{Df Residuals:}                          &         380      & \\textbf{  BIC:               } &     3659.   \\\\\n",
       "\\textbf{Df Model:}                              &          19      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                              &      95.1698  &       34.781     &     2.736  &         0.007        &       26.783    &      163.556     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Normal\")[T.True]}     &      18.3653  &        3.373     &     5.445  &         0.000        &       11.733    &       24.997     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Water\")[T.True]}      &       9.2913  &        3.140     &     2.959  &         0.003        &        3.117    &       15.466     \\\\\n",
       "\\textbf{I(Generation == 2)[T.True]}             &       7.0711  &        2.950     &     2.397  &         0.017        &        1.271    &       12.871     \\\\\n",
       "\\textbf{I(Generation == 5)[T.True]}             &       7.8557  &        2.687     &     2.923  &         0.004        &        2.572    &       13.140     \\\\\n",
       "\\textbf{Attack}                                 &      -0.6975  &        0.458     &    -1.523  &         0.129        &       -1.598    &        0.203     \\\\\n",
       "\\textbf{Speed}                                  &      -1.8147  &        0.554     &    -3.274  &         0.001        &       -2.905    &       -0.725     \\\\\n",
       "\\textbf{Attack:Speed}                           &       0.0189  &        0.007     &     2.882  &         0.004        &        0.006    &        0.032     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                           &      -0.5532  &        0.546     &    -1.013  &         0.312        &       -1.627    &        0.521     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                    &       0.0090  &        0.007     &     1.311  &         0.191        &       -0.004    &        0.023     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                     &       0.0208  &        0.008     &     2.571  &         0.011        &        0.005    &        0.037     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}              &      -0.0002  &     9.06e-05     &    -2.277  &         0.023        &       -0.000    &    -2.82e-05     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                           &      -0.7277  &        0.506     &    -1.439  &         0.151        &       -1.722    &        0.267     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                    &       0.0136  &        0.005     &     2.682  &         0.008        &        0.004    &        0.024     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                     &       0.0146  &        0.007     &     2.139  &         0.033        &        0.001    &        0.028     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}              &      -0.0002  &      5.4e-05     &    -3.383  &         0.001        &       -0.000    &    -7.65e-05     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0103  &        0.007     &     1.516  &         0.130        &       -0.003    &        0.024     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &      -0.0001  &     6.71e-05     &    -2.119  &         0.035        &       -0.000    &    -1.03e-05     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0002  &     8.82e-05     &    -2.075  &         0.039        &       -0.000    &    -9.62e-06     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &     2.03e-06  &     7.42e-07     &     2.734  &         0.007        &      5.7e-07    &     3.49e-06     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.34e+09. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.378\n",
       "Model:                            OLS   Adj. R-squared:                  0.347\n",
       "Method:                 Least Squares   F-statistic:                     12.16\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           4.20e-29\n",
       "Time:                        02:03:13   Log-Likelihood:                -1769.5\n",
       "No. Observations:                 400   AIC:                             3579.\n",
       "Df Residuals:                     380   BIC:                             3659.\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                                 95.1698     34.781      2.736      0.007      26.783     163.556\n",
       "I(Q(\"Type 1\") == \"Normal\")[T.True]        18.3653      3.373      5.445      0.000      11.733      24.997\n",
       "I(Q(\"Type 1\") == \"Water\")[T.True]          9.2913      3.140      2.959      0.003       3.117      15.466\n",
       "I(Generation == 2)[T.True]                 7.0711      2.950      2.397      0.017       1.271      12.871\n",
       "I(Generation == 5)[T.True]                 7.8557      2.687      2.923      0.004       2.572      13.140\n",
       "Attack                                    -0.6975      0.458     -1.523      0.129      -1.598       0.203\n",
       "Speed                                     -1.8147      0.554     -3.274      0.001      -2.905      -0.725\n",
       "Attack:Speed                               0.0189      0.007      2.882      0.004       0.006       0.032\n",
       "Q(\"Sp. Def\")                              -0.5532      0.546     -1.013      0.312      -1.627       0.521\n",
       "Attack:Q(\"Sp. Def\")                        0.0090      0.007      1.311      0.191      -0.004       0.023\n",
       "Speed:Q(\"Sp. Def\")                         0.0208      0.008      2.571      0.011       0.005       0.037\n",
       "Attack:Speed:Q(\"Sp. Def\")                 -0.0002   9.06e-05     -2.277      0.023      -0.000   -2.82e-05\n",
       "Q(\"Sp. Atk\")                              -0.7277      0.506     -1.439      0.151      -1.722       0.267\n",
       "Attack:Q(\"Sp. Atk\")                        0.0136      0.005      2.682      0.008       0.004       0.024\n",
       "Speed:Q(\"Sp. Atk\")                         0.0146      0.007      2.139      0.033       0.001       0.028\n",
       "Attack:Speed:Q(\"Sp. Atk\")                 -0.0002    5.4e-05     -3.383      0.001      -0.000   -7.65e-05\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0103      0.007      1.516      0.130      -0.003       0.024\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          -0.0001   6.71e-05     -2.119      0.035      -0.000   -1.03e-05\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0002   8.82e-05     -2.075      0.039      -0.000   -9.62e-06\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")   2.03e-06   7.42e-07      2.734      0.007     5.7e-07    3.49e-06\n",
       "==============================================================================\n",
       "Omnibus:                      252.300   Durbin-Watson:                   1.953\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3474.611\n",
       "Skew:                           2.438   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.590   Cond. No.                     2.34e+09\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.34e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83bb808",
   "metadata": {},
   "source": [
    "Now we’re adding some interactions between the ingredients (using *), which means we’re considering how these factors work together, like Attack and Speed affecting HP in combination. This helps us capture more complex relationships between the Pokémon’s features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3b677e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.35055389205977444\n"
     ]
    }
   ],
   "source": [
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91649ed9",
   "metadata": {},
   "source": [
    "Just like before, we test model7 using the new data and compare the predicted values with the actual Pokémon HP to check how good our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "feeb1ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>    15.4</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } &     15.4  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9d8b4",
   "metadata": {},
   "source": [
    "Here we use a trick called centering and scaling to make sure all the ingredients (predictors) are on the same scale. This helps the model work better and avoid issues with large numbers, especially when some ingredients are much bigger than others. We also see that the Condition Number is much better now (15.4), which means the model is less confused by the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "821a61e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1f6ec",
   "metadata": {},
   "source": [
    "Lastly, we compare the Condition Number of the original model (model7_fit) without centering and scaling. It’s much worse (2,340,000,000) than the improved version, showing how important centering and scaling can be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e7821",
   "metadata": {},
   "source": [
    "### In Summary:\n",
    "- **Model 5** adds a lot of features, like Attack, Defense, Speed, and others.\n",
    "- **Model 6** simplifies **Model 5** and adds special types and generations as factors.\n",
    "- **Model 7** builds on **Model 6** by looking at interactions between the features to capture more complex relationships.\n",
    "- **Part 7** improves **Model 7** by centering and scaling the data, which helps the model avoid big number issues and improve its performance.\n",
    "\n",
    "Each new model tries to improve predictions by carefully adding or adjusting the factors that affect Pokémon HP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea701d",
   "metadata": {},
   "source": [
    "### Discussion of Model Development:\n",
    "\n",
    "- **Model 5** builds upon **Model 3** and **Model 4** by adding more features like Attack, Defense, Speed, Special Defense, and Special Attack, as well as categorical factors such as Type and Generation. This broadens the scope of the model, capturing more potential influences on Pokémon HP.\n",
    "  \n",
    "- **Model 6** simplifies **Model 5** by reducing the number of features and focuses more on significant categorical variables like specific types and generations. This helps to refine the model while maintaining important predictive features.\n",
    "\n",
    "- **Model 7** extends **Model 6** by considering interactions between features (such as Attack * Speed * Special Defense). This allows the model to account for complex relationships between different features, leading to a more nuanced prediction.\n",
    "- **Part 7** improves **Model 7** by centering and scaling the data, which helps the model avoid big number issues and improve its performance.\n",
    "\n",
    "In summary, each model builds upon the previous one by carefully adding, simplifying, or interacting features, with the goal of improving predictions and capturing more relevant patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbf01d",
   "metadata": {},
   "source": [
    "### 8. Work with a ChatBot to write a *for* loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" *model performance* metric actualizations (by not using *np.random.seed(130)* within each loop iteration); and explain in your own words the meaning of your results and purpose of this demonstration<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _The following code could likely be slightly edited and repurposed to match the 50-50 **train-test split** analysis and data above (in the `train_test_split` method of \"Question 5\").  Considering the behavior of the `model3_fit` **linear form** specification would be the suggested way to start, but it might also be interesting and/or helpful to consider the different available **linear form** specifications in the manner of this problem..._\n",
    ">    \n",
    "> ```python\n",
    "> import plotly.express as px  # etc.\n",
    ">\n",
    "> songs_training_data,songs_testing_data = train_test_split(songs, train_size=31)\n",
    "> linear_form = 'danceability ~ energy * loudness + energy * mode'\n",
    ">    \n",
    "> reps = 100\n",
    "> in_sample_Rsquared = np.array([0.0]*reps)\n",
    "> out_of_sample_Rsquared = np.array([0.0]*reps)\n",
    "> for i in range(reps):\n",
    ">     songs_training_data,songs_testing_data = \\\n",
    ">       train_test_split(songs, train_size=31)\n",
    ">     final_model_fit = smf.ols(formula=linear_form, \n",
    ">                               data=songs_training_data).fit()\n",
    ">     in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    ">     out_of_sample_Rsquared[i] = \\\n",
    ">       np.corrcoef(songs_testing_data.danceability, \n",
    ">                   final_model_fit.predict(songs_testing_data))[0,1]**2\n",
    ">     \n",
    "> df = pd.DataFrame({\"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    ">                    \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared})   >  \n",
    "> fig = px.scatter(df, x=\"In Sample Performance (Rsquared)\", \n",
    ">                      y=\"Out of Sample Performance (Rsquared)\")\n",
    "> fig.add_trace(go.Scatter(x=[0,1], y=[0,1], name=\"y=x\", line_shape='linear'))  \n",
    "> ```\n",
    ">\n",
    "> _When you first look at this question, you might be unsure about the specific issue that the code is addressing. Take a moment to think about why the code repeatedly randomly re-splits the data, fits the model, and compares the \"in sample\" versus \"out of sample\" **R-squared** values (over and over). Of course, if a **fit model** performs well on the **training** dataset but doesn't do as well on the **testing** dataset then we might be observing the affects of **overfitting**. But why might it sometimes be the opposite situation (which we actually encountered right away for `model3_fit` when the **train-test split** was based on  `np.random.seed(130)` and resulted in a better \"out of sample\" **R-squared** of about `0.21` vereses the 'in-sample\" **R-squared** of about `0.15`)? If you're thinking that this should therefore vice-versa intuitively mean **underfitting**, actually that's not right because **underfitting** is when the **generalizability** of a different model **linear form** specification that provides improved **model performance** is **validated**. What were seeing here, the variable, is something else..._\n",
    ">        \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166dd734",
   "metadata": {},
   "source": [
    "### Explanation and Purpose of the Demonstration:\n",
    "\n",
    "This exercise is about assessing how well a model performs on both the data it was trained on (\"in sample\") and new, unseen data (\"out of sample\"). The goal of repeatedly splitting the data into training and testing sets is to see how consistent the model's performance is across different random splits. \n",
    "\n",
    "By fitting the model to the training data and comparing the performance on the testing data, we can investigate two key things:\n",
    "1. **Overfitting**: When the model performs really well on the training data but poorly on the test data, it has learned the noise or specificities of the training set, instead of generalizable patterns.\n",
    "2. **Underfitting**: When the model is too simplistic and doesn't perform well on either the training or test data, indicating that it hasn't captured important relationships in the data.\n",
    "\n",
    "However, what we’re also seeing here in some cases is that **out-of-sample performance** (the testing set) might be better than **in-sample performance** (the training set), which isn't a typical sign of overfitting or underfitting. This can happen for a few reasons:\n",
    "- The training data might have a lot of noise or irrelevant features, while the test set might have more meaningful patterns that the model can easily capture.\n",
    "- Randomness in the data splits could cause an unusually good fit on the test set due to favorable conditions in the specific test data.\n",
    "\n",
    "The purpose of this demonstration is to visualize these fluctuations in model performance, helping us understand the generalizability of our model across different data splits. By plotting the \"in sample\" versus \"out of sample\" R-squared values across many iterations, we can get a sense of how well the model might generalize to new data in a real-world scenario.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    "The code performs the following:\n",
    "1. **Data Splitting**: The `train_test_split` method divides the data into training and testing sets randomly. We repeat this process 100 times to observe variations in the model's performance.\n",
    "2. **Model Fitting**: For each split, a model is fit on the training data using Ordinary Least Squares (OLS) regression with the formula `'danceability ~ energy * loudness + energy * mode'`.\n",
    "3. **Performance Metrics**:\n",
    "   - **In-Sample R-Squared**: The model's R-squared value on the training data.\n",
    "   - **Out-of-Sample R-Squared**: The model's R-squared value on the testing data.\n",
    "4. **Collecting Results**: These R-squared values are stored for each iteration, allowing for further analysis of model performance across different data splits.\n",
    "5. **Visualization**: The results are visualized in a scatter plot, with each point representing a single iteration. A line `y = x` is added to the plot to show where perfect agreement between in-sample and out-of-sample R-squared values would lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44c6e2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "In Sample Performance (Rsquared)=%{x}<br>Out of Sample Performance (Rsquared)=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.8085314462085617,
          0.781584974291186,
          0.7883637040621617,
          0.7809248449736704,
          0.7825100705679051,
          0.8077482747133605,
          0.7965211812704013,
          0.7976595585585832,
          0.8123995198921917,
          0.7892006493699533,
          0.7923391342936756,
          0.7753127935911022,
          0.8000471843925874,
          0.7902932210794017,
          0.7966271017110458,
          0.7991778131529392,
          0.7962113869559962,
          0.8114828561734669,
          0.7961644812967106,
          0.8080540203084551,
          0.7921261592464691,
          0.7936273417400356,
          0.7983828912837815,
          0.8138697409634765,
          0.7995993456845897,
          0.8065001981113645,
          0.8108096738877015,
          0.7931046685229133,
          0.7964461018970793,
          0.784157987585743,
          0.808572516333975,
          0.8036206556628398,
          0.7862801144241875,
          0.79665565890777,
          0.791168471535491,
          0.7940777597626578,
          0.7830773400789437,
          0.8162625672786599,
          0.7941919302414203,
          0.8101216345692195,
          0.798160203395545,
          0.7905626685346144,
          0.7868929590947439,
          0.8001004111853389,
          0.7858239058642688,
          0.7956480314101526,
          0.8004002314037877,
          0.7862383484759511,
          0.7959343059835223,
          0.7954600697122391,
          0.7970830101331061,
          0.7856057421511216,
          0.7871787108990059,
          0.8040103896778534,
          0.7640544756102097,
          0.7980959998118482,
          0.7932434682538767,
          0.8021287525967082,
          0.8092680908568092,
          0.77734994557211,
          0.7963829691444231,
          0.7922186759432217,
          0.7877241172442018,
          0.8076091154630607,
          0.7944481824679133,
          0.7853666558092465,
          0.7973060861906849,
          0.7963605874741867,
          0.7904226036602198,
          0.7745986233720928,
          0.7974651186430732,
          0.8031964181652368,
          0.7887217975900062,
          0.7964892192507946,
          0.784384734009917,
          0.7984713472129109,
          0.8044449990181892,
          0.8032236542224305,
          0.7989025603219084,
          0.8039978401028636,
          0.8121369338937625,
          0.8028285724441302,
          0.7966252455161605,
          0.7814609215102696,
          0.7957155633830333,
          0.7932435337153028,
          0.7817230578694373,
          0.7869638148997447,
          0.7998408423256603,
          0.795475925632093,
          0.7938812033810698,
          0.7975500276410049,
          0.7960472678477477,
          0.7982832709991478,
          0.7931062342165746,
          0.7924028843698272,
          0.8076126396866264,
          0.7858007701208841,
          0.8023519838444367,
          0.7900238745044453
         ],
         "xaxis": "x",
         "y": [
          null,
          null,
          0.8234980173570926,
          0.8324126434093935,
          0.8380063684504129,
          0.7437452320010607,
          null,
          0.7876166147881615,
          0.7313025518220401,
          0.8269805460382976,
          0.8033615581828158,
          0.8589027965832952,
          0.7771667042548615,
          0.801383557497495,
          0.7863875675613597,
          0.7759605447103554,
          0.7842536979723158,
          null,
          null,
          0.74917235735956,
          0.80664850245239,
          0.8054005749137974,
          0.7851727907172081,
          0.6916755920395612,
          null,
          0.7405407398245372,
          0.6907023692054841,
          null,
          0.7856053557496471,
          0.8358855745164933,
          null,
          0.7422005426786934,
          null,
          null,
          0.8064254669563038,
          null,
          0.8417484850828056,
          0.7018199533700038,
          0.7853635251611971,
          null,
          0.7772522696076598,
          null,
          0.8306206585450621,
          0.7726667249663625,
          null,
          0.7937700090704768,
          null,
          0.8241466704536068,
          null,
          0.7789121470023561,
          0.7855077752828378,
          0.8323864132841047,
          0.8219572917275014,
          null,
          0.8700905505252701,
          null,
          0.8049702334286255,
          null,
          0.7409575497985783,
          0.862993761139233,
          0.7963386388564978,
          null,
          0.81886100770975,
          0.7429292774796462,
          0.7995219180611796,
          0.828419705011031,
          null,
          null,
          0.8122692631025138,
          null,
          null,
          0.7616757064178716,
          0.8146387632482791,
          null,
          0.8331807634314752,
          0.7761833904339867,
          0.7352191331687888,
          0.7581593651949948,
          0.7697745196234482,
          null,
          null,
          null,
          0.7899143448558202,
          0.8446755922313465,
          null,
          0.7984654824106653,
          0.8330343584298987,
          0.836904644048363,
          0.7751071108077432,
          null,
          null,
          0.7667734512673637,
          null,
          0.7748309421082327,
          null,
          0.8036168986305531,
          0.7354798745887974,
          0.8279334409883281,
          0.7644703147787658,
          null
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "shape": "linear"
         },
         "name": "y=x",
         "type": "scatter",
         "x": [
          0,
          1
         ],
         "y": [
          0,
          1
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "In Sample Performance (Rsquared)"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Out of Sample Performance (Rsquared)"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ba7a0409-ca9d-41b8-9e58-a7ec2069246e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ba7a0409-ca9d-41b8-9e58-a7ec2069246e\")) {                    Plotly.newPlot(                        \"ba7a0409-ca9d-41b8-9e58-a7ec2069246e\",                        [{\"hovertemplate\":\"In Sample Performance (Rsquared)=%{x}\\u003cbr\\u003eOut of Sample Performance (Rsquared)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.8085314462085617,0.781584974291186,0.7883637040621617,0.7809248449736704,0.7825100705679051,0.8077482747133605,0.7965211812704013,0.7976595585585832,0.8123995198921917,0.7892006493699533,0.7923391342936756,0.7753127935911022,0.8000471843925874,0.7902932210794017,0.7966271017110458,0.7991778131529392,0.7962113869559962,0.8114828561734669,0.7961644812967106,0.8080540203084551,0.7921261592464691,0.7936273417400356,0.7983828912837815,0.8138697409634765,0.7995993456845897,0.8065001981113645,0.8108096738877015,0.7931046685229133,0.7964461018970793,0.784157987585743,0.808572516333975,0.8036206556628398,0.7862801144241875,0.79665565890777,0.791168471535491,0.7940777597626578,0.7830773400789437,0.8162625672786599,0.7941919302414203,0.8101216345692195,0.798160203395545,0.7905626685346144,0.7868929590947439,0.8001004111853389,0.7858239058642688,0.7956480314101526,0.8004002314037877,0.7862383484759511,0.7959343059835223,0.7954600697122391,0.7970830101331061,0.7856057421511216,0.7871787108990059,0.8040103896778534,0.7640544756102097,0.7980959998118482,0.7932434682538767,0.8021287525967082,0.8092680908568092,0.77734994557211,0.7963829691444231,0.7922186759432217,0.7877241172442018,0.8076091154630607,0.7944481824679133,0.7853666558092465,0.7973060861906849,0.7963605874741867,0.7904226036602198,0.7745986233720928,0.7974651186430732,0.8031964181652368,0.7887217975900062,0.7964892192507946,0.784384734009917,0.7984713472129109,0.8044449990181892,0.8032236542224305,0.7989025603219084,0.8039978401028636,0.8121369338937625,0.8028285724441302,0.7966252455161605,0.7814609215102696,0.7957155633830333,0.7932435337153028,0.7817230578694373,0.7869638148997447,0.7998408423256603,0.795475925632093,0.7938812033810698,0.7975500276410049,0.7960472678477477,0.7982832709991478,0.7931062342165746,0.7924028843698272,0.8076126396866264,0.7858007701208841,0.8023519838444367,0.7900238745044453],\"xaxis\":\"x\",\"y\":[null,null,0.8234980173570926,0.8324126434093935,0.8380063684504129,0.7437452320010607,null,0.7876166147881615,0.7313025518220401,0.8269805460382976,0.8033615581828158,0.8589027965832952,0.7771667042548615,0.801383557497495,0.7863875675613597,0.7759605447103554,0.7842536979723158,null,null,0.74917235735956,0.80664850245239,0.8054005749137974,0.7851727907172081,0.6916755920395612,null,0.7405407398245372,0.6907023692054841,null,0.7856053557496471,0.8358855745164933,null,0.7422005426786934,null,null,0.8064254669563038,null,0.8417484850828056,0.7018199533700038,0.7853635251611971,null,0.7772522696076598,null,0.8306206585450621,0.7726667249663625,null,0.7937700090704768,null,0.8241466704536068,null,0.7789121470023561,0.7855077752828378,0.8323864132841047,0.8219572917275014,null,0.8700905505252701,null,0.8049702334286255,null,0.7409575497985783,0.862993761139233,0.7963386388564978,null,0.81886100770975,0.7429292774796462,0.7995219180611796,0.828419705011031,null,null,0.8122692631025138,null,null,0.7616757064178716,0.8146387632482791,null,0.8331807634314752,0.7761833904339867,0.7352191331687888,0.7581593651949948,0.7697745196234482,null,null,null,0.7899143448558202,0.8446755922313465,null,0.7984654824106653,0.8330343584298987,0.836904644048363,0.7751071108077432,null,null,0.7667734512673637,null,0.7748309421082327,null,0.8036168986305531,0.7354798745887974,0.8279334409883281,0.7644703147787658,null],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"line\":{\"shape\":\"linear\"},\"name\":\"y=x\",\"x\":[0,1],\"y\":[0,1],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"In Sample Performance (Rsquared)\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Out of Sample Performance (Rsquared)\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ba7a0409-ca9d-41b8-9e58-a7ec2069246e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px  # Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the dataset (you can use the penguins dataset you already have)\n",
    "pingees = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\")\n",
    "\n",
    "# Data splitting and model specification\n",
    "linear_form = 'body_mass_g ~ bill_length_mm * bill_depth_mm + flipper_length_mm'  # Example linear formula\n",
    "\n",
    "reps = 100  # Number of repetitions\n",
    "in_sample_Rsquared = np.array([0.0]*reps)  # To store in-sample R-squared values\n",
    "out_of_sample_Rsquared = np.array([0.0]*reps)  # To store out-of-sample R-squared values\n",
    "\n",
    "for i in range(reps):\n",
    "    # Randomly split the data into training and testing sets\n",
    "    songs_training_data, songs_testing_data = train_test_split(pingees, train_size=0.8)  # 80% training data\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    final_model_fit = smf.ols(formula=linear_form, data=songs_training_data).fit()\n",
    "    \n",
    "    # Collect in-sample R-squared\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    \n",
    "    # Calculate and collect out-of-sample R-squared\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(songs_testing_data.body_mass_g, \n",
    "                                             final_model_fit.predict(songs_testing_data))[0, 1]**2\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "df = pd.DataFrame({\"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    "                   \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared})\n",
    "\n",
    "# Create a scatter plot of the results\n",
    "fig = px.scatter(df, x=\"In Sample Performance (Rsquared)\", \n",
    "                 y=\"Out of Sample Performance (Rsquared)\")\n",
    "\n",
    "# Add a line for perfect correlation (y=x)\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name=\"y=x\", line_shape='linear'))\n",
    "\n",
    "fig.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0145e1",
   "metadata": {},
   "source": [
    "### 9. Work with a ChatBot to understand the meaning of the illustration below; and, explain this in your own words<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _While we had seemed to **validate** the **generalizability** of `model7_fit` in **model building** exercise of the previous \"Question 7\" above, as well as the improved **model performance** of `model7_fit` comapred to `model6_fit`, the `model7_fit` model was always nonetheless more complex than `model6_fit` model (as seen by comparing their `.summary()` methods). This complexity, despite the minimal concerns regarding **multicollinearity**, should always have suggested some room for caution. This is because, as previously discussed in \"Question 6\" above, a complex **linear form** specification can allow a \"**model fit** to 'detect' idiosyncratic associations spuriously present specifically in the **training** dataset but which did not **generalize** to the **testing** dataset.\" Indeed, a close look at the **p-values** in `model7_fit.summary()` will show that the **evidence** (in the data) for many of the **estimated coefficients** of `model7_fit` is in fact not very strong. In comparision, the **evidence** (in the data) for many of the **estimated coefficients** of `model6_fit.summary()` is consistently stronger._\n",
    ">\n",
    "> _As discussed towards the end of the commentary in the previous \"Question 7\" above, the primary purpose of **multiple linear regression** methodology is to allow us to assess the **evidence** (in the data) for a given **linear form** specification based on **coefficient hypothesis testing**. In this regard, then, `model6_fit` might be preferred over `model7_fit` despite the better \"out of sample\" **model performance** of `model7_fit` over `model6_fit`. This may not be enough to convince everyone however, so an additional consideration that might be made here is that the more simpler (more parsimoneous) nature of `model6_fit` should be preferred over `model7_fit` from the perspective of **model interpretability**. Indeed, it is quite unclear how exactly one should think about and understand a four-way **interaction** variable such as `Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")` in conjunction with the whole host of the additional lower order interations. From a **model interpretability** perspective, understanding the meaning of the complex specification of `model7_fit` is \"challenging\" and \"complicated\" to say the least._\n",
    ">\n",
    "> - _There are also often circumstances where **model interpretability** can be MORE IMPORTANT than raw **model performance** in \"out of sample\" **prediction**._\n",
    "> - _This is ESPECIALLY true if **predictive model performance** is relatively comparable between models of two different complexity levels. In such cases, the benefits of better **model interpretability** might provide a clear argument for the simpler (more parsimoneous) model, not to mention the additional potential benefit of more consistent improved **generalizability** over the the more complex model this might offer._\n",
    ">\n",
    "> _This question drives home the point that a simpler (more parsimoneous) model always offers the potential benefit of more consistent **generalizability**, not to mention **interpretability**, over more complex models. We should *ONLY* use increasingly complex models that without questions outperfrm simler models. The code below illustrates this by further additionally raising the consideration that the random **train-test** approach used above is actually not the most natural one available for our dataset, which has different \"Generations\". In fact, if we were actually using this model to make **predictions**, we would increasingly acquire more data over time which we would use to make **precictions** about future data which we haven't yet seen, which is what the code demonstrates. And low and behold, this exposes **generalizability** concerns that we missed when we used the dataset in an idealized way and not actually how we would use such a dataset in practice in the real world (where data would arrive sequentially, and current data is used to predict future data). These **generalizability** concerns do affect both models, but the appear to be more problematic for `model7_fit` than `model6_fit`, which is certainly a result of the increased complexity of `model7_fit` which always opens up the possibility of model **overfitting**._\n",
    "\n",
    "<details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41d57caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1dd39d",
   "metadata": {},
   "source": [
    "### Simplified Explanation:\n",
    "\n",
    "The robot is trying to guess Pokémon's HP.\n",
    "- The **in-sample R-squared** tells us how well it does when guessing Pokémon it has already seen.\n",
    "- The **out-of-sample R-squared** tells us how well it guesses Pokémon it hasn’t seen before, which is more important.\n",
    "- The fancy robot (**model7**) might be too complicated, so it guesses very well for the Pokémon it has already seen, but not as well for Pokémon it hasn’t seen yet (**future Pokémon**).\n",
    "- We are comparing how the fancy model (**model7**) performs on Pokémon from **Generation 1** and how it would perform on future Pokémon from **other Generations**.\n",
    "- This helps us understand if the robot (the model) is **overfitting** to the data it has and may not be very good at predicting new Pokémon's HP in the future, so we need to be careful about how complex our models are.\n",
    "\n",
    "### Explanation of the R-squared Values:\n",
    "\n",
    "- **'In sample' R-squared: 0.378** (original):\n",
    "  - This value tells us how well the model is performing on the data it has already seen (in the training set). A value of 0.378 is not very high, meaning the model’s predictions are not super accurate on the training data.\n",
    "\n",
    "- **'Out of sample' R-squared: 0.351** (original):\n",
    "  - This tells us how well the model performs on new data (that it hasn't seen before). The fact that it's slightly lower than the in-sample R-squared (0.378) suggests the model is struggling a bit more to predict new data.\n",
    "\n",
    "- **'In sample' R-squared: 0.573** (gen1_predict_future):\n",
    "  - For the new model built using just **Generation 1** data, the in-sample R-squared jumps to 0.573, showing that this model performs better when predicting **Generation 1** Pokémon’s HP than the original model did on the full dataset.\n",
    "\n",
    "- **'Out of sample' R-squared: 0.112** (gen1_predict_future):\n",
    "  - However, when tested on **other Generations** (out-of-sample), this model’s performance drops dramatically to 0.112. This shows that while the model works okay on the data it has seen (Generation 1), it is not doing well at all when predicting new Pokémon data from future generations. This indicates that the model might be **overfitting** to the Generation 1 data and may not generalize well to new data.\n",
    "\n",
    "### Key Takeaways:\n",
    "- The original model's in-sample performance is decent but not perfect, and it struggles a bit more with new data.\n",
    "- The model built on **Generation 1** data does better with Generation 1 but fails when applied to future generations, highlighting the risks of overfitting to specific data and not generalizing well to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "628daf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9576cc93",
   "metadata": {},
   "source": [
    "### Summary of Findings:\n",
    "\n",
    "The code examines the performance of **model7** on the Pokémon data, focusing on two aspects: its **in-sample** and **out-of-sample** R-squared values, with a particular emphasis on how the model generalizes across different Pokémon generations.\n",
    "\n",
    "1. **Original Model (model7_fit):**\n",
    "   - **In-sample R-squared:** 0.378\n",
    "     - This value suggests the model performs moderately well on the data it has already seen (training data).\n",
    "   - **Out-of-sample R-squared:** 0.351\n",
    "     - This value indicates how well the model performs on new, unseen data. It's slightly lower than the in-sample R-squared, showing that the model struggles a bit when predicting data it hasn't encountered before.\n",
    "\n",
    "2. **Model with Generations 1-5 (model7_gen1to5_predict_future):**\n",
    "   - **In-sample R-squared:** 0.390\n",
    "     - For this model, built using data from **Generations 1 to 5**, the in-sample R-squared is slightly better than the original model (0.390 vs. 0.378), indicating a small improvement in predicting data the model has already seen.\n",
    "   - **Out-of-sample R-squared:** 0.234\n",
    "     - However, when tested on **Generation 6** Pokémon (new data that the model has not seen before), the out-of-sample R-squared drops significantly (from 0.351 to 0.234), which shows that the model doesn't generalize well to unseen generations, even though it performed better on the training data.\n",
    "\n",
    "### Key Takeaways:\n",
    "- The **model7** performs slightly better when trained with data from **Generations 1-5** compared to the original model.\n",
    "- However, the **out-of-sample** performance on **Generation 6** shows a noticeable drop, which suggests that the model may not generalize well to new, unseen data (future generations).\n",
    "- This indicates that the model could be **overfitting** to the data it has seen and may not be the best at predicting future Pokémon's HP, highlighting the importance of balancing model complexity and generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c52c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14056bf9",
   "metadata": {},
   "source": [
    "### Summary of Findings:\n",
    "\n",
    "The code evaluates the performance of **model6** on the Pokémon data, focusing on both **in-sample** and **out-of-sample** R-squared values, specifically for **Generation 1** Pokémon and future generations.\n",
    "\n",
    "1. **Original Model (model6_fit):**\n",
    "   - **In-sample R-squared:** 0.333\n",
    "     - This value indicates how well the model fits the training data (Pokémon from all generations except Generation 1). It suggests that the model has a moderate fit to the original data.\n",
    "   - **Out-of-sample R-squared:** 0.296\n",
    "     - The out-of-sample R-squared, which measures the model's ability to predict unseen data, is slightly lower than the in-sample R-squared. This indicates that the model is somewhat less accurate when predicting Pokémon that it hasn't seen during training.\n",
    "\n",
    "2. **Model Trained on Generation 1 (model6_gen1_predict_future):**\n",
    "   - **In-sample R-squared:** 0.443\n",
    "     - When the model is trained only on **Generation 1** Pokémon, its in-sample R-squared increases to 0.443. This shows that the model fits the training data (only Generation 1) better compared to the original model that includes all generations.\n",
    "   - **Out-of-sample R-squared:** 0.193\n",
    "     - However, when predicting Pokémon from generations other than Generation 1 (future Pokémon), the out-of-sample R-squared drops significantly to 0.193. This suggests that the model is poorly generalizing to Pokémon from different generations, meaning that it doesn't perform well on new, unseen data.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Model6** performs better on **Generation 1** Pokémon than on the full dataset, as seen from the higher in-sample R-squared (0.443 vs. 0.333).\n",
    "- However, when the model is tested on **future generations** (those not included in the training data), the out-of-sample R-squared significantly drops (0.193 vs. 0.296).\n",
    "- The large drop in out-of-sample performance indicates **overfitting** to Generation 1 data, meaning the model may not generalize well to Pokémon from other generations. This highlights the importance of ensuring the model is capable of generalizing to unseen data rather than just performing well on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ea14cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01ca36",
   "metadata": {},
   "source": [
    "## Summary of Findings:\n",
    "\n",
    "The code evaluates the performance of **model6** using Pokémon data from **Generations 1 to 5** (excluding Generation 6), comparing both **in-sample** and **out-of-sample** R-squared values for the original model and the model trained on Generations 1-5.\n",
    "\n",
    "### 1. **Original Model (model6_fit):**\n",
    "   - **In-sample R-squared:** 0.333\n",
    "     - This value suggests that the model is a moderate fit to the data when using all generations except Generation 6, meaning the model explains about 33% of the variability in the training data.\n",
    "   - **Out-of-sample R-squared:** 0.296\n",
    "     - When applied to new, unseen data (test data), the model's performance slightly drops, which is expected, but it remains relatively close to the in-sample R-squared, indicating that it doesn't perform poorly on new data.\n",
    "\n",
    "### 2. **Model Trained on Generations 1-5 (model6_gen1to5_predict_future):**\n",
    "   - **In-sample R-squared:** 0.335\n",
    "     - When the model is trained on just **Generations 1-5**, the in-sample R-squared value improves slightly to 0.335, suggesting that excluding Generation 6 doesn’t significantly affect how well the model fits the training data.\n",
    "   - **Out-of-sample R-squared:** 0.263\n",
    "     - However, when tested on Generation 6 data, the model’s performance decreases, with an out-of-sample R-squared of 0.263, which is still relatively close to the original model’s out-of-sample performance. This indicates the model has some difficulty predicting new Pokémon from a new generation, but not drastically so.\n",
    "\n",
    "### Key Insights:\n",
    "- The **in-sample R-squared** values are very close between the original model (0.333) and the model trained on Generations 1-5 (0.335). This shows that removing Generation 6 data doesn't significantly affect the model’s ability to fit the training data.\n",
    "- The **out-of-sample R-squared** for the model trained on Generations 1-5 is still close to that of the original model, with a small decline from 0.296 to 0.263. This suggests that the model can still make reasonable predictions on unseen data, even from a new generation, though not perfectly.\n",
    "- While there's a slight drop in performance when the model is tested on new data, it’s not a large drop, indicating that overfitting is likely not a significant issue. The model is fairly robust and has a decent ability to generalize to new data, with only minor adjustments needed.\n",
    "\n",
    "### Conclusion:\n",
    "The model demonstrates good generalization capabilities, even when tested on new Pokémon generations. The difference in performance between the original model and the model trained on Generations 1-5 is minimal, which suggests that the model performs well overall without significant overfitting. This makes it a **reliable model for prediction**, as it maintains fairly consistent accuracy for both known and unseen Pokémon data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbded53",
   "metadata": {},
   "source": [
    "## Summary of Chatbot Session\n",
    "\n",
    "### Key Tasks:\n",
    "1. **Analysis of Model Performance:**\n",
    "   - We discussed the results of several models, specifically focusing on R-squared values for both in-sample and out-of-sample data. These evaluations were done using Pokémon data from various generations.\n",
    "   - The performance of **model6** was assessed, both for data from **Generations 1-5** and for the **original model** that included all generations except Generation 6.\n",
    "   - The R-squared values indicated moderate fitting for both the in-sample and out-of-sample data, with a slight drop in predictive accuracy when tested on unseen data (Generation 6).\n",
    "\n",
    "2. **Key Findings:**\n",
    "   - The model shows a solid fit to the training data, with **in-sample R-squared values** around 0.33 for the original model and 0.34 for the model trained on Generations 1-5.\n",
    "   - **Out-of-sample R-squared values** slightly dropped when tested on Generation 6 data, but the decrease was relatively small, suggesting the model does not overfit significantly.\n",
    "   - In general, the model showed good predictive capabilities without large differences between the training and test data, indicating the model generalizes well to new data.\n",
    "\n",
    "3. **Suggestions for Improvement:**\n",
    "   - Although the model performed decently across both training and test sets, it was noted that **Generation 6** data presented a slight challenge for predictions. This suggests that further tuning may be necessary to enhance the model’s generalization to truly unseen Pokémon.\n",
    "\n",
    "4. **Model Performance Recap:**\n",
    "   - The analysis demonstrated that the **model6** was effective at predicting data from earlier generations but struggled somewhat with future, unseen data from Generation 6.\n",
    "   - However, **overfitting** did not appear to be a major concern, as the **out-of-sample R-squared** values remained relatively close to the **in-sample** R-squared values.\n",
    "   - The model is **useful for general predictions**, with only minor adjustments needed for more accurate forecasting for new data.\n",
    "\n",
    "5. **Final Thoughts:**\n",
    "   - Overall, the model’s performance indicates a **reliable predictive model** that can be used with confidence for Pokémon data, particularly for Generations 1-5. For future use, some slight improvements to handle new generations may be beneficial but not critical.\n",
    "\n",
    "This summary captures the essence of the chatbot session, with a focus on understanding model performance and ensuring accurate prediction for Pokémon data.\n",
    "\n",
    "Chatgpt Link: https://chatgpt.com/share/6736b652-1724-8008-afa7-6ebae14baa3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992924af",
   "metadata": {},
   "source": [
    "## Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course\n",
    "\n",
    "<br>\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "### Ethical Professionalism Considerations\n",
    "\n",
    "This week addresses **multiple linear regression**, perhaps best exemplified through the consideration on **interactions** and their impact on the **model interpretation**, **evidence** and **validity* of models using **coefficient hypothesis testing** and \"in sample\" versus \"out of sample\" **model performance** comparision. Exactly, as in **simple linear regression**, the correctness of **p-values** used to give **evidence** for **predictive associations** that are **estimated** from a dataset depends on the (at least approximate) \"truth\" of the assumptions of the **multiple linear regression**, which are the same as those of the **simple linear regression** with the exception that the specification **linear form** can now model a much richer set of relationships between **predictor** and **outcome variables** based on **predictive associations** observed and **evidenced** in the data. With all this in mind, and reflecting back on the **Ethical Professionalism Considerations** from the previous week concerning **simple linear regression**...\n",
    "\n",
    "> - Which of the methods used for diagnostically assessing the assumptions of a **simple linear regression** specification could be used analogously generalized to the **multiple linear regression** context for the same purpose? \n",
    "> \n",
    "> - Examining the assumption of the **linear form** is more challenging in **multiple linear context**, but can be done using so-called **partial regression** (or **added variable**) **plot**. Is a ChatBot able to provide code to perform this diagnostic and instructions regarding its purpose, interpretation, and appropriate usage?\n",
    ">     \n",
    "> - Are there other diagnostic analyses that a ChatBot might suggest for you to help you evaluate the appropriateness of the assumptions of **fitted multiple linear regression model** you are considering using for **interpretation** or **prediction**? And if so, s the ChatBot able to provide code to perform these additional diagnostic and instructions regarding their purpose, interpretation, and appropriate usages?\n",
    ">     \n",
    "> - What do you think your ethical and professional responsibilites are when it comes to using and leveraging **multiple linear regression** methodology (and associated assumptions therein) in your work? To illustrate and demonstrate your thoughts on these considerations, can you give any specific examples of decisions that might be made during your process of executing a **multiple linear regression** that could have ethical and professional implications, risks, or consequences? What do you think are the simplest steps can you take to ensure that the conclusions of your work are both valid and reliable? What steps do you think are the most challenging from a practical perspective? \n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "\n",
    "**Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times.**\n",
    "\n",
    "Information about the course project is available on the course github repo [here](https://github.com/pointOfive/stat130chat130/tree/main/CP), including a draft [course project specfication](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F23_course_project_specification.ipynb) (subject to change). \n",
    "- The Week 01 HW introduced [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb), and the [available variables](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view). \n",
    "- Please do not download the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage (or the course github repo) multiple times.\n",
    "    \n",
    "> ### NEW DEVELOPMENT<br>New Abilities Achieved and New Levels Unlocked!!!    \n",
    "> \n",
    "> \"Question 3\" as described below only addresses **multiple linear regression**... but you'll also perhaps notice that \"Question 3\" as described above extends this to address addresses **logistic regression**.\n",
    "    \n",
    "### Current Course Project Capability Level \n",
    "    \n",
    "This homework's \"Question 3\" introduced the idea of performing some **multiple linear regression** analyses on dataset from the Canadian Social Connection Survey. While other questions of this homework focussed on other datasets, the general analyses and principles they introduce are no doubt informative and applicable to this the dataset for our course project. Ideally, this should put you in a position to quite proficiently perform **multiple linear regression** analyses for the course project if you so desire and find appropriate for the objectives of you course project submission. Thus, the following (and more) should be possible at this stage... \n",
    "\n",
    "1. Select multiple **predictors predictor** from the Canadian Social Connection Survey data and examine how they jointly influence an outcome variable, paying special attention to the inclusion and interpretation of **categorical** and **indicator variables** and **interactions** (in terms of \"baseline\" reference groups and \"contrast\" or \"offsets\").\n",
    "\n",
    "2. Visualize different kinds of **predictive association** relationships, including **interactions** and relationship between **predictor** and the **outcome** variables that change across different levels of other **categorical** or **indicator predictor variables**, using tools like `plotly.express`.\n",
    "\n",
    "3. Use **coefficient hypothesis testing** and \"in sample\" versus \"out of sample\" **model performance** evaluation to perform **model building** and examine **generalizability** of **fitted models**.\n",
    "       \n",
    "4. Assess the presence of **multicollinearity** by considering the **condition numbers** of **fitted models** (with \"centering and scaling\") and their subsequent potential implications on **generalizability** of **fitted models**; and, perhaps even examine **pairwise correlation** and/or **variance inflation factors** for each **predictor variable** if you're feeling extra ambitious and want to go well \"above and beyond\" (in which case you could also consider the relationship between **multicollinearity** and why one level of a **categorical** variable is always omitted).\n",
    "\n",
    "5. Compare and contrast such analyses and their benefits with previous methodologies introduced and considered in the course.\n",
    "    \n",
    "6. Explore using model diagnostic to check assess the assumptions of your **multiple linear regression** analyses, and reflect on how failurse of these assumptions might impact the reliability of your findings and conlusions derived from your **fitted model**.\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
